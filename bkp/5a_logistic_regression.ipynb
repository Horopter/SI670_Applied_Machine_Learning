{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression: Performance Analysis\n",
    "\n",
    "This notebook provides a comprehensive analysis of the **Logistic Regression** model performance.\n",
    "\n",
    "## Model Configuration\n",
    "\n",
    "- **Model ID**: `5a`\n",
    "- **Model Type**: `logistic_regression`\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "- **Metrics**: Dynamically resolved via `get_model_data_path()` (typically `data/stage5/logistic_regression/`)\n",
    "- **Logs**: `logs/stage5/` (dynamically resolved)\n",
    "- **MLflow**: `mlruns/`\n",
    "\n",
    "> **Note**: Execute cells sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Configuration\n",
    "\n",
    "### Standard Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:21:21.463308Z",
     "iopub.status.busy": "2025-12-18T06:21:21.463150Z",
     "iopub.status.idle": "2025-12-18T06:21:21.467230Z",
     "shell.execute_reply": "2025-12-18T06:21:21.466606Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third-Party Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:21:21.469102Z",
     "iopub.status.busy": "2025-12-18T06:21:21.468968Z",
     "iopub.status.idle": "2025-12-18T06:21:22.155247Z",
     "shell.execute_reply": "2025-12-18T06:21:22.154849Z"
    }
   },
   "outputs": [],
   "source": [
    "# Third-party imports for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure plotting style\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Root Detection\n",
    "\n",
    "Locate the project root directory by searching for the `lib/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:21:22.156885Z",
     "iopub.status.busy": "2025-12-18T06:21:22.156770Z",
     "iopub.status.idle": "2025-12-18T06:21:22.158938Z",
     "shell.execute_reply": "2025-12-18T06:21:22.158679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/santoshdesai/Downloads/fvc\n"
     ]
    }
   ],
   "source": [
    "def get_project_root() -> Path:\n",
    "    \"\"\"Find project root by locating lib/ directory.\n",
    "    \n",
    "    Returns:\n",
    "        Path to project root directory.\n",
    "    \"\"\"\n",
    "    current = Path.cwd()\n",
    "    for _ in range(10):\n",
    "        if (current / \"lib\").exists() and (\n",
    "            current / \"lib\" / \"__init__.py\"\n",
    "        ).exists():\n",
    "            return current\n",
    "        parent = current.parent\n",
    "        if parent == current:\n",
    "            break\n",
    "        current = parent\n",
    "    return Path.cwd()\n",
    "\n",
    "\n",
    "PROJECT_ROOT = get_project_root()\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Shared Utilities\n",
    "\n",
    "Import common functions from `notebook_utils.py` to avoid code duplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:21:22.173295Z",
     "iopub.status.busy": "2025-12-18T06:21:22.173213Z",
     "iopub.status.idle": "2025-12-18T06:21:22.176647Z",
     "shell.execute_reply": "2025-12-18T06:21:22.176337Z"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 639 (notebook_utils.py, line 641)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[36m(most recent call last)\u001b[39m:\n",
      "\u001b[36m  \u001b[39m\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfrom src.notebooks.notebook_utils import (\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m\u001b[31m:\u001b[39m No module named 'src'\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "Traceback \u001b[36m(most recent call last)\u001b[39m:\n",
      "  File \u001b[92m~/Downloads/fvc/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3701\u001b[39m in \u001b[95mrun_code\u001b[39m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[36m  \u001b[39m\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfrom notebook_utils import (\u001b[39m\n",
      "  \u001b[36mFile \u001b[39m\u001b[32m~/Downloads/fvc/src/notebooks/notebook_utils.py:641\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mresults_file = model_path / \"results.json\"\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after 'if' statement on line 639\n"
     ]
    }
   ],
   "source": [
    "# Import shared utilities from notebook_utils\n",
    "try:\n",
    "    from src.notebooks.notebook_utils import (\n",
    "        extract_training_times_comprehensive,\n",
    "        load_mlflow_metrics_by_model_type,\n",
    "        extract_hyperparameters_from_metrics,\n",
    "        get_latest_job_ids,\n",
    "        MODEL_TYPE_MAPPING,\n",
    "        get_model_data_path,\n",
    "        load_model_metrics,\n",
    "        load_results_json,\n",
    "        find_roc_pr_curve_files,\n",
    "        display_roc_pr_curve_images,\n",
    "        plot_cv_comparison,\n",
    "        plot_confusion_matrices,\n",
    "        plot_metric_summary_table,\n",
    "        plot_roc_curves_comprehensive,\n",
    "        plot_pr_curves_comprehensive,\n",
    "        plot_training_curves,\n",
    "        load_training_curves_from_jsonl,\n",
    "        plot_validation_metrics_across_folds,\n",
    "        query_duckdb_metrics\n",
    "    )\n",
    "    print(\"[OK] Successfully imported notebook utilities\")\n",
    "except ImportError:\n",
    "    # Fallback: try direct import\n",
    "    notebook_utils_path = PROJECT_ROOT / \"src\" / \"notebooks\"\n",
    "    if (notebook_utils_path / \"notebook_utils.py\").exists():\n",
    "        sys.path.insert(0, str(notebook_utils_path))\n",
    "        try:\n",
    "            from notebook_utils import (\n",
    "        extract_training_times_comprehensive,\n",
    "        load_mlflow_metrics_by_model_type,\n",
    "        extract_hyperparameters_from_metrics,\n",
    "        get_latest_job_ids,\n",
    "        MODEL_TYPE_MAPPING,\n",
    "        get_model_data_path,\n",
    "        load_model_metrics,\n",
    "        load_results_json,\n",
    "        find_roc_pr_curve_files,\n",
    "        display_roc_pr_curve_images,\n",
    "        plot_cv_comparison,\n",
    "        plot_confusion_matrices,\n",
    "        plot_metric_summary_table,\n",
    "        plot_roc_curves_comprehensive,\n",
    "        plot_pr_curves_comprehensive,\n",
    "        plot_training_curves,\n",
    "        load_training_curves_from_jsonl,\n",
    "        plot_validation_metrics_across_folds,\n",
    "        query_duckdb_metrics\n",
    "            )\n",
    "            print(\"[OK] Successfully imported notebook utilities\")\n",
    "        except ImportError as e:\n",
    "            print(f\"[WARN] Failed to import utilities: {e}\")\n",
    "            extract_training_times_comprehensive = None\n",
    "            load_mlflow_metrics_by_model_type = None\n",
    "            extract_hyperparameters_from_metrics = None\n",
    "            get_latest_job_ids = None\n",
    "            MODEL_TYPE_MAPPING = {}\n",
    "            get_model_data_path = None\n",
    "            load_model_metrics = None\n",
    "            load_results_json = None\n",
    "            find_roc_pr_curve_files = None\n",
    "            display_roc_pr_curve_images = None\n",
    "            plot_cv_comparison = None\n",
    "            plot_confusion_matrices = None\n",
    "            plot_metric_summary_table = None\n",
    "            plot_roc_curves_comprehensive = None\n",
    "            plot_pr_curves_comprehensive = None\n",
    "            plot_training_curves = None\n",
    "            load_training_curves_from_jsonl = None\n",
    "            plot_validation_metrics_across_folds = None\n",
    "            print(\"[OK] Successfully imported notebook utilities\")\n",
    "    else:\n",
    "        print(\"[WARN] notebook_utils.py not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration\n",
    "\n",
    "Define model-specific identifiers and dynamically resolve latest job IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:21:22.177614Z",
     "iopub.status.busy": "2025-12-18T06:21:22.177545Z",
     "iopub.status.idle": "2025-12-18T06:21:22.181075Z",
     "shell.execute_reply": "2025-12-18T06:21:22.180763Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model identifiers\n",
    "model_id = \"5a\"\n",
    "model_name = \"Logistic Regression\"\n",
    "\n",
    "# Dynamically get latest job IDs from log files\n",
    "if get_latest_job_ids:\n",
    "    latest_job_ids = get_latest_job_ids(PROJECT_ROOT)\n",
    "    job_id = latest_job_ids.get(model_id, \"unknown\")\n",
    "    print(f\"Latest job ID for 5a: {job_id}\")\n",
    "else:\n",
    "    job_id = \"unknown\"\n",
    "    print(\"[WARN] Could not resolve job ID dynamically\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load metrics and results from various data sources including JSON files, MLflow, and log files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model Metrics\n",
    "\n",
    "Load aggregated metrics from `metrics.json` or `results.json` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:21:22.182050Z",
     "iopub.status.busy": "2025-12-18T06:21:22.181995Z",
     "iopub.status.idle": "2025-12-18T06:21:22.184038Z",
     "shell.execute_reply": "2025-12-18T06:21:22.183738Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load metrics using shared utility function\n",
    "if load_model_metrics:\n",
    "    metrics = load_model_metrics(\n",
    "        model_id, PROJECT_ROOT, MODEL_TYPE_MAPPING\n",
    "    )\n",
    "    if metrics:\n",
    "        print(f\"[OK] Loaded metrics for Logistic Regression\")\n",
    "    else:\n",
    "        print(f\"[WARN] No metrics found for Logistic Regression\")\n",
    "        metrics = None\n",
    "else:\n",
    "    print(\"[WARN] load_model_metrics function unavailable\")\n",
    "    metrics = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test Results\n",
    "\n",
    "Load test set evaluation results from `results.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:21:22.184977Z",
     "iopub.status.busy": "2025-12-18T06:21:22.184916Z",
     "iopub.status.idle": "2025-12-18T06:21:22.187136Z",
     "shell.execute_reply": "2025-12-18T06:21:22.186813Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load test results (if available)\n",
    "# NOTE: Model 5a uses k-fold CV only (no separate test set)\n",
    "# Test results (results.json) are only available for models trained\n",
    "# with separate test set evaluation (e.g., 5alpha, 5beta)\n",
    "# Model 5a only saves validation metrics in metrics.json\n",
    "if load_results_json:\n",
    "    results = load_results_json(\n",
    "        model_id, PROJECT_ROOT, MODEL_TYPE_MAPPING\n",
    "    )\n",
    "    if results:\n",
    "        print(f\"[OK] Loaded test results\")\n",
    "        if 'test_f1' in results:\n",
    "            print(f\"  - Test F1: {results.get('test_f1', 0):.4f}\")\n",
    "            print(f\"  - Test AUC: {results.get('test_auc', 0):.4f}\")\n",
    "    else:\n",
    "        print(\"[INFO] No test results found (expected for 5a)\")\n",
    "        print(\"  Model 5a uses k-fold cross-validation with train/val splits only.\")\n",
    "        print(\"  Test set evaluation is not performed for this model.\")\n",
    "        print(\"  Validation metrics are available in metrics.json\")\n",
    "        results = None\n",
    "else:\n",
    "    print(\"[WARN] load_results_json function unavailable\")\n",
    "    results = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MLflow Metrics\n",
    "\n",
    "Load additional metrics from MLflow experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:21:22.188047Z",
     "iopub.status.busy": "2025-12-18T06:21:22.187993Z",
     "iopub.status.idle": "2025-12-18T06:21:23.237382Z",
     "shell.execute_reply": "2025-12-18T06:21:23.236977Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load MLflow metrics\n",
    "if load_mlflow_metrics_by_model_type:\n",
    "    model_type = MODEL_TYPE_MAPPING.get(model_id)\n",
    "    if model_type:\n",
    "        mlflow_data = load_mlflow_metrics_by_model_type(\n",
    "            model_type, \"mlruns/\", PROJECT_ROOT\n",
    "        )\n",
    "        if mlflow_data:\n",
    "            if isinstance(mlflow_data, dict) and \"message\" in mlflow_data:\n",
    "                # Baseline models don't use MLflow - this is expected\n",
    "                print(f\"[INFO] {mlflow_data.get('message', 'No MLflow data (expected for baseline models)')}\")\n",
    "                mlflow_data = None\n",
    "            else:\n",
    "                print(f\"[OK] Loaded MLflow metrics\")\n",
    "        else:\n",
    "            print(\"[WARN] No MLflow data found\")\n",
    "            mlflow_data = None\n",
    "    else:\n",
    "        print(f\"[WARN] Unknown model type for 5a\")\n",
    "        mlflow_data = None\n",
    "else:\n",
    "    print(\"[WARN] MLflow integration unavailable\")\n",
    "    mlflow_data = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query DuckDB Metrics\n",
    "\n",
    "Query metrics from DuckDB analytics database for fast SQL-based analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:21:23.238479Z",
     "iopub.status.busy": "2025-12-18T06:21:23.238401Z",
     "iopub.status.idle": "2025-12-18T06:21:23.240769Z",
     "shell.execute_reply": "2025-12-18T06:21:23.240367Z"
    }
   },
   "outputs": [],
   "source": [
    "# Query DuckDB for metrics\n",
    "# Debug: Check function availability\n",
    "try:\n",
    "    func_type = type(query_duckdb_metrics).__name__\n",
    "    func_is_none = query_duckdb_metrics is None\n",
    "    func_callable = callable(query_duckdb_metrics) if query_duckdb_metrics else False\n",
    "    print(f\"[DEBUG] query_duckdb_metrics: type={func_type}, is_none={func_is_none}, callable={func_callable}\")\n",
    "except NameError as e:\n",
    "    print(f\"[DEBUG] query_duckdb_metrics not in namespace: {e}\")\n",
    "    func_is_none = True\n",
    "    func_callable = False\n",
    "\n",
    "try:\n",
    "    duckdb_results = query_duckdb_metrics(\n",
    "        model_id, PROJECT_ROOT, MODEL_TYPE_MAPPING\n",
    "    )\n",
    "    if duckdb_results:\n",
    "        print(f\"[OK] Loaded metrics from DuckDB\")\n",
    "        print(f\"  Model type: {duckdb_results.get('model_type')}\")\n",
    "        print(f\"  Number of folds: {duckdb_results.get('num_folds', 0)}\")\n",
    "        agg = duckdb_results.get('aggregated', {})\n",
    "        if agg:\n",
    "            print(f\"  Mean Val Acc: {agg.get('mean_val_acc', 0):.4f} ± {agg.get('std_val_acc', 0):.4f}\")\n",
    "            print(f\"  Mean Val F1: {agg.get('mean_val_f1', 0):.4f} ± {agg.get('std_val_f1', 0):.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] DuckDB query failed: {e}\")\n",
    "else:\n",
    "        print(\"[WARN] No DuckDB data found for this model\")\n",
    "print(\"[WARN] query_duckdb_metrics function unavailable\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Training Times\n",
    "\n",
    "Extract training duration and per-fold execution times from log files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:21:23.241754Z",
     "iopub.status.busy": "2025-12-18T06:21:23.241698Z",
     "iopub.status.idle": "2025-12-18T06:21:23.250183Z",
     "shell.execute_reply": "2025-12-18T06:21:23.249839Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract training times from log files\n",
    "if extract_training_times_comprehensive and job_id != \"unknown\":\n",
    "    model_info = {\n",
    "        \"5a\": \"a\", \"5alpha\": \"alpha\", \"5b\": \"b\", \"5beta\": \"beta\",\n",
    "        \"5f\": \"f\", \"5g\": \"g\", \"5h\": \"h\"\n",
    "    }\n",
    "    suffix = model_info.get(model_id)\n",
    "    if suffix:\n",
    "        log_file = (\n",
    "            PROJECT_ROOT / \"logs\" / \"stage5\" /\n",
    "            f\"stage5{suffix}_{job_id}.log\"\n",
    "        )\n",
    "        training_times = extract_training_times_comprehensive(\n",
    "            log_file, model_id\n",
    "        )\n",
    "        if training_times:\n",
    "            print(f\"[OK] Extracted training times\")\n",
    "            if 'total_minutes' in training_times:\n",
    "                print(\n",
    "                    f\"  - Total time: \"\n",
    "                    f\"{training_times['total_minutes']:.2f} minutes\"\n",
    "                )\n",
    "        else:\n",
    "            print(\"[WARN] Could not extract training times\")\n",
    "            training_times = {}\n",
    "    else:\n",
    "        print(f\"[WARN] Unknown suffix for 5a\")\n",
    "        training_times = {}\n",
    "else:\n",
    "    print(\"[WARN] Training time extraction unavailable\")\n",
    "    training_times = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "\n",
    "Generate comprehensive visualizations of model performance across multiple metrics and folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Summary Table\n",
    "\n",
    "Display aggregated statistics (mean, std, min, max) for all metrics across cross-validation folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:21:23.251227Z",
     "iopub.status.busy": "2025-12-18T06:21:23.251158Z",
     "iopub.status.idle": "2025-12-18T06:21:23.256727Z",
     "shell.execute_reply": "2025-12-18T06:21:23.256377Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate and display metrics summary table\n",
    "if metrics and plot_metric_summary_table:\n",
    "    summary_df = plot_metric_summary_table(metrics, model_name)\n",
    "else:\n",
    "    print(\"[WARN] Cannot generate summary table - metrics or function unavailable\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Metrics Comparison\n",
    "\n",
    "Visualize the distribution of performance metrics across all cross-validation folds using boxplots and violin plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:21:23.257782Z",
     "iopub.status.busy": "2025-12-18T06:21:23.257721Z",
     "iopub.status.idle": "2025-12-18T06:21:23.388857Z",
     "shell.execute_reply": "2025-12-18T06:21:23.388298Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot CV metrics comparison\n",
    "if metrics and plot_cv_comparison:\n",
    "    fig = plot_cv_comparison(metrics, model_name)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"[WARN] Cannot generate CV comparison - metrics or function unavailable\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices\n",
    "\n",
    "Display confusion matrices for each cross-validation fold to visualize classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:21:23.389933Z",
     "iopub.status.busy": "2025-12-18T06:21:23.389858Z",
     "iopub.status.idle": "2025-12-18T06:21:23.643463Z",
     "shell.execute_reply": "2025-12-18T06:21:23.643157Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrices\n",
    "if metrics and plot_confusion_matrices:\n",
    "    fig = plot_confusion_matrices(\n",
    "        metrics, model_name, model_id, PROJECT_ROOT, MODEL_TYPE_MAPPING\n",
    "    )\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"[WARN] Cannot generate confusion matrices - metrics or function unavailable\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curves\n",
    "\n",
    "Display Receiver Operating Characteristic (ROC) curves from pre-generated PNG files or show a warning if unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:21:23.644711Z",
     "iopub.status.busy": "2025-12-18T06:21:23.644643Z",
     "iopub.status.idle": "2025-12-18T06:21:23.656547Z",
     "shell.execute_reply": "2025-12-18T06:21:23.656205Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display ROC curves\n",
    "if plot_roc_curves_comprehensive:\n",
    "    plot_roc_curves_comprehensive(\n",
    "        metrics, model_name, model_id, PROJECT_ROOT, MODEL_TYPE_MAPPING\n",
    "    )\n",
    "else:\n",
    "    print(\"[WARN] Cannot display ROC curves - function unavailable\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall Curves\n",
    "\n",
    "Display Precision-Recall (PR) curves from pre-generated PNG files or show a warning if unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:21:23.657790Z",
     "iopub.status.busy": "2025-12-18T06:21:23.657729Z",
     "iopub.status.idle": "2025-12-18T06:21:23.666397Z",
     "shell.execute_reply": "2025-12-18T06:21:23.666056Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display PR curves\n",
    "if plot_pr_curves_comprehensive:\n",
    "    plot_pr_curves_comprehensive(\n",
    "        metrics, model_name, model_id, PROJECT_ROOT, MODEL_TYPE_MAPPING\n",
    "    )\n",
    "else:\n",
    "    print(\"[WARN] Cannot display PR curves - function unavailable\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Curves\n",
    "\n",
    "Plot training and validation metrics over iterations/epochs from \n",
    "metrics.jsonl files. Shows iterative training progress.\n",
    "\n",
    "**Note**: Logistic Regression now supports epoch-wise training with 100 iterations. \n",
    "Training curves will be available after retraining the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:21:23.667437Z",
     "iopub.status.busy": "2025-12-18T06:21:23.667375Z",
     "iopub.status.idle": "2025-12-18T06:21:23.670828Z",
     "shell.execute_reply": "2025-12-18T06:21:23.670527Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot training curves from metrics.jsonl (for models with epochs)\n",
    "# Note: Baseline models (sklearn) don't have epoch-by-epoch training,\n",
    "# so this will show validation metrics at epoch 0 only\n",
    "if plot_training_curves:\n",
    "    model_data_path = get_model_data_path(\n",
    "        model_id, PROJECT_ROOT, MODEL_TYPE_MAPPING\n",
    "    )\n",
    "    \n",
    "    curves_found = False\n",
    "    # Dynamically find all available folds\n",
    "    if model_data_path and model_data_path.exists():\n",
    "        fold_dirs = sorted([d for d in model_data_path.iterdir() if d.is_dir() and d.name.startswith(\"fold_\")])\n",
    "        for fold_dir in fold_dirs:\n",
    "            metrics_file = fold_dir / \"metrics.jsonl\"\n",
    "        \n",
    "            if metrics_file.exists():\n",
    "                fig = plot_training_curves(metrics_file, model_name)\n",
    "                if fig:\n",
    "                    plt.show()\n",
    "                    curves_found = True\n",
    "                    break\n",
    "    \n",
    "    if not curves_found:\n",
    "        print(\"[WARN] No training curves data found\")\n",
    "\n",
    "    # Fallback: Try extracting from log files\n",
    "    if not curves_found and get_latest_job_ids:\n",
    "        from notebook_utils import extract_training_curves_from_log\n",
    "        latest_job_ids = get_latest_job_ids(PROJECT_ROOT)\n",
    "        job_id = latest_job_ids.get(model_id, \"unknown\")\n",
    "        if job_id != \"unknown\":\n",
    "            log_file = PROJECT_ROOT / \"logs\" / \"stage5\" / f\"stage5{model_id[-1]}_{job_id}.log\"\n",
    "            if log_file.exists():\n",
    "                log_curves = extract_training_curves_from_log(log_file, model_id)\n",
    "                if log_curves and log_curves.get(\"val\", {}).get(\"epoch\"):\n",
    "                    print(f\"[INFO] Extracted validation metrics from log file (fold-wise)\")\n",
    "                    # Plot the extracted curves\n",
    "                    from notebook_utils import plot_training_curves_from_history\n",
    "                    if plot_training_curves_from_history:\n",
    "                        fig = plot_training_curves_from_history(log_curves, model_name)\n",
    "                        if fig:\n",
    "                            plt.show()\n",
    "                            curves_found = True\n",
    "                    # Note: Logs only have fold-wise validation metrics, not per-epoch training\n",
    "else:\n",
    "    print(\"[WARN] Cannot display training curves - function unavailable\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Metrics Across Folds\n",
    "\n",
    "For baseline models (sklearn), plot validation metrics across CV folds.\n",
    "Since these models don't have training epochs, this shows how validation\n",
    "performance varies across different data splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:21:23.671900Z",
     "iopub.status.busy": "2025-12-18T06:21:23.671846Z",
     "iopub.status.idle": "2025-12-18T06:21:23.673404Z",
     "shell.execute_reply": "2025-12-18T06:21:23.673129Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot validation metrics across CV folds for baseline models\n",
    "try:\n",
    "    if metrics:\n",
    "        fig = plot_validation_metrics_across_folds(metrics, model_name)\n",
    "    if fig:\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"[WARN] Could not generate validation metrics plot\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] Error in try block: {e}\")\n",
    "if not metrics:\n",
    "    print(\"[WARN] No metrics available for plotting\")\n",
    "else:\n",
    "    print(\"[WARN] Cannot display validation metrics - function unavailable\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
