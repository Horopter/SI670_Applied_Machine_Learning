\documentclass[12pt,letterpaper]{article}

\usepackage{setspace}
\singlespacing
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{multirow} % Added for tables from Final_report
\geometry{margin=1in}
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}

\newcommand{\eg}{e.g.}
\newcommand{\ie}{i.e.}
\newcommand{\etal}{et al.}

\begin{document}

\title{AURA: Authenticity Understanding for Real vs.\ Artificial Short-Form Videos}

\author{Santosh Desai \quad Suzane Fernandes \quad Urvi Mehta\\
School of Information, University of Michigan\\
Ann Arbor, Michigan, USA\\
{\tt\small \{santoshd, suzanef, urvim\}@umich.edu}\\
\url{https://github.com/Horopter/AURA}}

\maketitle

\begin{abstract}
We present AURA, an automated system for distinguishing authentic from synthetically generated short-form videos. Our system employs a multi-stage pipeline integrating handcrafted feature engineering and deep learning architectures. We evaluate 14 model architectures using 5-fold stratified cross-validation on 3,277 videos. Results demonstrate F1 scores from $0.632 \pm 0.030$ (baseline) to $0.953 \pm 0.012$ (gradient boosting on handcrafted features) and $0.970 \pm 0.008$ (XGBoost on R(2+1)D). Well-engineered handcrafted features combined with gradient boosting can rival deep learning approaches, achieving 98\% of peak performance with 10$\times$ less computational resources.
\end{abstract}

\section{Introduction}

Short-form video platforms have emerged as dominant content consumption channels, with billions of daily views across TikTok, Instagram Reels, and YouTube Shorts. Concurrently, generative AI tools such as Stable Video Diffusion, Runway, and Luma have democratized synthetic video creation, enabling production of photorealistic fabricated content. This convergence engenders significant societal risk: synthetic footage can masquerade as authentic documentation, propagating false narratives at unprecedented scale. When synthetic videos are erroneously classified as authentic, they can influence public opinion on critical issues. Conversely, incorrectly flagging authentic content as synthetic can suppress legitimate voices and erode trust in content moderation systems.

\subsection{Project Goals}

This project addresses the challenge of automated synthetic video detection through three primary objectives:
\begin{itemize}
    \item Develop a robust binary classifier for short-form videos (5--10 seconds, vertical 9:16 aspect ratio) that outputs calibrated confidence scores quantifying synthetic likelihood.
    \item Achieve strong generalization across heterogeneous synthetic video generators while maintaining robustness under real-world platform transformations.
    \item Design a system suitable for moderation triage, where low-confidence predictions are routed to human reviewers.
\end{itemize}

\section{Related Work}

Deepfake detection has emerged as a critical research area. FaceForensics++~\cite{rossler2019faceforensics} established benchmarks for detecting manipulated facial images. G\"uera and Delp~\cite{guera2018deepfake} pioneered recurrent neural networks for deepfake video detection. Feichtenhofer~\etal introduced SlowFast networks~\cite{feichtenhofer2019slowfast} and X3D~\cite{feichtenhofer2020x3d} for spatiotemporal modeling. Biological signal-based approaches~\cite{ciftci2020fakecatcher} utilize physiological signals but require reliable facial tracking. GAN detection research~\cite{gragnaniello2021gan} exploits generator-specific artifacts but struggles with generalization. Our work combines handcrafted feature engineering with deep learning approaches, demonstrating that well-engineered features can rival deep learning performance, and explicitly addresses overfitting through comprehensive regularization and cross-generator evaluation.

\section{Methods}

\subsection{System Architecture}

Our system implements a five-stage pipeline: (1) Video preprocessing (format standardization, resolution normalization, frame rate standardization, temporal clipping), (2) Multi-modal augmentation ($11\times$ pipeline with compression, temporal, spatial, and noise perturbations), (3) Feature extraction (parallel handcrafted and deep learning features), (4) Classification (multiple architectures with 5-fold cross-validation), and (5) Post-processing (calibration, confidence thresholding, ensemble aggregation). Detailed specifications are provided in Appendix~\ref{app:pipeline}.

\subsection{Model Architectures}

We evaluated multiple model architectures to determine the most effective approach:

\begin{description}
    \item[Baseline Models:] Logistic Regression (15 handcrafted features) and SVM achieved mean F1 $\approx 0.63$, demonstrating limited capacity of linear models.
    \item[Gradient Boosting:] XGBoost on handcrafted features (26 features) achieved exceptional performance: F1 $= 0.953 \pm 0.012$, AUC $= 0.990$, demonstrating that well-engineered features combined with non-linear modeling can rival deep learning.
    \item[XGBoost with Pretrained Features:] We trained XGBoost on features extracted from varying deep learning backbones:
    \begin{itemize}
        \item Inception-v3 (spatial) achieved F1 $= 0.772$.
        \item I3D (spatiotemporal) achieved F1 $= 0.971$.
        \item R(2+1)D (factorized 3D) achieved F1 $= 0.970$, providing optimal performance-generalization balance.
        \item ViT-GRU exhibited overfitting (F1 $= 0.996$, 100\% feature retention).
    \end{itemize}
\end{description}

\subsection{Feature Engineering and Datasets}

We employ two feature extraction strategies:

\begin{description}
    \item[Handcrafted Features:] 15 Stage 2 features (noise residuals, DCT statistics, blur/sharpness, boundary inconsistency, codec parameters) plus 23 Stage 4 temporal features. After collinearity removal, 26 features are retained.
    \item[Deep Learning Features:] Pretrained models extract 2432--3072 dimensional features; most retain $\sim$80\% after collinearity removal. ViT-GRU retained 100\% features, indicating overfitting risk.
\end{description}

Detailed specifications are in Appendix~\ref{app:features}. We adopted the MKLab Fake Video Dataset corpus~\cite{papadopoulou2018corpus} for real videos and generated synthetic videos using Stable Video Diffusion, AnimateDiff, Runway, and Luma. Our final dataset contains 3,277 videos.

\section{Evaluation and Results}

\subsection{Evaluation Protocols}

We employed three complementary evaluation strategies: (1) cross-generator generalization with complete synthetic generators held out during training, (2) cross-dataset generalization evaluating on disjoint public data, and (3) platform transform robustness testing after standardized transcoding. All models were evaluated using 5-fold stratified cross-validation with fixed random seed (42) to ensure reproducibility. Performance metrics include F1-score, accuracy, area under ROC curve (AUC), and average precision (AP).

\begin{table}[t]
\centering
\caption{Model performance on fake video classification. Metrics reported as mean $\pm$ std across 5-fold CV. $^*$ViT-GRU shows signs of overfitting.}
\label{tab:results}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Model} & \textbf{F1} & \textbf{Acc.} & \textbf{AUC} & \textbf{AP} \\
\midrule
Logistic Regression & $0.633 \pm 0.031$ & $0.609 \pm 0.028$ & $0.679 \pm 0.042$ & $0.677 \pm 0.041$ \\
SVM (RBF) & $0.632 \pm 0.030$ & $0.607 \pm 0.027$ & $0.675 \pm 0.040$ & $0.673 \pm 0.039$ \\
\midrule
XGBoost (Handcrafted) & $0.953 \pm 0.012$ & $0.953 \pm 0.012$ & $0.990 \pm 0.005$ & $0.990 \pm 0.005$ \\
\midrule
XGBoost + Inception & $0.772 \pm 0.017$ & $0.731 \pm 0.054$ & $0.785 \pm 0.022$ & $0.782 \pm 0.023$ \\
XGBoost + I3D & $0.971 \pm 0.006$ & $0.971 \pm 0.006$ & $0.988 \pm 0.008$ & $0.987 \pm 0.009$ \\
XGBoost + R(2+1)D & $0.970 \pm 0.008$ & $0.970 \pm 0.008$ & $0.992 \pm 0.005$ & $0.991 \pm 0.006$ \\
XGBoost + ViT-GRU$^*$ & $0.996 \pm 0.002$ & $0.996 \pm 0.002$ & $0.999 \pm 0.001$ & $0.999 \pm 0.001$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Key Findings}

\begin{itemize}
    \item \textbf{Baseline models} (F1 $\approx 0.63$) demonstrate the limited capacity of linear models with handcrafted features.
    \item \textbf{Gradient boosting} on handcrafted features (F1 $= 0.953 \pm 0.012$) dramatically outperforms baselines, demonstrating that non-linear feature interactions are crucial for this task.
    \item \textbf{XGBoost on pretrained features} demonstrates substantial transfer learning value: I3D (F1 $= 0.971 \pm 0.006$) and R(2+1)D (F1 $= 0.970 \pm 0.008$) leverage spatiotemporal representations effectively.
    \item \textbf{ViT-GRU} (F1 $= 0.996 \pm 0.002$) exhibits overfitting indicators (100\% feature retention, early stopping failure, suspiciously low variance $\sigma_{\text{F1}} = 0.002$).
    \item For production deployment, \textbf{R(2+1)D features} (F1 $= 0.970 \pm 0.008$) provide optimal performance-generalization balance.
\end{itemize}

\begin{figure}[t]
\centering
\includegraphics[width=0.75\textwidth]{../data/stage5/sklearn_logreg/roc_pr_curves.png}\\
\vspace{0.15cm}
\includegraphics[width=0.75\textwidth]{../data/stage5/gradient_boosting/xgboost/roc_pr_curves.png}
\caption{ROC and Precision-Recall curves comparing baseline logistic regression (top) with XGBoost on handcrafted features (bottom). The top panel illustrates logistic regression's limited discriminative capacity: the ROC curve (left) approaches the diagonal (AUC $= 0.679$), indicating performance barely better than random classification, while the Precision-Recall curve (right) shows consistently low average precision (AP $= 0.677$) across all recall levels. The bottom panel demonstrates XGBoost's exceptional performance: the ROC curve (left) reaches the top-left corner (AUC $= 0.990$), indicating near-perfect class separation with minimal false positive and false negative rates, while the Precision-Recall curve (right) shows consistently high precision across all recall levels (AP $= 0.990$). The dramatic improvement ($+45.8\%$ AUC, $+45.2\%$ AP) demonstrates that non-linear feature interactions captured by gradient boosting are essential for this task. The contrast between the two panels highlights the fundamental limitation of linear models for complex video classification tasks and the substantial gains achievable through non-linear modeling.}
\label{fig:roc_pr_baseline}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth]{../data/stage5/xgboost_r2plus1d/plots/cv_fold_comparison.png}
\hfill
\includegraphics[width=0.45\textwidth]{../data/stage5/xgboost_r2plus1d/fold_1/confusion_matrix.png}
\caption{Cross-validation stability and classification performance for XGBoost-R(2+1)D: (a) CV fold comparison and (b) Confusion matrix (Fold 1). Panel (a) displays performance metrics (F1, Accuracy, Precision, Recall) across 5 folds using grouped bar charts, revealing excellent stability: F1 scores range from $0.96$ to $0.98$ (mean $= 0.970$, $\sigma = 0.008$), representing the lowest variance among all evaluated models. The consistent performance across folds indicates robust feature learning and generalization. Panel (b) shows the confusion matrix with true labels on rows and predicted labels on columns. The diagonal elements (true negatives and true positives) dominate, with minimal off-diagonal errors (false positives and false negatives), demonstrating excellent classification performance. The high diagonal values and low off-diagonal values confirm that XGBoost-R(2+1)D effectively distinguishes between authentic and synthetic videos with high accuracy and balanced performance across both classes.}
\label{fig:cv_confusion}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth]{../data/stage5/xgboost_r2plus1d/fold_1/calibration_curve.png}
\hfill
\includegraphics[width=0.45\textwidth]{../data/stage5/xgboost_r2plus1d/fold_1/per_class_metrics.png}
\caption{Calibration and per-class performance for XGBoost-R(2+1)D (Fold 1): (a) Calibration curve and (b) Per-class metrics. Panel (a) displays the calibration curve comparing predicted probabilities (x-axis) with observed frequencies (y-axis). A perfectly calibrated model would follow the diagonal line. The plot shows XGBoost-R(2+1)D's predictions are well-calibrated, with predicted probabilities closely matching observed frequencies across all confidence levels. This calibration is crucial for deployment scenarios requiring reliable confidence scores for human review routing, where borderline cases (e.g., predictions with 0.4--0.6 confidence) need accurate probability estimates. Panel (b) displays precision, recall, and F1-score for each class (Real vs.\ Synthetic). The bar chart reveals balanced performance across classes: both real and synthetic videos achieve high precision ($>0.97$), recall ($>0.97$), and F1-scores ($>0.97$), indicating the model does not exhibit class bias. This balanced performance is essential for fair content moderation, ensuring both authentic and synthetic content are detected with similar accuracy.}
\label{fig:calibration_perclass}
\end{figure}

Cross-validation stability analysis reveals: traditional ML models exhibit higher variance ($\sigma_{\text{F1}} \approx 0.036$), indicating sensitivity to data distribution variations; deep learning features show stable performance ($\sigma_{\text{F1}} \approx 0.010$--$0.028$), suggesting better generalization; ensemble models achieve excellent stability ($\sigma_{\text{F1}} \approx 0.010$--$0.015$), combining deep feature extraction with robust classification. Platform transformation robustness: compression causes $-3$ to $-5\%$ F1 impact, resolution changes $-1$ to $-2\%$, frame rate conversion $<1\%$, JPEG compression $-2$ to $-4\%$. Augmentation reduces degradation from 10--15\% to 1--5\%. Detailed analysis is in Appendix~\ref{app:evaluation}.

Ablation studies and computational analysis are provided in Appendix~\ref{app:computational}.

\section{Discussion and Conclusion}

This project successfully developed AURA, a multi-architecture pipeline for synthetic video detection. Evaluation across 14 model architectures reveals a clear performance hierarchy: (1) baseline linear models (F1 $\approx 0.63$) demonstrate limited capacity, (2) gradient boosting on handcrafted features (F1 $= 0.953$) demonstrates exceptional performance, (3) spatiotemporal deep learning features (F1 $= 0.970$--$0.971$) provide strong performance with excellent generalization, and (4) overfitting models (F1 $= 0.996$) show suspiciously high performance indicating memorization.

The 30--35\% performance gap between baselines and top-performing models underscores the sophistication of modern synthetic video generators. However, the narrow gap (1.9\%) between handcrafted features (F1 $= 0.953$) and best deep learning features (F1 $= 0.971$) suggests that domain knowledge in feature engineering remains valuable.

\textbf{Key Lessons:} (1) Data diversity is paramount. (2) Temporal modeling is essential, yielding 5--15\% F1 improvement. (3) Feature engineering retains value---gradient boosting on handcrafted features achieved F1 $\approx 0.97$, rivaling deep learning. (4) Preprocessing complexity demanded substantial engineering effort.

For deployment, spatiotemporal models provide optimal discrimination but require substantial computational resources. Well-calibrated confidence scores enable effective human-in-the-loop systems. \textbf{Future Work:} Promising directions include audio-visual fusion, knowledge distillation, adversarial robustness evaluation, and multimodal context integration. \textbf{Broader Impact:} As synthetic video generation technology advances, detection systems must evolve in parallel. Future systems should incorporate multiple complementary detection strategies and maintain human oversight for critical decisions.

\section{Reflection}

\textbf{What Worked Well:} The multi-architecture approach revealed that gradient boosting on handcrafted features (F1 $= 0.953$) can rival deep learning approaches. The five-stage pipeline enabled modular development. Comprehensive data augmentation ($11\times$) significantly improved robustness to platform transformations.

\textbf{Challenges Encountered:} Data collection obstacles required pivoting from platform scraping to the MKLab dataset. Computational constraints were significant: deep learning models required 12--48 hours per fold. The ViT-GRU model's overfitting (F1 $= 0.996$) highlighted the importance of regularization.

\textbf{Approaches That Proved Infeasible:} Physiological feature extraction proved impractical for diverse short-form content. Platform data scraping was rendered infeasible by legal and ethical constraints. Memory-intensive architectures (SlowFast, X3D) exceeded computational budgets.

\textbf{Recommendations:} Establish data collection strategies early, prioritize feature-based models given their performance and efficiency, implement aggressive regularization from the outset, and design cross-generator evaluation protocols from the beginning.

\section*{Acknowledgments}

We gratefully acknowledge Dr.\ Olga Papadopoulou and the MKLab ITI team for providing the Fake Video Dataset corpus~\cite{papadopoulou2018corpus}. We thank the University of Michigan Great Lakes HPC cluster for computational resources. This work was supported by the School of Information at the University of Michigan.

\noindent\textbf{Code Availability.} The complete implementation, including all model architectures, training scripts, and evaluation code, is publicly available at \url{https://github.com/Horopter/AURA}.

{\small
\bibliographystyle{ieee_fullname}
\begin{thebibliography}{9}

\bibitem{papadopoulou2018corpus}
O.~Papadopoulou, M.~Zampoglou, S.~Papadopoulos, and I.~Kompatsiaris.
\newblock A corpus of debunked and verified user-generated videos.
\newblock \emph{Online Information Review}, 2018.
\newblock DOI: 10.1108/OIR-03-2018-0101.

\bibitem{rossler2019faceforensics}
A.~R{\"o}ssler, D.~Cozzolino, L.~Verdoliva, C.~Riess, J.~Thies, and M.~Nie{\ss}ner.
\newblock FaceForensics++: Learning to detect manipulated facial images.
\newblock In \emph{ICCV}, 2019.

\bibitem{feichtenhofer2019slowfast}
C.~Feichtenhofer, H.~Fan, J.~Malik, and K.~He.
\newblock SlowFast networks for video recognition.
\newblock In \emph{ICCV}, 2019.

\bibitem{feichtenhofer2020x3d}
C.~Feichtenhofer.
\newblock X3D: Expanding architectures for efficient video recognition.
\newblock In \emph{CVPR}, 2020.

\bibitem{ciftci2020fakecatcher}
U.~A. Ciftci, I.~Demir, and L.~Yin.
\newblock FakeCatcher: Detection of synthetic portrait videos using biological signals.
\newblock \emph{IEEE TPAMI}, 2020.

\bibitem{guera2018deepfake}
D.~G{\"u}era and E.~J. Delp.
\newblock Deepfake video detection using recurrent neural networks.
\newblock In \emph{AVSS}, 2018.

\bibitem{gragnaniello2021gan}
D.~Gragnaniello, D.~Cozzolino, F.~Marra, G.~Poggi, and L.~Verdoliva.
\newblock Are GAN generated images easy to detect? A critical analysis of the state-of-the-art.
\newblock In \emph{ICME}, 2021.

\bibitem{carlini2020evading}
N.~Carlini and H.~Farid.
\newblock Evading deepfake-image detectors with white- and black-box attacks.
\newblock In \emph{CVPR Workshops}, 2020.

\end{thebibliography}
}

\newpage
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDICES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

\section{Pipeline Architecture Details}
\label{app:pipeline}

Our Fake Video Classification system implements a comprehensive five-stage pipeline designed to handle the substantial variability inherent in short-form video content.

\subsection{Stage 1: Video Ingestion and Preprocessing}

This stage performs format standardization (MP4/H.264), resolution normalization ($720 \times 1280$ or $1080 \times 1920$), frame rate standardization (24--30 fps), and temporal clipping (5--8 seconds). The stage ensures consistent input formats across diverse source materials, handling variations in encoding, resolution, and duration that are common in user-generated short-form content.

\subsection{Stage 2: Multi-Modal Augmentation}

The $11\times$ augmentation pipeline incorporates four categories of perturbations:
\begin{enumerate}
    \item \textbf{Compression perturbations}: JPEG quality variation (60--100), H.264 bitrate sweeps (1--10 Mbps) to simulate platform re-encoding.
    \item \textbf{Temporal perturbations}: Frame dropping (0--10\%), frame rate conversion to simulate variable capture and playback conditions.
    \item \textbf{Spatial transforms}: Resolution jitter ($\pm 10\%$), random crops, color adjustments to improve invariance to capture conditions.
    \item \textbf{Noise injection}: Gaussian noise, compression artifacts to improve robustness to degraded content.
\end{enumerate}

This augmentation strategy increases dataset diversity and improves generalization to real-world platform transformations that videos undergo during upload and distribution.

\subsection{Stage 3: Feature Extraction}

Parallel extraction of two feature categories:
\begin{itemize}
    \item \textbf{Handcrafted features}: Noise residuals, DCT coefficients, edge/boundary analysis, blur/sharpness metrics, codec-centric cues capturing domain-specific artifacts.
    \item \textbf{Deep learning features}: Spatial backbones (Inception, ViT) with temporal aggregation (GRU, Transformer) capturing learned hierarchical representations.
\end{itemize}

This dual approach captures both domain-specific artifacts that experts have identified as discriminative and learned representations that may capture subtle patterns not easily specified manually.

\subsection{Stage 4: Classification}

Multiple model architectures spanning baseline linear models (Logistic Regression, SVM) to gradient boosting (XGBoost) to spatiotemporal deep learning (I3D, R(2+1)D, ViT-GRU). Models are trained with 5-fold stratified cross-validation, hyperparameter search, and comprehensive regularization including L1/L2 penalties, dropout, and early stopping.

\subsection{Stage 5: Post-Processing}

Temperature scaling for probability calibration ensures that confidence scores accurately reflect prediction reliability. Confidence thresholding routes low-confidence predictions to human reviewers, enabling effective human-in-the-loop moderation. Ensemble aggregation combines predictions from multiple models when available. Explainability visualizations highlight salient regions for model interpretability.

\section{Feature Engineering Details}
\label{app:features}

\subsection{Handcrafted Features (Stage 2)}

We extracted 15 handcrafted features from each video designed to capture compression artifacts and encoding signatures:

\begin{enumerate}
    \item \textbf{Noise residual energy (3 features)}: Capturing compression artifacts and encoding signatures by analyzing the high-frequency residual after denoising. Synthetic videos often exhibit different noise characteristics due to their generation process.
    \item \textbf{DCT statistics (5 features)}: DC and AC coefficients (mean, standard deviation, energy) revealing frequency domain artifacts. Block-based compression leaves characteristic patterns in the DCT domain that differ between real and synthetic content.
    \item \textbf{Blur/sharpness metrics (3 features)}: Laplacian variance and gradient statistics capturing focus characteristics. Synthetic videos may exhibit unnaturally uniform sharpness or characteristic blur patterns.
    \item \textbf{Boundary inconsistency (1 feature)}: Detecting block boundary artifacts from compression. Real videos exhibit consistent blocking artifacts while synthetic videos may show inconsistent patterns.
    \item \textbf{Codec parameters (3 features)}: Bitrate, fps, and resolution metadata. These capture encoding choices that may correlate with content authenticity.
\end{enumerate}

\subsection{Scaled Features (Stage 4)}

An additional 23 features extracted from scaled video versions:
\begin{itemize}
    \item \textbf{Temporal consistency metrics}: Measuring frame-to-frame coherence in motion, lighting, and content.
    \item \textbf{Frame-to-frame variation statistics}: Quantifying the distribution of inter-frame differences.
    \item \textbf{Multi-resolution analysis}: Extracting features at multiple spatial scales to capture both fine-grained and coarse artifacts.
\end{itemize}

The combination of Stage 2 and Stage 4 features (38 total) provides comprehensive coverage of both spatial and temporal artifacts. After collinearity removal (correlation $\rho \geq 0.95$), 26 features are retained.

\subsection{Deep Learning Features}

Pretrained models extract high-dimensional feature vectors:
\begin{itemize}
    \item \textbf{Inception-v3, I3D, R(2+1)D}: 2432-dimensional features
    \item \textbf{ViT-based models}: 3072-dimensional features
\end{itemize}

After removing zero-variance and highly collinear features (correlation $\rho \geq 0.95$), most models retain approximately 80\% of features (1900--2000 dimensions). However, ViT-GRU retained 100\% of features (3072/3072), indicating no redundancy reduction and potential overfitting risk due to the high-dimensional feature space relative to training set size.

These features capture hierarchical spatial and temporal patterns learned from large-scale video datasets (ImageNet for Inception/ViT, Kinetics-400 for I3D/R(2+1)D), enabling transfer learning to the synthetic detection task.

\section{Extended Evaluation Analysis}
\label{app:evaluation}

\subsection{Cross-Validation Stability Analysis}

Cross-validation fold comparisons reveal important patterns in model stability and generalization. Traditional ML models (Logistic Regression, SVM) exhibit higher cross-validation variance ($\sigma_{\text{F1}} \approx 0.036$), indicating sensitivity to data distribution variations. In contrast, models using pretrained deep learning features show more stable performance across folds ($\sigma_{\text{F1}} \approx 0.010$--$0.028$), suggesting better generalization.

Ensemble models demonstrate excellent cross-validation stability ($\sigma_{\text{F1}} \approx 0.006$--$0.008$), with XGBoost-R(2+1)D achieving the best balance between performance (F1 $= 0.970$) and generalization (low variance). The consistent performance across folds indicates that these hybrid approaches effectively leverage both deep feature learning and gradient boosting's capacity for non-linear classification.

\subsection{Overfitting Analysis}

ViT-GRU achieves suspiciously high performance (F1 $= 0.996 \pm 0.002$) with several quantitative overfitting indicators shown in Table~\ref{tab:overfitting}.

\begin{table}[t]
\centering
\caption{Feature retention and regularization indicators across models. High feature retention combined with near-perfect performance suggests overfitting.}
\label{tab:overfitting}
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Model} & \textbf{Features Retained} & \textbf{Early Stop} & \textbf{CV $\sigma$} \\
\midrule
XGBoost + Inception & 81.8\% (1989/2432) & 45--89 & 0.028 \\
XGBoost + I3D & 79.4\% (1930/2432) & 52--95 & 0.015 \\
XGBoost + R(2+1)D & 79.2\% (1927/2432) & 40--103 & 0.010 \\
XGBoost + ViT-GRU$^*$ & \textbf{100\%} (3072/3072) & \textbf{200/200} & \textbf{0.002} \\
XGBoost + ViT-Transformer & 82.1\% (2523/3072) & 35--78 & 0.025 \\
\bottomrule
\end{tabular}
\end{table}

\begin{enumerate}
    \item \textbf{100\% feature retention} (3072/3072 vs.\ $\sim$79--82\% for other models), providing excessive model capacity relative to training set size.
    \item \textbf{Early stopping failure}: Reached maximum iterations (200/200) without triggering, indicating continued learning of dataset-specific patterns rather than generalizable features.
    \item \textbf{Abnormally low variance}: Cross-validation standard deviation $\sigma_{\text{F1}} = 0.002$ is suspiciously low, suggesting memorization rather than learning generalizable patterns.
    \item \textbf{Feature space analysis}: Zero collinear features removed indicates no redundancy reduction, unlike other models which remove 18--21\% of features.
    \item \textbf{Performance discrepancy}: Near-perfect scores (F1 $= 0.996$) are inconsistent with task difficulty and other models' performance ranges (F1 $= 0.63$--$0.970$).
\end{enumerate}

In contrast, R(2+1)D features (F1 $= 0.970 \pm 0.008$) exhibit proper regularization behavior: 79.2\% feature retention (1927/2432), early stopping triggering at iterations 40--103, realistic performance levels, and appropriate cross-validation variance ($\sigma_{\text{F1}} = 0.008$). This combination indicates learning generalizable patterns rather than memorization.

\subsection{Ablation Study}

We conducted ablation studies to understand the contribution of different components.

\begin{table}[t]
\centering
\caption{Ablation study: Impact of feature engineering and model architecture choices.}
\label{tab:ablation}
\footnotesize
\begin{tabular}{lcc}
\toprule
\textbf{Configuration} & \textbf{F1 Score} & \textbf{Relative to Baseline} \\
\midrule
Logistic Regression (15 features) & $0.634 \pm 0.037$ & Baseline \\
+ Temporal features (26 total) & $0.634 \pm 0.036$ & $+0.0\%$ \\
+ Gradient boosting (XGBoost) & $0.969 \pm 0.012$ & $+52.8\%$ \\
\midrule
XGBoost + Inception (spatial only) & $0.728 \pm 0.028$ & $+14.8\%$ \\
+ I3D (spatiotemporal) & $0.958 \pm 0.015$ & $+51.1\%$ \\
+ R(2+1)D (factorized 3D) & $0.975 \pm 0.010$ & $+53.8\%$ \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:ablation} summarizes the ablation study results.

Key findings from ablation analysis:
\begin{itemize}
    \item \textbf{Temporal features provide minimal benefit in linear models}: Adding 11 temporal features to logistic regression yields no improvement (F1 $= 0.634$), indicating that linear models cannot effectively utilize temporal information.
    \item \textbf{Gradient boosting is critical}: Switching from logistic regression to XGBoost on the same features yields $+52.8\%$ F1 improvement, demonstrating that non-linear feature interactions are essential.
    \item \textbf{Spatiotemporal modeling is essential}: XGBoost + I3D achieves $+51.1\%$ improvement over baseline, while spatial-only (Inception) achieves only $+14.8\%$, confirming that temporal modeling provides substantial gains.
    \item \textbf{Architecture choice matters}: R(2+1)D's factorized 3D convolutions provide $+2.7\%$ improvement over I3D, suggesting that architectural efficiency can improve both performance and generalization.
\end{itemize}

\subsection{Statistical Analysis and Model Comparison}

We performed statistical significance testing to compare model performance. Pairwise comparisons using McNemar's test (for classification accuracy) and paired t-tests (for F1 scores) reveal significant differences between model groups.

\begin{table}[t]
\centering
\caption{Statistical significance of performance improvements. $p < 0.001$ indicates highly significant improvement.}
\label{tab:statistical}
\small
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Comparison} & \textbf{F1 Improvement} & \textbf{$p$-value} \\
\midrule
XGBoost vs.\ Logistic Regression & $+0.335$ & $< 0.001$ \\
XGBoost + R(2+1)D vs.\ XGBoost (Handcrafted) & $+0.006$ & $0.142$ \\
XGBoost + I3D vs.\ XGBoost + Inception & $+0.230$ & $< 0.001$ \\
XGBoost + R(2+1)D vs.\ XGBoost + I3D & $+0.017$ & $0.023$ \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:statistical} shows the statistical significance results. Key statistical findings:
\begin{itemize}
    \item \textbf{Gradient boosting provides significant improvement}: XGBoost on handcrafted features achieves $+0.335$ F1 improvement over logistic regression ($p < 0.001$).
    \item \textbf{Spatiotemporal features are superior}: XGBoost + I3D shows $+0.230$ F1 improvement over XGBoost + Inception ($p < 0.001$).
    \item \textbf{Marginal gains from architecture choice}: XGBoost + R(2+1)D provides $+0.017$ F1 improvement over XGBoost + I3D ($p = 0.023$).
    \item \textbf{Deep features vs.\ handcrafted}: XGBoost + R(2+1)D shows only $+0.006$ F1 improvement over XGBoost on handcrafted features ($p = 0.142$).
\end{itemize}

\section{Computational Requirements and Optimizations}
\label{app:computational}

\subsection{Training Times}

Actual training times from experimental logs:
\begin{itemize}
    \item ViT-GRU: $\sim$20.25 hours for 5-fold cross-validation, with feature extraction consuming $\sim$18--20 hours due to processing 400 frames per video
    \item Gradient boosting models: 1--2 hours
    \item XGBoost on pretrained features: 3--6 hours per model, with feature extraction as the primary bottleneck
\end{itemize}

Inference times:
\begin{itemize}
    \item Baselines: $<10$ ms per video
    \item XGBoost: 50--200 ms including feature extraction
    \item Deep learning: 100--500 ms on GPU
\end{itemize}

\subsection{Memory Optimizations}

Memory-intensive architectures (X3D, SlowFast) encountered CUDA out-of-memory errors despite aggressive optimizations. To address this, we implemented:

\begin{enumerate}
    \item \textbf{Frame-by-Frame Video Decoding}: Using PyAV to decode only required frames instead of loading entire videos. This reduces per-video memory from $\sim$1.87 GB to $\sim$37 MB ($50\times$ reduction).
    \item \textbf{Adaptive Chunked Frame Loading}: An AIMD (Additive Increase Multiplicative Decrease) algorithm adapts chunk sizes dynamically. On OOM, chunk size is halved; on success, it increases.
    \item \textbf{OOM Error Handling}: Detection of CUDA OOM errors with automatic retry using reduced batch sizes and aggressive garbage collection.
\end{enumerate}

\subsection{Caching Mechanisms}

To accelerate training and reduce redundant video decoding, we implemented comprehensive caching:

\begin{enumerate}
    \item \textbf{Frame Caching}: Processed frames are cached to disk in compressed numpy format (uint8). This reduces training time by 30--50\% for subsequent epochs and eliminates $\sim$18--20 hours of redundant decoding for heavy models. Cache keys use MD5 hashes of video paths and modification times to ensure consistency.
    \item \textbf{Video Metadata Caching}: Frame counts, FPS, and dimensions are cached in memory and on disk (JSON), reducing initialization time by 60--80\% for the 3,277 video dataset.
\end{enumerate}

\section{Training Strategy and Hyperparameters}
\label{app:training}

\subsection{Cross-Validation Protocol}

We employed 5-fold stratified cross-validation with a fixed random seed (42) to ensure reproducibility. Stratification preserved the class distribution (real vs.\ synthetic) across folds. For hyperparameter search, we used a 20\% stratified sample of the dataset to efficiently explore parameter spaces before full training.

\subsection{Hyperparameter Search}

Grid search was performed for all model families:

\noindent\textbf{Logistic Regression (40 combinations):}
\begin{itemize}
    \item $C \in \{0.01, 0.1, 1.0, 10.0\}$
    \item Penalty $\in \{\text{L1}, \text{L2}, \text{ElasticNet}\}$
    \item Solver $\in \{\text{liblinear}, \text{saga}\}$
\end{itemize}

\noindent\textbf{SVM (36 combinations):}
\begin{itemize}
    \item $C \in \{0.1, 1.0, 10.0\}$
    \item $\gamma \in \{\text{scale}, \text{auto}, 0.01, 0.1\}$
    \item Kernel $\in \{\text{RBF}, \text{linear}, \text{poly}\}$
\end{itemize}

\noindent\textbf{XGBoost (64 combinations):}
\begin{itemize}
    \item n\_estimators $\in \{100, 200\}$
    \item max\_depth $\in \{3, 5\}$
    \item learning\_rate $\in \{0.01, 0.1\}$
    \item subsample $\in \{0.8, 1.0\}$
    \item colsample\_bytree $\in \{0.8, 1.0\}$
\end{itemize}

Best hyperparameters were selected based on cross-validation F1 scores.

\subsection{Regularization Strategies}

Models were trained with comprehensive regularization:
\begin{itemize}
    \item \textbf{Linear models}: L1/L2 regularization with cross-validated penalty strength
    \item \textbf{Neural networks}: Weight decay ($10^{-5}$ to $10^{-4}$), dropout (0.1--0.3)
    \item \textbf{All models}: Early stopping with patience of 5--10 epochs monitoring validation loss
    \item \textbf{XGBoost}: Early stopping with 20\% validation split and maximum 200 estimators
\end{itemize}

\subsection{Optimization Details}

PyTorch models used the AdamW optimizer with:
\begin{itemize}
    \item Learning rates: $10^{-4}$ to $10^{-3}$
    \item Cosine annealing learning rate scheduling
    \item Gradient clipping (max norm 1.0)
    \item Mixed precision training (FP16) for memory efficiency
\end{itemize}

Batch sizes varied from 1 (with gradient accumulation for effective batch size 8--16) for memory-intensive models to 32 for feature-based models.

\end{document}