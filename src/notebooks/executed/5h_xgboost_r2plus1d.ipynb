{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Workflow\n",
    "\n",
    "The analysis proceeds through six systematic steps, each handling a specific aspect of performance evaluation:\n",
    "\n",
    "1. **Loading Results**: Extract metrics from `results.json` or model-specific files\n",
    "2. **DuckDB Query**: Fast SQL-based retrieval of fold-wise metrics\n",
    "3. **MLflow Integration**: Load experiment tracking data (if available)\n",
    "4. **Training Curves**: Extract epoch-wise or iteration-wise metrics from all folds\n",
    "5. **Visualizations**: Generate comprehensive plots and charts\n",
    "6. **Performance Summary**: Display detailed metrics breakdown\n",
    "\n",
    "Each step is independent, allowing partial analysis even if some data sources are unavailable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Configuration\n",
    "\n",
    "### Standard Library Imports\n",
    "\n",
    "Essential Python standard library modules for file I/O, path manipulation, type hints, and system operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:29:15.091886Z",
     "iopub.status.busy": "2025-12-20T15:29:15.091622Z",
     "iopub.status.idle": "2025-12-20T15:29:15.095819Z",
     "shell.execute_reply": "2025-12-20T15:29:15.095094Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Any\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Root Path Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:29:15.097596Z",
     "iopub.status.busy": "2025-12-20T15:29:15.097465Z",
     "iopub.status.idle": "2025-12-20T15:29:16.048788Z",
     "shell.execute_reply": "2025-12-20T15:29:16.048374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Project root: /Users/santoshdesai/Downloads/fvc\n",
      "✓ notebook_utils found at: /Users/santoshdesai/Downloads/fvc/src/notebooks/notebook_utils.py\n"
     ]
    }
   ],
   "source": [
    "# Resolve project root by searching for lib/ directory\n",
    "project_root = Path.cwd()\n",
    "for _ in range(10):\n",
    "    if (project_root / 'lib').exists() and (project_root / 'lib' / '__init__.py').exists():\n",
    "        break\n",
    "    parent = project_root.parent\n",
    "    if parent == project_root:\n",
    "        break\n",
    "    project_root = parent\n",
    "\n",
    "# Add paths to sys.path\n",
    "sys.path.insert(0, str(project_root))\n",
    "sys.path.insert(0, str(project_root / \"src\" / \"notebooks\"))\n",
    "\n",
    "# Verify notebook_utils can be imported\n",
    "try:\n",
    "    import notebook_utils\n",
    "    print(f\"✓ Project root: {project_root}\")\n",
    "    print(f\"✓ notebook_utils found at: {notebook_utils.__file__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠ Warning: {e}\")\n",
    "    print(f\"Project root: {project_root}\")\n",
    "    print(f\"Looking for: {project_root / 'src' / 'notebooks' / 'notebook_utils.py'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third-Party Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:29:16.063134Z",
     "iopub.status.busy": "2025-12-20T15:29:16.062976Z",
     "iopub.status.idle": "2025-12-20T15:29:16.064675Z",
     "shell.execute_reply": "2025-12-20T15:29:16.064386Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Image, display\n",
    "import tempfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:29:16.065907Z",
     "iopub.status.busy": "2025-12-20T15:29:16.065846Z",
     "iopub.status.idle": "2025-12-20T15:29:16.067439Z",
     "shell.execute_reply": "2025-12-20T15:29:16.067161Z"
    }
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project-Specific Utilities Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:29:16.068386Z",
     "iopub.status.busy": "2025-12-20T15:29:16.068327Z",
     "iopub.status.idle": "2025-12-20T15:29:16.070110Z",
     "shell.execute_reply": "2025-12-20T15:29:16.069765Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from notebook_utils import (\n",
    "        get_model_data_path, load_results_json, load_mlflow_metrics_by_model_type,\n",
    "        query_duckdb_metrics, plot_training_curves, find_roc_pr_curve_files,\n",
    "        display_roc_pr_curve_images, plot_validation_metrics_across_folds,\n",
    "        display_png_plots_from_folds, MODEL_TYPE_MAPPING, MLFLOW_MODEL_TYPE_MAPPING,\n",
    "        analyze_mlflow_experiment_structure, print_mlflow_experiment_structure,\n",
    "        plot_mlflow_experiment_structure, analyze_and_visualize_mlflow_performance,\n",
    "        display_duckdb_metrics_summary, get_metrics_data\n",
    "    )\n",
    "except ImportError as e:\n",
    "    print(f\"[ERROR] Failed to import notebook_utils: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Root Detection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:29:16.071346Z",
     "iopub.status.busy": "2025-12-20T15:29:16.071273Z",
     "iopub.status.idle": "2025-12-20T15:29:16.073040Z",
     "shell.execute_reply": "2025-12-20T15:29:16.072727Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_project_root() -> Path:\n",
    "    \"\"\"Find project root directory.\"\"\"\n",
    "    current = Path.cwd()\n",
    "    for _ in range(10):\n",
    "        if (current / \"lib\").exists() and (current / \"lib\" / \"__init__.py\").exists():\n",
    "            return current\n",
    "        parent = current.parent\n",
    "        if parent == current:\n",
    "            break\n",
    "        current = parent\n",
    "    return Path.cwd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Project Root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:29:16.073938Z",
     "iopub.status.busy": "2025-12-20T15:29:16.073882Z",
     "iopub.status.idle": "2025-12-20T15:29:16.075541Z",
     "shell.execute_reply": "2025-12-20T15:29:16.075256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/santoshdesai/Downloads/fvc\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ROOT = get_project_root()\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Workflow\n",
    "\n",
    "The analysis proceeds through six systematic steps, each handling a specific aspect of performance evaluation:\n",
    "\n",
    "1. **Loading Results**: Extract metrics from `results.json` or model-specific files\n",
    "2. **DuckDB Query**: Fast SQL-based retrieval of fold-wise metrics\n",
    "3. **MLflow Integration**: Load experiment tracking data (if available)\n",
    "4. **Training Curves**: Extract epoch-wise or iteration-wise metrics from all folds\n",
    "5. **Visualizations**: Generate comprehensive plots and charts\n",
    "6. **Performance Summary**: Display detailed metrics breakdown\n",
    "\n",
    "Each step is independent, allowing partial analysis even if some data sources are unavailable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1/6: Loading Results/Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:29:16.076498Z",
     "iopub.status.busy": "2025-12-20T15:29:16.076424Z",
     "iopub.status.idle": "2025-12-20T15:29:16.078195Z",
     "shell.execute_reply": "2025-12-20T15:29:16.077886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "XGBoost R2Plus1D (5h) - Comprehensive Performance Analysis\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize analysis variables\n",
    "project_root = get_project_root()\n",
    "model_id = \"5h\"\n",
    "model_name = \"XGBoost R2Plus1D\"\n",
    "print(\"=\" * 70)\n",
    "print(\"XGBoost R2Plus1D (5h) - Comprehensive Performance Analysis\")\n",
    "print(\"=\" * 70)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:29:16.079099Z",
     "iopub.status.busy": "2025-12-20T15:29:16.079045Z",
     "iopub.status.idle": "2025-12-20T15:29:16.081987Z",
     "shell.execute_reply": "2025-12-20T15:29:16.081644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/6] Loading results/metrics...\n",
      "  [INFO] CV-only model - no test results.json (expected)\n"
     ]
    }
   ],
   "source": [
    "print(\"[1/6] Loading results/metrics...\")\n",
    "model_path = get_model_data_path(model_id, project_root, MODEL_TYPE_MAPPING)\n",
    "if not model_path:\n",
    "    print(f\"[ERROR] Model path not found for {model_id}\")\n",
    "    \n",
    "results = load_results_json(model_id, project_root, MODEL_TYPE_MAPPING)\n",
    "if results:\n",
    "    print(f\"  ✓ Loaded results/metrics\")\n",
    "else:\n",
    "    print(f\"  [INFO] CV-only model - no test results.json (expected)\")\n",
    "    results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2/6: Querying DuckDB for Fold-wise Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:29:16.083010Z",
     "iopub.status.busy": "2025-12-20T15:29:16.082939Z",
     "iopub.status.idle": "2025-12-20T15:29:16.107387Z",
     "shell.execute_reply": "2025-12-20T15:29:16.107029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/6] Querying DuckDB for fold-wise metrics...\n",
      "  ✓ Retrieved DuckDB metrics\n",
      "    - 4 fold results\n",
      "    - F1 Score:     min=0.6667, max=0.7692\n",
      "    - Accuracy:     min=0.6615, max=0.7727\n",
      "    - Precision:    min=0.6667, max=0.8065\n",
      "    - Recall:       min=0.6667, max=0.7353\n",
      "\n",
      "    Aggregated Metrics (Mean ± Std):\n",
      "      F1 Score:     0.7601 ± 0.1101\n",
      "      Accuracy:     0.7583 ± 0.1115\n",
      "      Precision:    0.7701 ± 0.1099\n",
      "      Recall:       0.7510 ± 0.1126\n",
      "      Class 0 F1:    0.7173 ± 0.0447\n",
      "      Class 1 F1:    0.7227 ± 0.0374\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[2/6] Querying DuckDB for fold-wise metrics...\")\n",
    "duckdb_metrics = query_duckdb_metrics(model_id, project_root, MODEL_TYPE_MAPPING)\n",
    "if duckdb_metrics:\n",
    "    print(f\"  ✓ Retrieved DuckDB metrics\")\n",
    "    \n",
    "    # Display fold results summary\n",
    "    if \"fold_results\" in duckdb_metrics:\n",
    "        fold_results = duckdb_metrics[\"fold_results\"]\n",
    "        print(f\"    - {len(fold_results)} fold results\")\n",
    "        \n",
    "        # Calculate min/max across folds\n",
    "        if fold_results:\n",
    "            val_f1_values = [f.get('val_f1', 0) for f in fold_results if f.get('val_f1') is not None]\n",
    "            val_acc_values = [f.get('val_acc', 0) for f in fold_results if f.get('val_acc') is not None]\n",
    "            val_prec_values = [f.get('val_precision', 0) for f in fold_results if f.get('val_precision') is not None]\n",
    "            val_recall_values = [f.get('val_recall', 0) for f in fold_results if f.get('val_recall') is not None]\n",
    "            \n",
    "            if val_f1_values:\n",
    "                print(f\"    - F1 Score:     min={min(val_f1_values):.4f}, max={max(val_f1_values):.4f}\")\n",
    "            if val_acc_values:\n",
    "                print(f\"    - Accuracy:     min={min(val_acc_values):.4f}, max={max(val_acc_values):.4f}\")\n",
    "            if val_prec_values:\n",
    "                print(f\"    - Precision:    min={min(val_prec_values):.4f}, max={max(val_prec_values):.4f}\")\n",
    "            if val_recall_values:\n",
    "                print(f\"    - Recall:       min={min(val_recall_values):.4f}, max={max(val_recall_values):.4f}\")\n",
    "    \n",
    "    # Display aggregated metrics\n",
    "    if \"aggregated\" in duckdb_metrics:\n",
    "        agg = duckdb_metrics[\"aggregated\"]\n",
    "        print(f\"\\n    Aggregated Metrics (Mean ± Std):\")\n",
    "        if agg.get('mean_val_f1') is not None:\n",
    "            print(f\"      F1 Score:     {agg.get('mean_val_f1', 0):.4f} ± {agg.get('std_val_f1', 0):.4f}\")\n",
    "        if agg.get('mean_val_acc') is not None:\n",
    "            print(f\"      Accuracy:     {agg.get('mean_val_acc', 0):.4f} ± {agg.get('std_val_acc', 0):.4f}\")\n",
    "        if agg.get('mean_val_precision') is not None:\n",
    "            print(f\"      Precision:    {agg.get('mean_val_precision', 0):.4f} ± {agg.get('std_val_precision', 0):.4f}\")\n",
    "        if agg.get('mean_val_recall') is not None:\n",
    "            print(f\"      Recall:       {agg.get('mean_val_recall', 0):.4f} ± {agg.get('std_val_recall', 0):.4f}\")\n",
    "        \n",
    "        # Display per-class metrics if available\n",
    "        if fold_results and any('val_f1_class0' in f or 'val_f1_class1' in f for f in fold_results):\n",
    "            class0_f1 = [f.get('val_f1_class0') for f in fold_results if f.get('val_f1_class0') is not None]\n",
    "            class1_f1 = [f.get('val_f1_class1') for f in fold_results if f.get('val_f1_class1') is not None]\n",
    "            if class0_f1:\n",
    "                print(f\"      Class 0 F1:    {np.mean(class0_f1):.4f} ± {np.std(class0_f1):.4f}\")\n",
    "            if class1_f1:\n",
    "                print(f\"      Class 1 F1:    {np.mean(class1_f1):.4f} ± {np.std(class1_f1):.4f}\")\n",
    "else:\n",
    "    print(f\"  [WARN] No DuckDB metrics found\")\n",
    "    duckdb_metrics = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3/6: Loading MLflow Tracking Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:29:16.108527Z",
     "iopub.status.busy": "2025-12-20T15:29:16.108458Z",
     "iopub.status.idle": "2025-12-20T15:29:18.419482Z",
     "shell.execute_reply": "2025-12-20T15:29:18.418974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/6] Loading MLflow tracking data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Loaded MLflow data\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[3/6] Loading MLflow tracking data...\")\n",
    "model_type = MODEL_TYPE_MAPPING.get(model_id)\n",
    "if model_type:\n",
    "    mlflow_data = load_mlflow_metrics_by_model_type(model_type, \"mlruns/\", project_root)\n",
    "    if mlflow_data and isinstance(mlflow_data, dict) and \"message\" not in mlflow_data:\n",
    "        print(f\"  ✓ Loaded MLflow data\")\n",
    "        if \"runs\" in mlflow_data:\n",
    "            print(f\"    - {len(mlflow_data['runs'])} MLflow runs\")\n",
    "    else:\n",
    "        print(f\"  [WARN] No MLflow data found\")\n",
    "else:\n",
    "    mlflow_data = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4/6: Extracting Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:29:18.420912Z",
     "iopub.status.busy": "2025-12-20T15:29:18.420835Z",
     "iopub.status.idle": "2025-12-20T15:29:18.423189Z",
     "shell.execute_reply": "2025-12-20T15:29:18.422883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/6] Extracting training curves...\n",
      "  ✓ Found 5 fold directories\n",
      "    ✓ Found 2 plot files in fold_2\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[4/6] Extracting training curves...\")\n",
    "fold_dirs = sorted(model_path.glob(\"fold_*\"))\n",
    "if fold_dirs:\n",
    "    print(f\"  ✓ Found {len(fold_dirs)} fold directories\")\n",
    "    for fold_dir in fold_dirs[1:]:\n",
    "        png_files = list(fold_dir.glob(\"*.png\"))\n",
    "        if png_files:\n",
    "            print(f\"    ✓ Found {len(png_files)} plot files in {fold_dir.name}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5/6: Generating Comprehensive Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:29:18.424375Z",
     "iopub.status.busy": "2025-12-20T15:29:18.424296Z",
     "iopub.status.idle": "2025-12-20T15:29:18.509305Z",
     "shell.execute_reply": "2025-12-20T15:29:18.508941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5/6] Generating comprehensive visualizations...\n",
      "  ✓ Found ROC/PR curve files\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "display_roc_pr_curve_images() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m curve_files:\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ✓ Found ROC/PR curve files\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[43mdisplay_roc_pr_curve_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurve_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mROC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     display_roc_pr_curve_images(curve_files, model_name, \u001b[33m\"\u001b[39m\u001b[33mPrecision-Recall\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m duckdb_metrics \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mfold_results\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m duckdb_metrics:\n",
      "\u001b[31mTypeError\u001b[39m: display_roc_pr_curve_images() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "print(\"\\n[5/6] Generating comprehensive visualizations...\")\n",
    "curve_files = find_roc_pr_curve_files(model_id, project_root, MODEL_TYPE_MAPPING)\n",
    "if curve_files:\n",
    "    print(f\"  ✓ Found ROC/PR curve files\")\n",
    "    display_roc_pr_curve_images(curve_files, model_name, \"ROC\")\n",
    "    display_roc_pr_curve_images(curve_files, model_name, \"Precision-Recall\")\n",
    "    \n",
    "if duckdb_metrics and \"fold_results\" in duckdb_metrics:\n",
    "    print(f\"  ✓ Plotting validation metrics across folds...\")\n",
    "    fig = plot_validation_metrics_across_folds(duckdb_metrics, model_name)\n",
    "    if fig:\n",
    "        plt.show()  # Display the plot in notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6/6: Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:29:18.510587Z",
     "iopub.status.busy": "2025-12-20T15:29:18.510523Z",
     "iopub.status.idle": "2025-12-20T15:29:18.512783Z",
     "shell.execute_reply": "2025-12-20T15:29:18.512331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[6/6] Performance Summary\n",
      "======================================================================\n",
      "\n",
      "DuckDB Aggregated Metrics:\n",
      "  Mean Val F1:     0.7601 ± 0.1101\n",
      "  Mean Val Acc:    0.7583 ± 0.1115\n",
      "\n",
      "======================================================================\n",
      "Analysis complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[6/6] Performance Summary\")\n",
    "print(\"=\" * 70)\n",
    "    \n",
    "if duckdb_metrics and \"aggregated\" in duckdb_metrics:\n",
    "    agg = duckdb_metrics[\"aggregated\"]\n",
    "    print(\"\\nDuckDB Aggregated Metrics:\")\n",
    "    print(f\"  Mean Val F1:     {agg.get('mean_val_f1', 0):.4f} ± {agg.get('std_val_f1', 0):.4f}\")\n",
    "    print(f\"  Mean Val Acc:    {agg.get('mean_val_acc', 0):.4f} ± {agg.get('std_val_acc', 0):.4f}\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Analysis complete!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Analysis\n",
    "\n",
    "Execute the cells below sequentially to perform the complete analysis. The analysis will:\n",
    "\n",
    "1. Load results and metrics from multiple data sources\n",
    "2. Query DuckDB for comprehensive fold-wise statistics\n",
    "3. Attempt to load MLflow tracking data (if available)\n",
    "4. Extract and visualize training curves from all folds\n",
    "5. Generate comprehensive visualizations (ROC/PR curves, validation metrics, analysis plots)\n",
    "6. Display detailed performance summary with aggregated statistics\n",
    "\n",
    "**Note**: Execution may take a few moments as it loads data from multiple sources, generates visualizations, and performs statistical aggregations. All plots will be displayed inline in the notebook.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
