{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5alpha: sklearn LogisticRegression\n",
    "\n",
    "This notebook demonstrates the sklearn LogisticRegression model for deepfake video detection.\n",
    "\n",
    "## Model Overview\n",
    "\n",
    "sklearn LogisticRegression with L1/L2/ElasticNet regularization. Uses handcrafted features from Stage 2/4. This is a standalone implementation separate from the pipeline's logistic regression.\n",
    "\n",
    "## Training Instructions\n",
    "\n",
    "To train this model, run:\n",
    "\n",
    "```bash\n",
    "sbatch scripts/slurm_jobs/slurm_stage5alpha.sh\n",
    "```\n",
    "\n",
    "Or use Python:\n",
    "\n",
    "```python\n",
    "from src.scripts.train_sklearn_logreg import train_sklearn_logreg\n",
    "\n",
    "results = train_sklearn_logreg(\n",
    "    project_root=\".\",\n",
    "    scaled_metadata_path=\"data/stage3/scaled_metadata.parquet\",\n",
    "    features_stage2_path=\"data/stage2/features_metadata.parquet\",\n",
    "    features_stage4_path=None,\n",
    "    output_dir=\"data/stage5/sklearn_logreg\",\n",
    "    n_splits=5,\n",
    "    delete_existing=False\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Deep-Dive\n",
    "\n",
    "**sklearn_logreg** architecture details.\n",
    "\n",
    "See model implementation in `lib/training/` for specific architecture code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Configuration\n",
    "\n",
    "Hyperparameters configured in `lib/training/grid_search.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLOps Integration\n",
    "\n",
    "### Experiment Tracking with MLflow\n",
    "\n",
    "This model integrates with MLflow for comprehensive experiment tracking:\n",
    "\n",
    "```python\n",
    "from lib.mlops.mlflow_tracker import create_mlflow_tracker\n",
    "\n",
    "# MLflow automatically tracks:\n",
    "# - Hyperparameters (learning_rate, batch_size, etc.)\n",
    "# - Metrics (train_loss, val_acc, test_f1, etc.)\n",
    "# - Model artifacts (checkpoints, configs)\n",
    "# - Run metadata (tags, timestamps, fold numbers)\n",
    "```\n",
    "\n",
    "**Access MLflow UI**:\n",
    "```bash\n",
    "mlflow ui --port 5000\n",
    "# Open http://localhost:5000\n",
    "```\n",
    "\n",
    "### DuckDB Analytics\n",
    "\n",
    "Query training results with SQL for fast analytics:\n",
    "\n",
    "```python\n",
    "from lib.utils.duckdb_analytics import DuckDBAnalytics\n",
    "\n",
    "analytics = DuckDBAnalytics()\n",
    "analytics.register_parquet('results', 'data/stage5/{model_type}/metrics.json')\n",
    "result = analytics.query(\"\"\"\n",
    "    SELECT \n",
    "        fold,\n",
    "        AVG(test_f1) as avg_f1,\n",
    "        STDDEV(test_f1) as std_f1\n",
    "    FROM results\n",
    "    GROUP BY fold\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "### Airflow Orchestration\n",
    "\n",
    "Pipeline orchestrated via Apache Airflow DAG (`airflow/dags/fvc_pipeline_dag.py`):\n",
    "- **Dependency Management**: Automatic task ordering\n",
    "- **Retry Logic**: Automatic retries on failure\n",
    "- **Monitoring**: Web UI for pipeline status\n",
    "- **Scheduling**: Cron-based scheduling support\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Methodology\n",
    "\n",
    "### 5-Fold Stratified Cross-Validation\n",
    "\n",
    "- **Purpose**: Robust performance estimates, prevents overfitting\n",
    "- **Stratification**: Ensures class balance in each fold\n",
    "- **Evaluation**: Metrics averaged across folds with standard deviation\n",
    "- **Rationale**: More reliable than single train/test split\n",
    "\n",
    "### Regularization Strategy\n",
    "\n",
    "- **Weight Decay (L2)**: 1e-4 (PyTorch models)\n",
    "- **Dropout**: 0.5 in classification heads (PyTorch models)\n",
    "- **Early Stopping**: Patience=5 epochs (prevents overfitting)\n",
    "- **Gradient Clipping**: max_norm=1.0 (prevents exploding gradients)\n",
    "- **Class Weights**: Balanced sampling for imbalanced datasets\n",
    "\n",
    "### Optimization\n",
    "\n",
    "- **Optimizer**: AdamW with betas=(0.9, 0.999)\n",
    "- **Mixed Precision**: AMP (Automatic Mixed Precision) for memory efficiency\n",
    "- **Gradient Accumulation**: Dynamic based on batch size (maintains effective batch size)\n",
    "- **Learning Rate Schedule**: Cosine annealing with warmup (2 epochs)\n",
    "- **Differential Learning Rates**: Lower LR for pretrained backbones (5e-6) vs heads (5e-4)\n",
    "\n",
    "### Data Pipeline\n",
    "\n",
    "- **Video Loading**: Frame-by-frame decoding (50x memory reduction)\n",
    "- **Augmentation**: Pre-generated augmentations (reproducible, fast)\n",
    "- **Scaling**: Fixed 256x256 max dimension with letterboxing\n",
    "- **Frame Sampling**: Uniform sampling across video duration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Rationale\n",
    "\n",
    "See master pipeline notebook (`00_MASTER_PIPELINE_JOURNEY.ipynb`) for comprehensive design rationale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Deep-Dive\n",
    "\n",
    "**sklearn_logreg** architecture details.\n",
    "\n",
    "See model implementation in `lib/training/` for specific architecture code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Configuration\n",
    "\n",
    "Hyperparameters configured in `lib/training/grid_search.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLOps Integration\n",
    "\n",
    "### Experiment Tracking with MLflow\n",
    "\n",
    "This model integrates with MLflow for comprehensive experiment tracking:\n",
    "\n",
    "```python\n",
    "from lib.mlops.mlflow_tracker import create_mlflow_tracker\n",
    "\n",
    "# MLflow automatically tracks:\n",
    "# - Hyperparameters (learning_rate, batch_size, etc.)\n",
    "# - Metrics (train_loss, val_acc, test_f1, etc.)\n",
    "# - Model artifacts (checkpoints, configs)\n",
    "# - Run metadata (tags, timestamps, fold numbers)\n",
    "```\n",
    "\n",
    "**Access MLflow UI**:\n",
    "```bash\n",
    "mlflow ui --port 5000\n",
    "# Open http://localhost:5000\n",
    "```\n",
    "\n",
    "### DuckDB Analytics\n",
    "\n",
    "Query training results with SQL for fast analytics:\n",
    "\n",
    "```python\n",
    "from lib.utils.duckdb_analytics import DuckDBAnalytics\n",
    "\n",
    "analytics = DuckDBAnalytics()\n",
    "analytics.register_parquet('results', 'data/stage5/{model_type}/metrics.json')\n",
    "result = analytics.query(\"\"\"\n",
    "    SELECT \n",
    "        fold,\n",
    "        AVG(test_f1) as avg_f1,\n",
    "        STDDEV(test_f1) as std_f1\n",
    "    FROM results\n",
    "    GROUP BY fold\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "### Airflow Orchestration\n",
    "\n",
    "Pipeline orchestrated via Apache Airflow DAG (`airflow/dags/fvc_pipeline_dag.py`):\n",
    "- **Dependency Management**: Automatic task ordering\n",
    "- **Retry Logic**: Automatic retries on failure\n",
    "- **Monitoring**: Web UI for pipeline status\n",
    "- **Scheduling**: Cron-based scheduling support\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Methodology\n",
    "\n",
    "### 5-Fold Stratified Cross-Validation\n",
    "\n",
    "- **Purpose**: Robust performance estimates, prevents overfitting\n",
    "- **Stratification**: Ensures class balance in each fold\n",
    "- **Evaluation**: Metrics averaged across folds with standard deviation\n",
    "- **Rationale**: More reliable than single train/test split\n",
    "\n",
    "### Regularization Strategy\n",
    "\n",
    "- **Weight Decay (L2)**: 1e-4 (PyTorch models)\n",
    "- **Dropout**: 0.5 in classification heads (PyTorch models)\n",
    "- **Early Stopping**: Patience=5 epochs (prevents overfitting)\n",
    "- **Gradient Clipping**: max_norm=1.0 (prevents exploding gradients)\n",
    "- **Class Weights**: Balanced sampling for imbalanced datasets\n",
    "\n",
    "### Optimization\n",
    "\n",
    "- **Optimizer**: AdamW with betas=(0.9, 0.999)\n",
    "- **Mixed Precision**: AMP (Automatic Mixed Precision) for memory efficiency\n",
    "- **Gradient Accumulation**: Dynamic based on batch size (maintains effective batch size)\n",
    "- **Learning Rate Schedule**: Cosine annealing with warmup (2 epochs)\n",
    "- **Differential Learning Rates**: Lower LR for pretrained backbones (5e-6) vs heads (5e-4)\n",
    "\n",
    "### Data Pipeline\n",
    "\n",
    "- **Video Loading**: Frame-by-frame decoding (50x memory reduction)\n",
    "- **Augmentation**: Pre-generated augmentations (reproducible, fast)\n",
    "- **Scaling**: Fixed 256x256 max dimension with letterboxing\n",
    "- **Frame Sampling**: Uniform sampling across video duration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Rationale\n",
    "\n",
    "See master pipeline notebook (`00_MASTER_PIPELINE_JOURNEY.ipynb`) for comprehensive design rationale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Video, display, HTML\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "# Add project root to path\n",
    "# Find project root by looking for lib/ directory\n",
    "import os\n",
    "current_dir = Path(os.getcwd()).resolve()\n",
    "project_root = current_dir\n",
    "\n",
    "# Walk up the directory tree to find project root (look for lib/ directory)\n",
    "for _ in range(10):  # Max 10 levels up\n",
    "    if (project_root / \"lib\").exists() and (project_root / \"lib\" / \"__init__.py\").exists():\n",
    "        break\n",
    "    parent = project_root.parent\n",
    "    if parent == project_root:  # Reached filesystem root\n",
    "        # Fallback: use current directory\n",
    "        project_root = current_dir\n",
    "        break\n",
    "    project_root = parent\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from lib.utils.paths import load_metadata_flexible\n",
    "from lib.training.metrics_utils import compute_classification_metrics\n",
    "\n",
    "# Configuration\n",
    "MODEL_TYPE = \"sklearn_logreg\"\n",
    "MODEL_DIR = project_root / \"data\" / \"stage5\" / \"sklearn_logreg\"\n",
    "SCALED_METADATA_PATH = project_root / \"data\" / \"stage3\" / \"scaled_metadata.parquet\"\n",
    "FEATURES_STAGE2_PATH = project_root / \"data\" / \"stage2\" / \"features_metadata.parquet\"\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Model directory: {MODEL_DIR}\")\n",
    "print(f\"Model directory exists: {MODEL_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Saved Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_saved_models(model_dir: Path):\n",
    "    \"\"\"Check for saved sklearn model files.\"\"\"\n",
    "    if not model_dir.exists():\n",
    "        print(f\"[X] Model directory does not exist: {model_dir}\")\n",
    "        return False, None\n",
    "    \n",
    "    # sklearn_logreg saves model directly in output_dir, not in fold subdirectories\n",
    "    model_file = model_dir / \"model.joblib\"\n",
    "    scaler_file = model_dir / \"scaler.joblib\"\n",
    "    metrics_file = model_dir / \"metrics.json\"\n",
    "    \n",
    "    if model_file.exists():\n",
    "        print(f\"[OK] Found model.joblib\")\n",
    "        if scaler_file.exists():\n",
    "            print(f\"[OK] Found scaler.joblib\")\n",
    "        if metrics_file.exists():\n",
    "            print(f\"[OK] Found metrics.json\")\n",
    "        return True, model_file\n",
    "    else:\n",
    "        print(f\"[X] No model.joblib found in {model_dir}\")\n",
    "        return False, None\n",
    "\n",
    "models_available, model_file = check_saved_models(MODEL_DIR)\n",
    "\n",
    "if not models_available:\n",
    "    print(\"\\n[WARN]  No trained models found. Please train the model first using the instructions above.\")\n",
    "    print(f\"Expected location: {MODEL_DIR / 'model.joblib'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_available:\n",
    "    print(f\"Loading model from: {model_file}\")\n",
    "    \n",
    "    model = joblib.load(model_file)\n",
    "    print(f\"[OK] Model loaded successfully\")\n",
    "    print(f\"Model type: {type(model)}\")\n",
    "    \n",
    "    # Load scaler if available\n",
    "    scaler = None\n",
    "    scaler_file = MODEL_DIR / \"scaler.joblib\"\n",
    "    if scaler_file.exists():\n",
    "        scaler = joblib.load(scaler_file)\n",
    "        print(f\"[OK] Scaler loaded\")\n",
    "    \n",
    "    # Load metadata\n",
    "    scaled_df = load_metadata_flexible(str(SCALED_METADATA_PATH))\n",
    "    features_df = load_metadata_flexible(str(FEATURES_STAGE2_PATH))\n",
    "    \n",
    "    if scaled_df is not None and features_df is not None:\n",
    "        print(f\"\\n[OK] Loaded {scaled_df.height} videos from scaled metadata\")\n",
    "        print(f\"[OK] Loaded {features_df.height} feature rows from Stage 2\")\n",
    "        \n",
    "        # Get sample videos\n",
    "        sample_videos = scaled_df.head(5).to_pandas()\n",
    "        print(f\"\\nSample videos for demonstration:\")\n",
    "        print(sample_videos[[\"video_path\", \"label\"]].to_string())\n",
    "    else:\n",
    "        print(\"[WARN]  Could not load metadata files\")\n",
    "else:\n",
    "    print(\"[WARN]  Skipping model loading - no trained models found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Sample Videos and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_available and 'model' in locals() and 'sample_videos' in locals():\n",
    "    # Create a simple visualization\n",
    "    fig, axes = plt.subplots(1, min(3, len(sample_videos)), figsize=(15, 5))\n",
    "    if len(sample_videos) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (ax, row) in enumerate(zip(axes, sample_videos.iterrows())):\n",
    "        video_path = project_root / row[1][\"video_path\"]\n",
    "        label = row[1][\"label\"]\n",
    "        \n",
    "        # Try to load and display video thumbnail\n",
    "        try:\n",
    "            cap = cv2.VideoCapture(str(video_path))\n",
    "            if cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if ret:\n",
    "                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    ax.imshow(frame_rgb)\n",
    "                    ax.set_title(f\"{Path(video_path).name}\\nLabel: {label}\", fontsize=10)\n",
    "                cap.release()\n",
    "        except Exception as e:\n",
    "            ax.text(0.5, 0.5, f\"Video: {Path(video_path).name}\\nLabel: {label}\", \n",
    "                    ha='center', va='center', fontsize=12, transform=ax.transAxes)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nNote: To play videos in the notebook, use:\")\n",
    "    print(\"display(Video('path/to/video.mp4', embed=True, width=640, height=480))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_available:\n",
    "    # Note: Performance metrics are not saved to JSON files\n",
    "    # Metrics are aggregated in training results returned from stage5_train_models()\n",
    "    # To view metrics, check:\n",
    "    # 1. Training logs in logs/stage5/\n",
    "    # 2. MLflow UI (if enabled): mlflow ui\n",
    "    # 3. Aggregated results returned from stage5_train_models()\n",
    "    \n",
    "    print(\"Model files available:\")\n",
    "    print(\"=\" * 50)\n",
    "    if MODEL_DIR.exists():\n",
    "        for item in sorted(MODEL_DIR.iterdir()):\n",
    "            if item.is_dir():\n",
    "                print(f\"  Directory: {item.name}\")\n",
    "                for file in sorted(item.iterdir()):\n",
    "                    print(f\"    - {file.name}\")\n",
    "            else:\n",
    "                print(f\"  File: {item.name}\")\n",
    "    else:\n",
    "        print(f\"[WARN]  Model directory not found: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Plots\n",
    "\n",
    "The following plots were generated during model training and provide insights into model performance across cross-validation folds and hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display training plots if available\n",
    "from pathlib import Path\n",
    "from IPython.display import Image, display, HTML\n",
    "\n",
    "# Define MODEL_DIR if not already defined\n",
    "if 'MODEL_DIR' not in globals():\n",
    "    import os\n",
    "    current_dir = Path(os.getcwd()).resolve()\n",
    "    project_root = current_dir\n",
    "    \n",
    "    # Walk up the directory tree to find project root (look for lib/ directory)\n",
    "    for _ in range(10):  # Max 10 levels up\n",
    "        if (project_root / \"lib\").exists() and (project_root / \"lib\" / \"__init__.py\").exists():\n",
    "            break\n",
    "        parent = project_root.parent\n",
    "        if parent == project_root:  # Reached filesystem root\n",
    "            # Fallback: use current directory\n",
    "            project_root = current_dir\n",
    "            break\n",
    "        project_root = parent\n",
    "    MODEL_TYPE = \"sklearn_logreg\"\n",
    "    MODEL_DIR = project_root / \"data\" / \"stage5\" / \"sklearn_logreg\"\n",
    "\n",
    "# sklearn_logreg saves plots in root directory, not plots/ subdirectory\n",
    "plots_dir = MODEL_DIR  # Plots are in root, not plots/ subdirectory\n",
    "\n",
    "# Check for roc_pr_curves.png in root\n",
    "roc_pr_file = MODEL_DIR / \"roc_pr_curves.png\"\n",
    "\n",
    "if roc_pr_file.exists():\n",
    "    print(f\"[OK] Found roc_pr_curves.png: {roc_pr_file}\")\n",
    "\n",
    "# List of expected plot files\n",
    "plot_files = {\n",
    "    \"roc_pr_curves.png\": \"ROC and Precision-Recall Curves\"\n",
    "}\n",
    "\n",
    "plots_found = []\n",
    "for plot_file, plot_name in plot_files.items():\n",
    "    plot_path = roc_pr_file\n",
    "    if plot_path.exists():\n",
    "        plots_found.append((plot_path, plot_name))\n",
    "        print(f\"  [OK] Found: {plot_file}\")\n",
    "\n",
    "if plots_found:\n",
    "    print(f\"\\n[PLOT] Displaying {len(plots_found)} training plot(s):\\n\")\n",
    "    for plot_path, plot_name in plots_found:\n",
    "        print(f\"\\n### {plot_name}\")\n",
    "        display(Image(str(plot_path), width=800))\n",
    "else:\n",
    "    print(\"[WARN]  No plot files found in plots directory.\")\n",
    "    print(f\"Expected plots directory: {plots_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture Summary\n",
    "\n",
    "**sklearn LogisticRegression** is a linear classifier that:\n",
    "- Uses handcrafted features from Stage 2 (noise residual, DCT statistics, blur/sharpness, codec cues)\n",
    "- Supports L1, L2, and ElasticNet regularization\n",
    "- Uses StandardScaler for feature normalization\n",
    "- Outputs probability scores for binary classification (real vs fake)\n",
    "- Trained with grid search on hyperparameters (C, penalty, solver)\n",
    "\n",
    "**Advantages:**\n",
    "- Simple and interpretable\n",
    "- Fast training and inference\n",
    "- Multiple regularization options (L1/L2/ElasticNet)\n",
    "- Good baseline for comparison\n",
    "\n",
    "**Limitations:**\n",
    "- Linear decision boundary (may not capture complex patterns)\n",
    "- Relies on quality of handcrafted features\n",
    "- No temporal modeling\n",
    "\n",
    "**Difference from 5a:** This is a standalone sklearn implementation with more regularization options, while 5a uses the pipeline's LogisticRegressionBaseline class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
