{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Workflow\n",
    "\n",
    "The analysis proceeds through six systematic steps, each handling a specific aspect of performance evaluation:\n",
    "\n",
    "1. **Loading Results**: Extract metrics from `results.json` or model-specific files\n",
    "2. **DuckDB Query**: Fast SQL-based retrieval of fold-wise metrics\n",
    "3. **MLflow Integration**: Load experiment tracking data (if available)\n",
    "4. **Training Curves**: Extract epoch-wise or iteration-wise metrics from all folds\n",
    "5. **Visualizations**: Generate comprehensive plots and charts\n",
    "6. **Performance Summary**: Display detailed metrics breakdown\n",
    "\n",
    "Each step is independent, allowing partial analysis even if some data sources are unavailable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Configuration\n",
    "\n",
    "### Standard Library Imports\n",
    "\n",
    "Essential Python standard library modules for file I/O, path manipulation, type hints, and system operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:28:56.035282Z",
     "iopub.status.busy": "2025-12-20T15:28:56.034842Z",
     "iopub.status.idle": "2025-12-20T15:28:56.041633Z",
     "shell.execute_reply": "2025-12-20T15:28:56.040659Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Any\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Root Path Setup\n",
    "\n",
    "Add project root and notebooks directory to Python path for importing project-specific utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:28:56.046277Z",
     "iopub.status.busy": "2025-12-20T15:28:56.045880Z",
     "iopub.status.idle": "2025-12-20T15:28:56.918351Z",
     "shell.execute_reply": "2025-12-20T15:28:56.917932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Project root: /Users/santoshdesai/Downloads/fvc\n",
      "✓ notebook_utils found at: /Users/santoshdesai/Downloads/fvc/src/notebooks/notebook_utils.py\n"
     ]
    }
   ],
   "source": [
    "# Resolve project root by searching for lib/ directory\n",
    "project_root = Path.cwd()\n",
    "for _ in range(10):\n",
    "    if (project_root / 'lib').exists() and (project_root / 'lib' / '__init__.py').exists():\n",
    "        break\n",
    "    parent = project_root.parent\n",
    "    if parent == project_root:\n",
    "        break\n",
    "    project_root = parent\n",
    "\n",
    "# Add paths to sys.path\n",
    "sys.path.insert(0, str(project_root))\n",
    "sys.path.insert(0, str(project_root / \"src\" / \"notebooks\"))\n",
    "\n",
    "# Verify notebook_utils can be imported\n",
    "try:\n",
    "    import notebook_utils\n",
    "    print(f\"✓ Project root: {project_root}\")\n",
    "    print(f\"✓ notebook_utils found at: {notebook_utils.__file__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠ Warning: {e}\")\n",
    "    print(f\"Project root: {project_root}\")\n",
    "    print(f\"Looking for: {project_root / 'src' / 'notebooks' / 'notebook_utils.py'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third-Party Library Imports\n",
    "\n",
    "Data science and visualization libraries for numerical computations, data manipulation, and statistical visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:28:56.932331Z",
     "iopub.status.busy": "2025-12-20T15:28:56.932178Z",
     "iopub.status.idle": "2025-12-20T15:28:56.933785Z",
     "shell.execute_reply": "2025-12-20T15:28:56.933502Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Configuration\n",
    "\n",
    "Configure matplotlib and seaborn for publication-quality plots with appropriate DPI and styling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:28:56.934975Z",
     "iopub.status.busy": "2025-12-20T15:28:56.934903Z",
     "iopub.status.idle": "2025-12-20T15:28:56.936560Z",
     "shell.execute_reply": "2025-12-20T15:28:56.936295Z"
    }
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project-Specific Utilities Import\n",
    "\n",
    "Import utility functions from `notebook_utils` module for data loading, metric extraction, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:28:56.937579Z",
     "iopub.status.busy": "2025-12-20T15:28:56.937509Z",
     "iopub.status.idle": "2025-12-20T15:28:56.939630Z",
     "shell.execute_reply": "2025-12-20T15:28:56.939296Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1957820862.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mtry:    from notebook_utils import (        get_model_data_path, load_results_json, load_mlflow_metrics_by_model_type,        query_duckdb_metrics, get_metrics_data, display_duckdb_metrics_summary, plot_training_curves, find_roc_pr_curve_files,        display_roc_pr_curve_images, plot_validation_metrics_across_folds,        MODEL_TYPE_MAPPING    )except ImportError as e:    print(f\"[ERROR] Failed to import notebook_utils: {e}\")    sys.exit(1)\u001b[39m\n                                                                                                                                                                                                                                                                                                                                                                 ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "try:    from notebook_utils import (        get_model_data_path, load_results_json, load_mlflow_metrics_by_model_type,        query_duckdb_metrics, get_metrics_data, display_duckdb_metrics_summary, plot_training_curves, find_roc_pr_curve_files,        display_roc_pr_curve_images, plot_validation_metrics_across_folds,        MODEL_TYPE_MAPPING    )except ImportError as e:    print(f\"[ERROR] Failed to import notebook_utils: {e}\")    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Root Detection Function\n",
    "\n",
    "Locate the project root directory by searching for the `lib/` folder marker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:28:56.940652Z",
     "iopub.status.busy": "2025-12-20T15:28:56.940589Z",
     "iopub.status.idle": "2025-12-20T15:28:56.942431Z",
     "shell.execute_reply": "2025-12-20T15:28:56.942092Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_project_root() -> Path:\n",
    "    \"\"\"Find project root directory.\"\"\"\n",
    "    current = Path.cwd()\n",
    "    for _ in range(10):\n",
    "        if (current / \"lib\").exists() and (current / \"lib\" / \"__init__.py\").exists():\n",
    "            return current\n",
    "        parent = current.parent\n",
    "        if parent == current:\n",
    "            break\n",
    "        current = parent\n",
    "    return Path.cwd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Project Root\n",
    "\n",
    "Call the project root detection function and display the resolved path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:28:56.943282Z",
     "iopub.status.busy": "2025-12-20T15:28:56.943221Z",
     "iopub.status.idle": "2025-12-20T15:28:56.944937Z",
     "shell.execute_reply": "2025-12-20T15:28:56.944587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/santoshdesai/Downloads/fvc\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ROOT = get_project_root()\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Workflow\n",
    "\n",
    "The analysis proceeds through six systematic steps, each handling a specific aspect of performance evaluation:\n",
    "\n",
    "1. **Loading Results**: Extract metrics from `results.json` or model-specific files\n",
    "2. **DuckDB Query**: Fast SQL-based retrieval of fold-wise metrics\n",
    "3. **MLflow Integration**: Load experiment tracking data (if available)\n",
    "4. **Training Curves**: Extract epoch-wise or iteration-wise metrics from all folds\n",
    "5. **Visualizations**: Generate comprehensive plots and charts\n",
    "6. **Performance Summary**: Display detailed metrics breakdown\n",
    "\n",
    "Each step is independent, allowing partial analysis even if some data sources are unavailable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1/6: Loading Results from results.json\n",
    "\n",
    "Load results from `results.json` which contains best hyperparameters found during grid search, best validation F1 score, and cross-validation results with mean and standard deviation metrics across folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:28:56.945778Z",
     "iopub.status.busy": "2025-12-20T15:28:56.945709Z",
     "iopub.status.idle": "2025-12-20T15:28:56.947375Z",
     "shell.execute_reply": "2025-12-20T15:28:56.947125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Sklearn Logistic Regression (5alpha) - Comprehensive Performance Analysis\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize analysis variables\n",
    "project_root = get_project_root()\n",
    "model_id = \"5alpha\"\n",
    "model_name = \"Sklearn Logistic Regression\"\n",
    "print(\"=\" * 70)\n",
    "print(\"Sklearn Logistic Regression (5alpha) - Comprehensive Performance Analysis\")\n",
    "print(\"=\" * 70)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:28:56.948212Z",
     "iopub.status.busy": "2025-12-20T15:28:56.948159Z",
     "iopub.status.idle": "2025-12-20T15:28:57.034112Z",
     "shell.execute_reply": "2025-12-20T15:28:57.033704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/6] Loading results.json...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_model_data_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[1/6] Loading results.json...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model_path = \u001b[43mget_model_data_path\u001b[49m(model_id, project_root, MODEL_TYPE_MAPPING)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_path:\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[ERROR] Model path not found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'get_model_data_path' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"[1/6] Loading results.json...\")\n",
    "model_path = get_model_data_path(model_id, project_root, MODEL_TYPE_MAPPING)\n",
    "if not model_path:\n",
    "    print(f\"[ERROR] Model path not found for {model_id}\")\n",
    "    \n",
    "results = load_results_json(model_id, project_root, MODEL_TYPE_MAPPING)\n",
    "if results:\n",
    "    print(f\"  ✓ Loaded results.json\")\n",
    "    if \"best_val_f1\" in results:\n",
    "        print(f\"    - Best Val F1: {results['best_val_f1']:.4f}\")\n",
    "    if \"cv_val_f1\" in results:\n",
    "        print(f\"    - CV Val F1: {results['cv_val_f1']:.4f} ± {results.get('cv_val_f1_std', 0):.4f}\")\n",
    "else:\n",
    "    print(f\"  [WARN] No results.json found\")\n",
    "    results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2/6: Querying DuckDB for Fold-wise Metrics\n",
    "\n",
    "Query the DuckDB analytics database for fast SQL-based retrieval of fold-wise metrics and aggregated statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:28:57.035173Z",
     "iopub.status.busy": "2025-12-20T15:28:57.035105Z",
     "iopub.status.idle": "2025-12-20T15:28:57.037163Z",
     "shell.execute_reply": "2025-12-20T15:28:57.036835Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1899669095.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(\"\\n[2/6] Querying DuckDB for fold-wise metrics...\")metrics_data = get_metrics_data(model_id, project_root)if metrics_data:    display_duckdb_metrics_summary(metrics_data, model_id)    # Convert to dict for backward compatibility with rest of notebook    duckdb_metrics = metrics_data.to_dict()else:    duckdb_metrics = {}\u001b[39m\n                                                             ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[2/6] Querying DuckDB for fold-wise metrics...\")metrics_data = get_metrics_data(model_id, project_root)if metrics_data:    display_duckdb_metrics_summary(metrics_data, model_id)    # Convert to dict for backward compatibility with rest of notebook    duckdb_metrics = metrics_data.to_dict()else:    duckdb_metrics = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3/6: Loading MLflow Tracking Data\n",
    "\n",
    "Attempt to load MLflow experiment tracking data. Note: Baseline sklearn models typically don't use MLflow tracking, so this step will return an informative message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:28:57.038312Z",
     "iopub.status.busy": "2025-12-20T15:28:57.038246Z",
     "iopub.status.idle": "2025-12-20T15:28:57.048173Z",
     "shell.execute_reply": "2025-12-20T15:28:57.047673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/6] Loading MLflow tracking data...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MODEL_TYPE_MAPPING' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[3/6] Loading MLflow tracking data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model_type = \u001b[43mMODEL_TYPE_MAPPING\u001b[49m.get(model_id)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_type:\n\u001b[32m      4\u001b[39m     mlflow_data = load_mlflow_metrics_by_model_type(model_type, \u001b[33m\"\u001b[39m\u001b[33mmlruns/\u001b[39m\u001b[33m\"\u001b[39m, project_root)\n",
      "\u001b[31mNameError\u001b[39m: name 'MODEL_TYPE_MAPPING' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n[3/6] Loading MLflow tracking data...\")\n",
    "model_type = MODEL_TYPE_MAPPING.get(model_id)\n",
    "if model_type:\n",
    "    mlflow_data = load_mlflow_metrics_by_model_type(model_type, \"mlruns/\", project_root)\n",
    "    if mlflow_data and isinstance(mlflow_data, dict) and \"message\" not in mlflow_data:\n",
    "        print(f\"  ✓ Loaded MLflow data\")\n",
    "    else:\n",
    "        print(f\"  [INFO] Baseline models don't use MLflow tracking\")\n",
    "else:\n",
    "    mlflow_data = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4/6: Extracting Training Curves\n",
    "\n",
    "Check for ROC/PR curves at the root level. Sklearn models may not have fold-specific metrics.jsonl files, but typically have visualization plots at the model root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:28:57.049275Z",
     "iopub.status.busy": "2025-12-20T15:28:57.049213Z",
     "iopub.status.idle": "2025-12-20T15:28:57.058649Z",
     "shell.execute_reply": "2025-12-20T15:28:57.058251Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/6] Extracting training curves...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[4/6] Extracting training curves...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m roc_pr_file = \u001b[43mmodel_path\u001b[49m / \u001b[33m\"\u001b[39m\u001b[33mroc_pr_curves.png\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m roc_pr_file.exists():\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ✓ Found ROC/PR curves at root level\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model_path' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n[4/6] Extracting training curves...\")\n",
    "roc_pr_file = model_path / \"roc_pr_curves.png\"\n",
    "if roc_pr_file.exists():\n",
    "    print(f\"  ✓ Found ROC/PR curves at root level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5/6: Generating Comprehensive Visualizations\n",
    "\n",
    "Generate visualizations including ROC/PR curves and validation metrics across folds if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:28:57.059662Z",
     "iopub.status.busy": "2025-12-20T15:28:57.059608Z",
     "iopub.status.idle": "2025-12-20T15:28:57.061420Z",
     "shell.execute_reply": "2025-12-20T15:28:57.061085Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3518120784.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(\"\\n[5/6] Generating comprehensive visualizations...\")curve_files = find_roc_pr_curve_files(model_id, project_root, MODEL_TYPE_MAPPING)if curve_files:    print(f\"  ✓ Found ROC/PR curve files\")    display_roc_pr_curve_images(curve_files, model_name)    if duckdb_metrics and \"fold_results\" in duckdb_metrics:    print(f\"  ✓ Plotting validation metrics across folds...\")    fig = plot_validation_metrics_across_folds(duckdb_metrics, model_name)    if fig:        plt.show()  # Display the plot in notebook\u001b[39m\n                                                               ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[5/6] Generating comprehensive visualizations...\")curve_files = find_roc_pr_curve_files(model_id, project_root, MODEL_TYPE_MAPPING)if curve_files:    print(f\"  ✓ Found ROC/PR curve files\")    display_roc_pr_curve_images(curve_files, model_name)    if duckdb_metrics and \"fold_results\" in duckdb_metrics:    print(f\"  ✓ Plotting validation metrics across folds...\")    fig = plot_validation_metrics_across_folds(duckdb_metrics, model_name)    if fig:        plt.show()  # Display the plot in notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6/6: Performance Summary\n",
    "\n",
    "Display comprehensive performance summary including best hyperparameters, cross-validation results, and aggregated metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:28:57.062305Z",
     "iopub.status.busy": "2025-12-20T15:28:57.062244Z",
     "iopub.status.idle": "2025-12-20T15:28:57.072639Z",
     "shell.execute_reply": "2025-12-20T15:28:57.072265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[6/6] Performance Summary\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[6/6] Performance Summary\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mresults\u001b[49m:\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCross-Validation Results:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mbest_val_f1\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "\u001b[31mNameError\u001b[39m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n[6/6] Performance Summary\")\n",
    "print(\"=\" * 70)\n",
    "    \n",
    "if results:\n",
    "    print(\"\\nCross-Validation Results:\")\n",
    "    if \"best_val_f1\" in results:\n",
    "        print(f\"  Best Val F1:        {results['best_val_f1']:.4f}\")\n",
    "    if \"cv_val_f1\" in results:\n",
    "        print(f\"  CV Val F1:          {results['cv_val_f1']:.4f} ± {results.get('cv_val_f1_std', 0):.4f}\")\n",
    "    if \"cv_val_auc\" in results:\n",
    "        print(f\"  CV Val AUC:         {results['cv_val_auc']:.4f} ± {results.get('cv_val_auc_std', 0):.4f}\")\n",
    "    if \"best_params\" in results:\n",
    "        print(f\"  Best Parameters:    {results['best_params']}\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Analysis complete!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Analysis\n",
    "\n",
    "Execute the cells below sequentially to perform the complete analysis. The analysis will:\n",
    "\n",
    "1. Load results and metrics from multiple data sources\n",
    "2. Query DuckDB for comprehensive fold-wise statistics\n",
    "3. Attempt to load MLflow tracking data (if available)\n",
    "4. Extract and visualize training curves from all folds\n",
    "5. Generate comprehensive visualizations (ROC/PR curves, validation metrics, analysis plots)\n",
    "6. Display detailed performance summary with aggregated statistics\n",
    "\n",
    "**Note**: Execution may take a few moments as it loads data from multiple sources, generates visualizations, and performs statistical aggregations. All plots will be displayed inline in the notebook.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
