{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5c: Naive CNN\n",
    "\n",
    "This notebook demonstrates the Naive CNN model for deepfake video detection.\n",
    "\n",
    "## Model Overview\n",
    "\n",
    "Naive CNN is a simple 3D convolutional neural network that processes video frames directly. It uses 3D convolutions to capture spatiotemporal patterns in videos.\n",
    "\n",
    "## Training Instructions\n",
    "\n",
    "To train this model, run:\n",
    "\n",
    "```bash\n",
    "sbatch src/scripts/slurm_stage5c.sh\n",
    "```\n",
    "\n",
    "Or use Python:\n",
    "\n",
    "```python\n",
    "from lib.training.pipeline import stage5_train_models\n",
    "\n",
    "results = stage5_train_models(\n",
    "    project_root=\".\",\n",
    "    scaled_metadata_path=\"data/stage3/scaled_metadata.parquet\",\n",
    "    features_stage2_path=None,\n",
    "    features_stage4_path=None,\n",
    "    model_types=[\"naive_cnn\"],\n",
    "    n_splits=5,\n",
    "    num_frames=500,  # Reduced for memory efficiency\n",
    "    output_dir=\"data/stage5\",\n",
    "    use_tracking=True,\n",
    "    use_mlflow=True\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Deep-Dive\n\n",
    "**Naive CNN** processes video frames independently using 2D convolutions, then aggregates frame-level predictions.\n\n",
    "### Architecture Details\n\n",
    "**Input**: (N, C, T, H, W) or (N, T, C, H, W) video tensors\n",
    "- N: batch size\n",
    "- C: channels (3 for RGB)\n",
    "- T: temporal frames (up to 1000)\n",
    "- H, W: spatial dimensions (256x256 after scaling)\n\n",
    "**Processing Pipeline**:\n",
    "1. **Frame Reshaping**: (N, T, C, H, W) → (N×T, C, H, W)\n",
    "2. **Chunked Processing**: Process 10 frames at a time to avoid OOM\n",
    "3. **2D CNN Layers**:\n",
    "   - Conv2d(3→32) + BatchNorm + ReLU + MaxPool(2)\n",
    "   - Conv2d(32→64) + BatchNorm + ReLU + MaxPool(2)\n",
    "   - Conv2d(64→128) + BatchNorm + ReLU + AdaptiveAvgPool(1,1)\n",
    "4. **Classification Head**:\n",
    "   - Linear(128→64) + ReLU + Dropout(0.5)\n",
    "   - Linear(64→2) for binary classification\n",
    "5. **Temporal Aggregation**: Average frame predictions for video-level output\n\n",
    "### Implementation Code\n\n",
    "**Location**: `lib/training/_cnn.py`\n\n",
    "```python\n",
    "class NaiveCNNBaseline(nn.Module):\n",
    "    def __init__(self, num_frames: int = 1000, num_classes: int = 2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Process frames in chunks, average predictions\n",
    "        ...\n",
    "```\n\n",
    "### Memory Optimization\n\n",
    "- **Chunked Processing**: 10 frames per chunk (prevents OOM)\n",
    "- **Batch Size**: 1 (processes 1000 frames per video)\n",
    "- **Gradient Accumulation**: 16 steps (effective batch size = 16)\n",
    "- **Initialization**: He initialization for ReLU activations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Configuration\n\n",
    "**Training Hyperparameters** (from `lib/training/grid_search.py`):\n\n",
    "- **learning_rate**: 0.0005\n",
    "- **weight_decay**: 0.0001\n",
    "- **batch_size**: 1\n",
    "- **num_epochs**: 25\n",
    "\n**Rationale**:\n",
    "- **Batch Size 1**: Memory-constrained (processes up to 1000 frames per video)\n",
    "- **Single Hyperparameter Combination**: Reduced from 5+ combinations for training efficiency\n",
    "- **Gradient Accumulation**: Maintains effective batch size despite small batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLOps Integration\n\n",
    "### Experiment Tracking with MLflow\n\n",
    "This model integrates with MLflow for comprehensive experiment tracking:\n\n",
    "```python\n",
    "from lib.mlops.mlflow_tracker import create_mlflow_tracker\n",
    "\n",
    "# MLflow automatically tracks:\n",
    "# - Hyperparameters (learning_rate, batch_size, etc.)\n",
    "# - Metrics (train_loss, val_acc, test_f1, etc.)\n",
    "# - Model artifacts (checkpoints, configs)\n",
    "# - Run metadata (tags, timestamps, fold numbers)\n",
    "```\n\n",
    "**Access MLflow UI**:\n",
    "```bash\n",
    "mlflow ui --port 5000\n",
    "# Open http://localhost:5000\n",
    "```\n\n",
    "### DuckDB Analytics\n\n",
    "Query training results with SQL for fast analytics:\n\n",
    "```python\n",
    "from lib.utils.duckdb_analytics import DuckDBAnalytics\n",
    "\n",
    "analytics = DuckDBAnalytics()\n",
    "analytics.register_parquet('results', 'data/stage5/{model_type}/metrics.json')\n",
    "result = analytics.query(\"\"\"\n",
    "    SELECT \n",
    "        fold,\n",
    "        AVG(test_f1) as avg_f1,\n",
    "        STDDEV(test_f1) as std_f1\n",
    "    FROM results\n",
    "    GROUP BY fold\n",
    "\"\"\")\n",
    "```\n\n",
    "### Airflow Orchestration\n\n",
    "Pipeline orchestrated via Apache Airflow DAG (`airflow/dags/fvc_pipeline_dag.py`):\n",
    "- **Dependency Management**: Automatic task ordering\n",
    "- **Retry Logic**: Automatic retries on failure\n",
    "- **Monitoring**: Web UI for pipeline status\n",
    "- **Scheduling**: Cron-based scheduling support\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Methodology\n\n",
    "### 5-Fold Stratified Cross-Validation\n\n",
    "- **Purpose**: Robust performance estimates, prevents overfitting\n",
    "- **Stratification**: Ensures class balance in each fold\n",
    "- **Evaluation**: Metrics averaged across folds with standard deviation\n",
    "- **Rationale**: More reliable than single train/test split\n\n",
    "### Regularization Strategy\n\n",
    "- **Weight Decay (L2)**: 1e-4 (PyTorch models)\n",
    "- **Dropout**: 0.5 in classification heads (PyTorch models)\n",
    "- **Early Stopping**: Patience=5 epochs (prevents overfitting)\n",
    "- **Gradient Clipping**: max_norm=1.0 (prevents exploding gradients)\n",
    "- **Class Weights**: Balanced sampling for imbalanced datasets\n\n",
    "### Optimization\n\n",
    "- **Optimizer**: AdamW with betas=(0.9, 0.999)\n",
    "- **Mixed Precision**: AMP (Automatic Mixed Precision) for memory efficiency\n",
    "- **Gradient Accumulation**: Dynamic based on batch size (maintains effective batch size)\n",
    "- **Learning Rate Schedule**: Cosine annealing with warmup (2 epochs)\n",
    "- **Differential Learning Rates**: Lower LR for pretrained backbones (5e-6) vs heads (5e-4)\n\n",
    "### Data Pipeline\n\n",
    "- **Video Loading**: Frame-by-frame decoding (50x memory reduction)\n",
    "- **Augmentation**: Pre-generated augmentations (reproducible, fast)\n",
    "- **Scaling**: Fixed 256x256 max dimension with letterboxing\n",
    "- **Frame Sampling**: Uniform sampling across video duration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Rationale\n\n",
    "### Why \"Naive\" CNN?\n\n",
    "- **Baseline Purpose**: Simple 2D CNN establishes baseline for video models\n",
    "- **Frame-Independent**: Processes each frame independently (no temporal modeling)\n",
    "- **Memory Efficient**: Chunked processing handles long videos (1000 frames)\n",
    "- **Comparison Point**: Demonstrates benefit of temporal models (3D CNNs, Transformers)\n\n",
    "### Trade-offs\n\n",
    "- **No Temporal Modeling**: Loses temporal relationships between frames\n",
    "- **Simple Architecture**: May underfit complex patterns\n",
    "- **Chunked Processing**: Adds complexity but necessary for memory constraints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Deep-Dive\n\n",
    "**Naive CNN** processes video frames independently using 2D convolutions, then aggregates frame-level predictions.\n\n",
    "### Architecture Details\n\n",
    "**Input**: (N, C, T, H, W) or (N, T, C, H, W) video tensors\n",
    "- N: batch size\n",
    "- C: channels (3 for RGB)\n",
    "- T: temporal frames (up to 1000)\n",
    "- H, W: spatial dimensions (256x256 after scaling)\n\n",
    "**Processing Pipeline**:\n",
    "1. **Frame Reshaping**: (N, T, C, H, W) → (N×T, C, H, W)\n",
    "2. **Chunked Processing**: Process 10 frames at a time to avoid OOM\n",
    "3. **2D CNN Layers**:\n",
    "   - Conv2d(3→32) + BatchNorm + ReLU + MaxPool(2)\n",
    "   - Conv2d(32→64) + BatchNorm + ReLU + MaxPool(2)\n",
    "   - Conv2d(64→128) + BatchNorm + ReLU + AdaptiveAvgPool(1,1)\n",
    "4. **Classification Head**:\n",
    "   - Linear(128→64) + ReLU + Dropout(0.5)\n",
    "   - Linear(64→2) for binary classification\n",
    "5. **Temporal Aggregation**: Average frame predictions for video-level output\n\n",
    "### Implementation Code\n\n",
    "**Location**: `lib/training/_cnn.py`\n\n",
    "```python\n",
    "class NaiveCNNBaseline(nn.Module):\n",
    "    def __init__(self, num_frames: int = 1000, num_classes: int = 2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Process frames in chunks, average predictions\n",
    "        ...\n",
    "```\n\n",
    "### Memory Optimization\n\n",
    "- **Chunked Processing**: 10 frames per chunk (prevents OOM)\n",
    "- **Batch Size**: 1 (processes 1000 frames per video)\n",
    "- **Gradient Accumulation**: 16 steps (effective batch size = 16)\n",
    "- **Initialization**: He initialization for ReLU activations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Configuration\n\n",
    "**Training Hyperparameters** (from `lib/training/grid_search.py`):\n\n",
    "- **learning_rate**: 0.0005\n",
    "- **weight_decay**: 0.0001\n",
    "- **batch_size**: 1\n",
    "- **num_epochs**: 25\n",
    "\n**Rationale**:\n",
    "- **Batch Size 1**: Memory-constrained (processes up to 1000 frames per video)\n",
    "- **Single Hyperparameter Combination**: Reduced from 5+ combinations for training efficiency\n",
    "- **Gradient Accumulation**: Maintains effective batch size despite small batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLOps Integration\n\n",
    "### Experiment Tracking with MLflow\n\n",
    "This model integrates with MLflow for comprehensive experiment tracking:\n\n",
    "```python\n",
    "from lib.mlops.mlflow_tracker import create_mlflow_tracker\n",
    "\n",
    "# MLflow automatically tracks:\n",
    "# - Hyperparameters (learning_rate, batch_size, etc.)\n",
    "# - Metrics (train_loss, val_acc, test_f1, etc.)\n",
    "# - Model artifacts (checkpoints, configs)\n",
    "# - Run metadata (tags, timestamps, fold numbers)\n",
    "```\n\n",
    "**Access MLflow UI**:\n",
    "```bash\n",
    "mlflow ui --port 5000\n",
    "# Open http://localhost:5000\n",
    "```\n\n",
    "### DuckDB Analytics\n\n",
    "Query training results with SQL for fast analytics:\n\n",
    "```python\n",
    "from lib.utils.duckdb_analytics import DuckDBAnalytics\n",
    "\n",
    "analytics = DuckDBAnalytics()\n",
    "analytics.register_parquet('results', 'data/stage5/{model_type}/metrics.json')\n",
    "result = analytics.query(\"\"\"\n",
    "    SELECT \n",
    "        fold,\n",
    "        AVG(test_f1) as avg_f1,\n",
    "        STDDEV(test_f1) as std_f1\n",
    "    FROM results\n",
    "    GROUP BY fold\n",
    "\"\"\")\n",
    "```\n\n",
    "### Airflow Orchestration\n\n",
    "Pipeline orchestrated via Apache Airflow DAG (`airflow/dags/fvc_pipeline_dag.py`):\n",
    "- **Dependency Management**: Automatic task ordering\n",
    "- **Retry Logic**: Automatic retries on failure\n",
    "- **Monitoring**: Web UI for pipeline status\n",
    "- **Scheduling**: Cron-based scheduling support\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Methodology\n\n",
    "### 5-Fold Stratified Cross-Validation\n\n",
    "- **Purpose**: Robust performance estimates, prevents overfitting\n",
    "- **Stratification**: Ensures class balance in each fold\n",
    "- **Evaluation**: Metrics averaged across folds with standard deviation\n",
    "- **Rationale**: More reliable than single train/test split\n\n",
    "### Regularization Strategy\n\n",
    "- **Weight Decay (L2)**: 1e-4 (PyTorch models)\n",
    "- **Dropout**: 0.5 in classification heads (PyTorch models)\n",
    "- **Early Stopping**: Patience=5 epochs (prevents overfitting)\n",
    "- **Gradient Clipping**: max_norm=1.0 (prevents exploding gradients)\n",
    "- **Class Weights**: Balanced sampling for imbalanced datasets\n\n",
    "### Optimization\n\n",
    "- **Optimizer**: AdamW with betas=(0.9, 0.999)\n",
    "- **Mixed Precision**: AMP (Automatic Mixed Precision) for memory efficiency\n",
    "- **Gradient Accumulation**: Dynamic based on batch size (maintains effective batch size)\n",
    "- **Learning Rate Schedule**: Cosine annealing with warmup (2 epochs)\n",
    "- **Differential Learning Rates**: Lower LR for pretrained backbones (5e-6) vs heads (5e-4)\n\n",
    "### Data Pipeline\n\n",
    "- **Video Loading**: Frame-by-frame decoding (50x memory reduction)\n",
    "- **Augmentation**: Pre-generated augmentations (reproducible, fast)\n",
    "- **Scaling**: Fixed 256x256 max dimension with letterboxing\n",
    "- **Frame Sampling**: Uniform sampling across video duration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Rationale\n\n",
    "### Why \"Naive\" CNN?\n\n",
    "- **Baseline Purpose**: Simple 2D CNN establishes baseline for video models\n",
    "- **Frame-Independent**: Processes each frame independently (no temporal modeling)\n",
    "- **Memory Efficient**: Chunked processing handles long videos (1000 frames)\n",
    "- **Comparison Point**: Demonstrates benefit of temporal models (3D CNNs, Transformers)\n\n",
    "### Trade-offs\n\n",
    "- **No Temporal Modeling**: Loses temporal relationships between frames\n",
    "- **Simple Architecture**: May underfit complex patterns\n",
    "- **Chunked Processing**: Adds complexity but necessary for memory constraints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Video, display, HTML\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from lib.training.model_factory import create_model\n",
    "from lib.mlops.config import RunConfig\n",
    "from lib.utils.paths import load_metadata_flexible\n",
    "from lib.training.metrics_utils import compute_classification_metrics\n",
    "\n",
    "# Configuration\n",
    "MODEL_TYPE = \"naive_cnn\"\n",
    "MODEL_DIR = project_root / \"data\" / \"stage5\" / MODEL_TYPE\n",
    "SCALED_METADATA_PATH = project_root / \"data\" / \"stage3\" / \"scaled_metadata.parquet\"\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Model directory: {MODEL_DIR}\")\n",
    "print(f\"Model directory exists: {MODEL_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Saved Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_saved_models(model_dir: Path):\n",
    "    \"\"\"Check for saved PyTorch model files.\"\"\"\n",
    "    if not model_dir.exists():\n",
    "        print(f\"❌ Model directory does not exist: {model_dir}\")\n",
    "        return False, []\n",
    "    \n",
    "    fold_dirs = sorted([d for d in model_dir.iterdir() if d.is_dir() and d.name.startswith(\"fold_\")])\n",
    "    \n",
    "    if not fold_dirs:\n",
    "        print(f\"❌ No fold directories found in {model_dir}\")\n",
    "        return False, []\n",
    "    \n",
    "    print(f\"✓ Found {len(fold_dirs)} fold(s)\")\n",
    "    \n",
    "    models_found = []\n",
    "    for fold_dir in fold_dirs:\n",
    "        model_file = fold_dir / \"model.pt\"\n",
    "        if model_file.exists():\n",
    "            models_found.append((fold_dir.name, model_file))\n",
    "            print(f\"  ✓ {fold_dir.name}: Found model.pt\")\n",
    "        else:\n",
    "            print(f\"  ❌ {fold_dir.name}: No model.pt found\")\n",
    "    \n",
    "    return len(models_found) > 0, models_found\n",
    "\n",
    "models_available, model_files = check_saved_models(MODEL_DIR)\n",
    "\n",
    "if not models_available:\n",
    "    print(\"\\n⚠️  No trained models found. Please train the model first using the instructions above.\")\n",
    "    print(f\"Expected location: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_available:\n",
    "    # Create model instance\n",
    "    config = RunConfig(\n",
    "        run_id=\"demo\",\n",
    "        experiment_name=\"demo\",\n",
    "        model_type=MODEL_TYPE,\n",
    "        num_frames=500\n",
    "    )\n",
    "    \n",
    "    model = create_model(MODEL_TYPE, config)\n",
    "    \n",
    "    # Load weights\n",
    "    fold_name, model_path = model_files[0]\n",
    "    print(f\"Loading model weights from: {model_path}\")\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "    \n",
    "    model.eval()\n",
    "    print(f\"✓ Model loaded successfully from {fold_name}\")\n",
    "    print(f\"Model architecture: {type(model).__name__}\")\n",
    "    \n",
    "    # Load metadata\n",
    "    scaled_df = load_metadata_flexible(str(SCALED_METADATA_PATH))\n",
    "    \n",
    "    if scaled_df is not None:\n",
    "        print(f\"\\n✓ Loaded {scaled_df.height} videos from scaled metadata\")\n",
    "        sample_videos = scaled_df.head(5).to_pandas()\n",
    "        print(f\"\\nSample videos for demonstration:\")\n",
    "        print(sample_videos[[\"video_path\", \"label\"]].to_string())\n",
    "    else:\n",
    "        print(\"⚠️  Could not load metadata files\")\n",
    "else:\n",
    "    print(\"⚠️  Skipping model loading - no trained models found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Sample Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_available and 'sample_videos' in locals():\n",
    "    fig, axes = plt.subplots(1, min(3, len(sample_videos)), figsize=(15, 5))\n",
    "    if len(sample_videos) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (ax, row) in enumerate(zip(axes, sample_videos.iterrows())):\n",
    "        video_path = project_root / row[1][\"video_path\"]\n",
    "        label = row[1][\"label\"]\n",
    "        \n",
    "        try:\n",
    "            import cv2\n",
    "            cap = cv2.VideoCapture(str(video_path))\n",
    "            if cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if ret:\n",
    "                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    ax.imshow(frame_rgb)\n",
    "                    ax.set_title(f\"{Path(video_path).name}\\nLabel: {label}\", fontsize=10)\n",
    "                cap.release()\n",
    "        except Exception as e:\n",
    "            ax.text(0.5, 0.5, f\"Video: {Path(video_path).name}\\nLabel: {label}\", \n",
    "                    ha='center', va='center', fontsize=12, transform=ax.transAxes)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nNote: To play videos in the notebook, use:\")\n",
    "    print(\"display(Video('path/to/video.mp4', embed=True, width=640, height=480))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_available:\n",
    "    fold_dir = model_files[0][0]\n",
    "    metrics_file = MODEL_DIR / fold_dir / \"metrics.json\"\n",
    "    \n",
    "    if metrics_file.exists():\n",
    "        with open(metrics_file, 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "        \n",
    "        print(\"Model Performance Metrics:\")\n",
    "        print(\"=\" * 50)\n",
    "        for key, value in metrics.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                print(f\"{key}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"{key}: {value}\")\n",
    "        \n",
    "        if 'accuracy' in metrics or 'f1_score' in metrics:\n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "            metric_names = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "            metric_values = [metrics.get(m, 0) for m in metric_names]\n",
    "            \n",
    "            bars = ax.bar(metric_names, metric_values, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "            ax.set_ylabel('Score')\n",
    "            ax.set_title('Naive CNN Model Performance')\n",
    "            ax.set_ylim(0, 1)\n",
    "            \n",
    "            for bar, val in zip(bars, metric_values):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{val:.3f}', ha='center', va='bottom')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"⚠️  Metrics file not found.\")\n",
    "        print(f\"Expected: {metrics_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture Summary\n",
    "\n",
    "**Naive CNN** is a simple 3D convolutional neural network that:\n",
    "- Processes raw video frames directly (no feature extraction)\n",
    "- Uses 3D convolutions to capture spatiotemporal patterns\n",
    "- Processes 500 frames at 256x256 resolution\n",
    "- End-to-end trainable architecture\n",
    "\n",
    "**Advantages:**\n",
    "- Learns features automatically from raw video\n",
    "- Captures temporal relationships through 3D convolutions\n",
    "- No manual feature engineering required\n",
    "\n",
    "**Limitations:**\n",
    "- Very memory intensive (requires batch_size=1)\n",
    "- Simple architecture may not capture complex patterns\n",
    "- Long training time due to processing many frames"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}