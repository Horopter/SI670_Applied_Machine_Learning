"""
Common utility functions for notebook analysis.

This module provides shared functions for all notebooks to avoid code duplication.
"""

from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
try:
    from typing import Protocol
except ImportError:
    # Python < 3.8 compatibility
    from typing_extensions import Protocol
from dataclasses import dataclass, field
import json
import re

import numpy as np
import pandas as pd
from datetime import datetime
import time
from collections import Counter, defaultdict
import tempfile
import os


# Optional imports for IPython display
try:
    from IPython.display import Image, display
    IPYTHON_AVAILABLE = True
except ImportError:
    IPYTHON_AVAILABLE = False
    Image = None
    display = None

# Optional imports for plotting functions
try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    PLOTTING_AVAILABLE = True
except ImportError:
    PLOTTING_AVAILABLE = False
    plt = None
    sns = None

# Model type mapping (shared across all notebooks)
MODEL_TYPE_MAPPING = {
    "5a": "logistic_regression",
    "5alpha": "sklearn_logreg",
    "5b": "svm",
    "5beta": "gradient_boosting/xgboost",
    "5f": "xgboost_pretrained_inception",
    "5g": "xgboost_i3d",
    "5h": "xgboost_r2plus1d"
}

# Mapping from notebook model_type to MLflow model_type tag
# MLflow uses different tag names than the data directory structure
MLFLOW_MODEL_TYPE_MAPPING = {
    "xgboost_pretrained_inception": "pretrained_inception",
    "xgboost_i3d": "x3d",
    "xgboost_r2plus1d": "slowfast",
    # For models that use the same name in both
    "logistic_regression": "logistic_regression",
    "svm": "svm",
    "sklearn_logreg": "sklearn_logreg",
    "gradient_boosting/xgboost": "gradient_boosting/xgboost"
}

# Mapping from model_id to MLflow experiment ID
# This allows explicit mapping of notebooks to MLflow experiments
MLFLOW_EXPERIMENT_ID_MAPPING = {
    "5f": "448523649298154796",  # xgboost_pretrained_inception
    "5g": "609328694875670898",  # xgboost_i3d
    "5h": "825185598273956279"   # xgboost_r2plus1d
}


def extract_training_times_comprehensive(
    log_file: Path, 
    model_id: str
) -> Dict[str, Any]:
    """
    Extract comprehensive training times, per-fold durations, and memory statistics from log file.
    
    Args:
        log_file: Path to log file
        model_id: Model identifier
    
    Returns:
        Dictionary with training time information
    """
    if not log_file.exists():
        return {}
    
    try:
        with open(log_file, 'r') as f:
            lines = f.readlines()
            content = ''.join(lines)
    except Exception as e:
        print(f"[ERROR] Failed to read log file {log_file}: {e}")
        return {}
    
    times = {}
    fold_times = []
    fold_start_times = {}
    
    # Extract execution time with minutes
    try:
        exec_time_match = re.search(
            r'Execution time:\s+([\d.]+)\s+seconds\s+\(([\d.]+)\s+minutes?\)', 
            content
        )
        if exec_time_match:
            times['total_seconds'] = float(exec_time_match.group(1))
            times['total_minutes'] = float(exec_time_match.group(2))
        else:
            # Fallback to seconds only
            exec_time_match = re.search(r'Execution time:\s+([\d.]+)\s+seconds', content)
            if exec_time_match:
                times['total_seconds'] = float(exec_time_match.group(1))
                times['total_minutes'] = times['total_seconds'] / 60.0
    except Exception as e:
        print(f"[WARN] Failed to parse execution time: {e}")
    
    # Extract per-fold completion times with timestamps
    fold_pattern = re.compile(r'Fold\s+(\d+)\s+-\s+Val\s+Loss:')
    timestamp_pattern = re.compile(r'(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2})')
    
    for i, line in enumerate(lines):
        # Look for fold start (training begins)
        fold_start_match = re.search(
            r'(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2}).*?Training baseline model.*?fold\s+(\d+)',
            line, re.IGNORECASE
        )
        if fold_start_match:
            timestamp_str = fold_start_match.group(1)
            fold_num = int(fold_start_match.group(2))
            try:
                fold_start_times[fold_num] = datetime.strptime(
                    timestamp_str, '%Y-%m-%d %H:%M:%S'
                )
            except ValueError:
                pass
        
        # Look for fold completion
        fold_match = fold_pattern.search(line)
        if fold_match:
            fold_num = int(fold_match.group(1))
            timestamp_match = timestamp_pattern.search(line)
            timestamp = None
            timestamp_dt = None
            
            if timestamp_match:
                timestamp = timestamp_match.group(1)
            try:
                timestamp_dt = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')
            except ValueError:
                pass
            
            # Calculate duration if we have start time
            duration_seconds = None
            if fold_num in fold_start_times and timestamp_dt:
                duration_seconds = (timestamp_dt - fold_start_times[fold_num]).total_seconds()
            
            fold_times.append({
            'fold': fold_num,
            'timestamp': timestamp,
            'timestamp_dt': timestamp_dt,
            'duration_seconds': duration_seconds,
            'line_number': i + 1
            })
    
    if fold_times:
        # Sort by fold number
        fold_times = sorted(fold_times, key=lambda x: x['fold'])
        times['fold_times'] = fold_times
        
        # Calculate per-fold durations if available
        if any(ft.get('duration_seconds') for ft in fold_times):
            times['per_fold_durations'] = {
            ft['fold']: ft['duration_seconds'] 
            for ft in fold_times 
            if ft.get('duration_seconds') is not None
            }
    
    # Extract memory statistics
    memory_before = {}
    memory_after = {}
    
    for line in lines:
        if 'Memory stats (Stage 5: before training)' in line:
            mem_match = re.search(r'\{([^}]+)\}', line)
            if mem_match:
                try:
                    stats_str = '{' + mem_match.group(1) + '}'
                    cpu_gb_match = re.search(r"'cpu_memory_gb':\s+([\d.]+)", stats_str)
                    if cpu_gb_match:
                        memory_before['cpu_memory_gb'] = float(cpu_gb_match.group(1))
                    cpu_mb_match = re.search(r"'cpu_memory_mb':\s+([\d.]+)", stats_str)
                    if cpu_mb_match:
                        memory_before['cpu_memory_mb'] = float(cpu_mb_match.group(1))
                    gpu_total_match = re.search(r"'gpu_total_gb':\s+([\d.]+)", stats_str)
                    if gpu_total_match:
                        memory_before['gpu_total_gb'] = float(gpu_total_match.group(1))
                except Exception:
                    pass
        
        if 'Memory stats (Stage 5: after training)' in line:
            mem_match = re.search(r'\{([^}]+)\}', line)
            if mem_match:
                try:
                    stats_str = '{' + mem_match.group(1) + '}'
                    cpu_gb_match = re.search(r"'cpu_memory_gb':\s+([\d.]+)", stats_str)
                    if cpu_gb_match:
                        memory_after['cpu_memory_gb'] = float(cpu_gb_match.group(1))
                    cpu_mb_match = re.search(r"'cpu_memory_mb':\s+([\d.]+)", stats_str)
                    if cpu_mb_match:
                        memory_after['cpu_memory_mb'] = float(cpu_mb_match.group(1))
                    gpu_total_match = re.search(r"'gpu_total_gb':\s+([\d.]+)", stats_str)
                    if gpu_total_match:
                        memory_after['gpu_total_gb'] = float(gpu_total_match.group(1))
                except Exception:
                    pass
    
    if memory_before:
        times['memory_before'] = memory_before
    if memory_after:
        times['memory_after'] = memory_after
    
    return times


def extract_mlflow_run_ids_from_log(
    log_file: Path,
    model_id: str
) -> List[str]:
    """
    Extract MLflow run IDs (UUIDs) from training log file.
    
    MLflow run IDs are typically logged during training and can be found
    in log messages like "MLflow run ID: <uuid>" or "Run ID: <uuid>".
    
    Args:
        log_file: Path to log file
        model_id: Model identifier
    
    Returns:
        List of MLflow run IDs (UUIDs) found in the log
    """
    if not log_file.exists():
        return []
    
    run_ids = []
    
    try:
        with open(log_file, 'r') as f:
            content = f.read()
    except Exception as e:
        print(f"[ERROR] Failed to read log file {log_file}: {e}")
        return []
    
    # Pattern for MLflow run ID (32 hex characters)
    uuid_pattern = re.compile(
        r'(?:mlflow|run_id|experiment).*?([0-9a-f]{32})',
        re.IGNORECASE
    )
    
    # Also check for standard UUID format (8-4-4-4-12)
    uuid_standard_pattern = re.compile(
        r'([0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})',
        re.IGNORECASE
    )
    
    # Find all matches
    for match in uuid_pattern.finditer(content):
        run_id = match.group(1)
        if run_id not in run_ids:
            run_ids.append(run_id)
    
    # Also check for standard UUID format
    for match in uuid_standard_pattern.finditer(content):
        run_id = match.group(1).replace('-', '')
        if len(run_id) == 32 and run_id not in run_ids:
            run_ids.append(run_id)
    
    # Check for run IDs in mlruns path format
    mlruns_pattern = re.compile(
        r'mlruns[/\\][0-9]+[/\\]([0-9a-f]{32})',
        re.IGNORECASE
    )
    for match in mlruns_pattern.finditer(content):
        run_id = match.group(1)
        if run_id not in run_ids:
            run_ids.append(run_id)
    
    return run_ids


def load_mlflow_metrics_by_model_type(
    model_type: str,
    mlruns_path: str = "mlruns/",
    project_root: Optional[Path] = None,
    experiment_id: Optional[str] = None
) -> Optional[Dict[str, Any]]:
    """
    Load metrics from MLflow using model_type and fold tags.
    
    Note: MLflow doesn't use job_id tags. Instead, it uses model_type and fold tags.
    This function loads all runs for a given model_type and aggregates them.
    Automatically converts model_type to MLflow tag using MLFLOW_MODEL_TYPE_MAPPING.
    
    Args:
        model_type: Model type (e.g., "xgboost_pretrained_inception", "logistic_regression", "svm")
        mlruns_path: Path to mlruns directory
        project_root: Project root directory
        experiment_id: Optional experiment ID to filter by (if provided, only loads runs from this experiment)
    
    Returns:
        Dictionary with MLflow metrics or None if not found
    """
    if project_root is None:
        project_root = get_project_root()
    
    # Convert model_type to MLflow tag if mapping exists
    mlflow_model_type = MLFLOW_MODEL_TYPE_MAPPING.get(model_type, model_type)
    
    mlruns_dir = project_root / mlruns_path
    if not mlruns_dir.exists():
        return None
    
    try:
        experiments = sorted([
            d for d in mlruns_dir.iterdir() 
            if d.is_dir() and d.name.isdigit()
        ])
    except Exception as e:
        print(f"[ERROR] Failed to list experiments in {mlruns_dir}: {e}")
        return None
    
    # Filter by experiment_id if provided
    if experiment_id:
        experiments = [exp_dir for exp_dir in experiments if exp_dir.name == str(experiment_id)]
        if not experiments:
            return None
    
    all_runs_data = []
    
    for exp_dir in experiments:
        try:
            # MLflow uses UUID directories, not 'run_*' directories
            # Filter for directories that have a 'tags' subdirectory (actual runs)
            runs = sorted([
            d for d in exp_dir.iterdir() 
            if d.is_dir() and not d.name.startswith('.') and (d / "tags").exists()
            ])
        except Exception as e:
            continue
        
        for run_dir in runs:
            tags_dir = run_dir / "tags"
            if not tags_dir.exists():
                continue
            
            # Check model_type tag
            model_type_tag = tags_dir / "model_type"
            if not model_type_tag.exists():
                continue
            
            try:
                with open(model_type_tag) as f:
                    tag_value = f.read().strip()
                if tag_value != mlflow_model_type:
                    continue
            except Exception as e:
                continue
            
            # Load metrics, params, and tags
            metrics = {}
            params = {}
            tags = {}
            
            # Load metrics
            metrics_dir = run_dir / "metrics"
            if metrics_dir.exists():
                try:
                    for metric_file in metrics_dir.iterdir():
                        if metric_file.is_file():
                            try:
                                with open(metric_file) as f:
                                    lines = f.readlines()
                                    if lines:
                                        values = [
                                            float(line.split()[1]) 
                                            for line in lines 
                                            if line.strip()
                                        ]
                                        if values:
                                            metrics[metric_file.name] = {
                                                'values': values,
                                                'latest': values[-1]
                                            }
                            except Exception:
                                pass
                except Exception:
                    pass
            
            # Load params
            params_dir = run_dir / "params"
            if params_dir.exists():
                try:
                    for param_file in params_dir.iterdir():
                        if param_file.is_file():
                            try:
                                with open(param_file) as f:
                                    params[param_file.name] = f.read().strip()
                            except Exception:
                                pass
                except Exception:
                    pass
            
            # Load tags
            try:
                for tag_file in tags_dir.iterdir():
                    if tag_file.is_file():
                        try:
                            with open(tag_file) as f:
                                tags[tag_file.name] = f.read().strip()
                        except Exception:
                            pass
            except Exception:
                pass
            
            all_runs_data.append({
            'experiment_id': exp_dir.name,
            'run_id': run_dir.name,
            'metrics': metrics,
            'params': params,
            'tags': tags
            })
    
    if not all_runs_data:
        # Baseline sklearn models (logistic_regression, svm, sklearn_logreg) don't log to MLflow
        # This is expected behavior - only deep learning models use MLflow
        baseline_models = ["logistic_regression", "svm", "sklearn_logreg"]
        if model_type in baseline_models:
            # Return empty dict instead of None to indicate "no data but expected"
            return {"runs": [], "message": f"Baseline model {model_type} does not use MLflow tracking"}
        return None
    
    # Separate CV fold runs: grid search CV (20% data) vs final training CV (100% data)
    # Grid search runs have both "fold" tag AND "param_combination" tag
    # Final training CV runs have "fold" tag but NO "param_combination" tag
    grid_search_cv_runs = []
    final_training_cv_runs = []
    final_runs = []
    
    for run_data in all_runs_data:
        has_fold_tag = 'fold' in run_data.get('tags', {})
        has_param_combo = 'param_combination' in run_data.get('tags', {})
        
        if has_fold_tag and has_param_combo:
            # Grid search CV fold (20% data for hyperparameter tuning)
            grid_search_cv_runs.append(run_data)
        elif has_fold_tag and not has_param_combo:
            # Final training CV fold (100% data, using best hyperparameters)
            final_training_cv_runs.append(run_data)
        else:
            # Final run (no fold tag) - single run on full dataset (if any)
            final_runs.append(run_data)
    
    # For backward compatibility, combine all CV runs
    cv_runs = grid_search_cv_runs + final_training_cv_runs
    
    # Aggregate CV metrics separately (per fold, don't take mean across final run)
    cv_aggregated_metrics = {}
    for run_data in cv_runs:
        for metric_name, metric_info in run_data['metrics'].items():
            if metric_name not in cv_aggregated_metrics:
                cv_aggregated_metrics[metric_name] = {
                    'values': [],
                    'runs': []
                }
            cv_aggregated_metrics[metric_name]['values'].extend(metric_info['values'])
            cv_aggregated_metrics[metric_name]['runs'].append({
                'run_id': run_data['run_id'],
                'fold': run_data.get('tags', {}).get('fold', 'unknown'),
                'latest': metric_info['latest']
            })
    
    # Calculate CV aggregated stats (for display, but keep per-fold data separate)
    for metric_name in cv_aggregated_metrics:
        values = cv_aggregated_metrics[metric_name]['values']
        if values:
            cv_aggregated_metrics[metric_name]['mean'] = np.mean(values)
            cv_aggregated_metrics[metric_name]['std'] = np.std(values)
            cv_aggregated_metrics[metric_name]['min'] = np.min(values)
            cv_aggregated_metrics[metric_name]['max'] = np.max(values)
            cv_aggregated_metrics[metric_name]['latest'] = values[-1]
    
    # Aggregate grid search CV metrics separately
    grid_search_cv_aggregated_metrics = {}
    for run_data in grid_search_cv_runs:
        for metric_name, metric_info in run_data['metrics'].items():
            if metric_name not in grid_search_cv_aggregated_metrics:
                grid_search_cv_aggregated_metrics[metric_name] = {
                    'values': [],
                    'runs': []
                }
            grid_search_cv_aggregated_metrics[metric_name]['values'].extend(metric_info['values'])
            grid_search_cv_aggregated_metrics[metric_name]['runs'].append({
                'run_id': run_data['run_id'],
                'fold': run_data.get('tags', {}).get('fold', 'unknown'),
                'param_combination': run_data.get('tags', {}).get('param_combination', 'unknown'),
                'latest': metric_info['latest']
            })
    
    # Calculate grid search CV aggregated stats
    for metric_name in grid_search_cv_aggregated_metrics:
        values = grid_search_cv_aggregated_metrics[metric_name]['values']
        if values:
            grid_search_cv_aggregated_metrics[metric_name]['mean'] = np.mean(values)
            grid_search_cv_aggregated_metrics[metric_name]['std'] = np.std(values)
            grid_search_cv_aggregated_metrics[metric_name]['min'] = np.min(values)
            grid_search_cv_aggregated_metrics[metric_name]['max'] = np.max(values)
            grid_search_cv_aggregated_metrics[metric_name]['latest'] = values[-1]
    
    # Aggregate final training CV metrics separately
    final_training_cv_aggregated_metrics = {}
    for run_data in final_training_cv_runs:
        for metric_name, metric_info in run_data['metrics'].items():
            if metric_name not in final_training_cv_aggregated_metrics:
                final_training_cv_aggregated_metrics[metric_name] = {
                    'values': [],
                    'runs': []
                }
            final_training_cv_aggregated_metrics[metric_name]['values'].extend(metric_info['values'])
            final_training_cv_aggregated_metrics[metric_name]['runs'].append({
                'run_id': run_data['run_id'],
                'fold': run_data.get('tags', {}).get('fold', 'unknown'),
                'latest': metric_info['latest']
            })
    
    # Calculate final training CV aggregated stats
    for metric_name in final_training_cv_aggregated_metrics:
        values = final_training_cv_aggregated_metrics[metric_name]['values']
        if values:
            final_training_cv_aggregated_metrics[metric_name]['mean'] = np.mean(values)
            final_training_cv_aggregated_metrics[metric_name]['std'] = np.std(values)
            final_training_cv_aggregated_metrics[metric_name]['min'] = np.min(values)
            final_training_cv_aggregated_metrics[metric_name]['max'] = np.max(values)
            final_training_cv_aggregated_metrics[metric_name]['latest'] = values[-1]
    
    # Aggregate final run metrics separately
    final_aggregated_metrics = {}
    for run_data in final_runs:
        for metric_name, metric_info in run_data['metrics'].items():
            if metric_name not in final_aggregated_metrics:
                final_aggregated_metrics[metric_name] = {
                    'values': [],
                    'runs': []
                }
            final_aggregated_metrics[metric_name]['values'].extend(metric_info['values'])
            final_aggregated_metrics[metric_name]['runs'].append({
                'run_id': run_data['run_id'],
                'latest': metric_info['latest']
            })
    
    # Calculate final run aggregated stats
    for metric_name in final_aggregated_metrics:
        values = final_aggregated_metrics[metric_name]['values']
        if values:
            final_aggregated_metrics[metric_name]['mean'] = np.mean(values)
            final_aggregated_metrics[metric_name]['std'] = np.std(values)
            final_aggregated_metrics[metric_name]['min'] = np.min(values)
            final_aggregated_metrics[metric_name]['max'] = np.max(values)
            final_aggregated_metrics[metric_name]['latest'] = values[-1]
    
    # Use params from first run (should be consistent)
    aggregated_params = all_runs_data[0]['params'] if all_runs_data else {}
    
    return {
        'experiment_id': all_runs_data[0]['experiment_id'] if all_runs_data else None,
        'run_ids': [r['run_id'] for r in all_runs_data],
        'cv_runs': cv_runs,  # All CV runs (backward compatibility)
        'grid_search_cv_runs': grid_search_cv_runs,  # Grid search CV (20% data)
        'final_training_cv_runs': final_training_cv_runs,  # Final training CV (100% data)
        'final_runs': final_runs,  # Final run data
        'cv_metrics': cv_aggregated_metrics,  # All CV aggregated metrics (backward compatibility)
        'grid_search_cv_metrics': grid_search_cv_aggregated_metrics,  # Grid search CV metrics
        'final_training_cv_metrics': final_training_cv_aggregated_metrics,  # Final training CV metrics
        'final_metrics': final_aggregated_metrics,  # Final run metrics
        'metrics': cv_aggregated_metrics,  # Backward compatibility - CV metrics
        'params': aggregated_params,
        'tags': all_runs_data[0]['tags'] if all_runs_data else {},
        'num_runs': len(all_runs_data),
        'num_cv_runs': len(cv_runs),
        'num_grid_search_cv_runs': len(grid_search_cv_runs),
        'num_final_training_cv_runs': len(final_training_cv_runs),
        'num_final_runs': len(final_runs)
    }


def supplement_mlflow_with_duckdb_losses(
    mlflow_data: Dict[str, Any],
    model_id: str,
    project_root: Path,
    model_type_mapping: Optional[Dict[str, str]] = None
) -> Dict[str, Any]:
    """
    Supplement MLflow data with train/validation loss from DuckDB or metrics.jsonl
    if not present in MLflow.
    
    Args:
        mlflow_data: MLflow data dictionary from load_mlflow_metrics_by_model_type
        model_id: Model identifier
        project_root: Project root directory
        model_type_mapping: Optional model type mapping dict
    
    Returns:
        Updated MLflow data with supplemented train/val loss metrics
    """
    # Handle None or invalid input
    if not mlflow_data or not isinstance(mlflow_data, dict):
        return mlflow_data
    
    if model_type_mapping is None:
        model_type_mapping = MODEL_TYPE_MAPPING
    
    model_type = model_type_mapping.get(model_id)
    if not model_type:
        return mlflow_data
    
    # Check if train_loss or val_loss are missing in MLflow
    # Handle both new format (with cv_runs/final_runs) and old format
    cv_runs = mlflow_data.get('cv_runs', [])
    final_runs = mlflow_data.get('final_runs', [])
    
    # If old format, try to extract runs from the data structure
    if not cv_runs and not final_runs:
        # Old format - return as-is, can't supplement without run structure
        return mlflow_data
    
    # Get DuckDB metrics
    duckdb_metrics = get_metrics_data(model_id, project_root)
    
    # Supplement CV runs
    for run_data in cv_runs:
        fold = run_data.get('tags', {}).get('fold')
        if fold and duckdb_metrics:
            fold_metrics = duckdb_metrics.get_fold_metrics(int(fold))
            if fold_metrics:
                metrics = run_data.get('metrics', {})
                if 'val_loss' not in metrics and fold_metrics.val_loss is not None:
                    metrics['val_loss'] = {
                        'values': [fold_metrics.val_loss],
                        'latest': fold_metrics.val_loss
                    }
    
    # Supplement final runs
    for run_data in final_runs:
        if duckdb_metrics and duckdb_metrics.aggregated:
            agg = duckdb_metrics.aggregated
            metrics = run_data.get('metrics', {})
            if 'val_loss' not in metrics and agg.mean_val_acc is not None:
                # Use aggregated metrics if available
                pass  # Final runs typically don't need supplementation
    
    return mlflow_data


def extract_hyperparameters_from_metrics(
    metrics: Dict[str, Any]
) -> Optional[Dict[str, Any]]:
    """
    Extract hyperparameters from metrics.json.
    
    Tries best_hyperparameters first, then aggregates from fold_results.
    
    Args:
        metrics: Metrics dictionary from metrics.json
    
    Returns:
        Dictionary of hyperparameters or None
    """
    if not metrics or not isinstance(metrics, dict):
        return None
    
    # First try best_hyperparameters
    if 'best_hyperparameters' in metrics:
        return metrics['best_hyperparameters']
    
    if 'best_params' in metrics:
        return metrics['best_params']
    
    # Fallback: aggregate from fold_results
    fold_results = metrics.get('fold_results', []) or metrics.get('cv_fold_results', [])
    if not fold_results:
        return None
    
    # Collect hyperparameters from all folds
    hyperparams = {}
    for fold_data in fold_results:
        if not isinstance(fold_data, dict):
            continue
        
        # Common hyperparameter keys to look for
        for key in ['C', 'max_iter', 'learning_rate', 'batch_size', 'num_epochs', 
                   'weight_decay', 'gamma', 'kernel', 'n_estimators', 'max_depth']:
            if key in fold_data:
                if key not in hyperparams:
                    hyperparams[key] = []
                value = fold_data[key]
                if value is not None:
                    hyperparams[key].append(value)
    
    # If we found hyperparameters, return the most common value or mean
    if hyperparams:
        result = {}
        for key, values in hyperparams.items():
            if values:
                # For numeric values, use mean; for strings, use most common
                if all(isinstance(v, (int, float)) for v in values):
                    result[key] = np.mean(values)
                else:
                    # Most common value
                    from collections import Counter
                    result[key] = Counter(values).most_common(1)[0][0]
        return result
    
    return None


def get_latest_job_ids(
    project_root: Optional[Path] = None
) -> Dict[str, str]:
    """
    Dynamically find latest job IDs from log files.
    
    Scans logs/stage5/ for log files matching pattern
    stage5{suffix}_{job_id}.log and returns the most recent
    job ID for each model based on file modification time.
    
    Args:
        project_root: Project root directory. If None, attempts
            to find project root by looking for lib/ directory.
    
    Returns:
        Dictionary mapping model_id to latest job_id string.
        Example: {"5a": "38451621", "5alpha": "38451622", ...}
        Returns empty dict if logs directory not found.
    """
    if project_root is None:
        project_root = get_project_root()
    
    logs_dir = project_root / "logs" / "stage5"
    if not logs_dir.exists():
        return {}
    
    # Model suffix mapping
    model_suffixes = {
        "5a": "a",
        "5alpha": "alpha",
        "5b": "b",
        "5beta": "beta",
        "5f": "f",
        "5g": "g",
        "5h": "h"
    }
    
    latest_job_ids = {}
    
    for model_id, suffix in model_suffixes.items():
        pattern = f"stage5{suffix}_*.log"
        log_files = list(logs_dir.glob(pattern))
        
        if not log_files:
            continue
        
        # Extract job IDs and find latest by modification time
        job_ids = []
        for log_file in log_files:
            # Extract job_id from filename: stage5{suffix}_{job_id}.log
            match = re.search(rf'stage5{suffix}_(\d+)\.log', log_file.name)
            if match:
                job_id = match.group(1)
                try:
                    # Use modification time to determine latest
                    mtime = log_file.stat().st_mtime
                    job_ids.append((job_id, mtime))
                except OSError:
                    # Skip if file stat fails
                    continue
        
        if job_ids:
            # Sort by modification time (most recent first)
            job_ids.sort(key=lambda x: x[1], reverse=True)
            latest_job_ids[model_id] = job_ids[0][0]
    
    return latest_job_ids


def get_model_data_path(
    model_id: str,
    project_root: Path,
    model_type_mapping: Optional[Dict[str, str]] = None
) -> Optional[Path]:
    """
    Get data directory path for model.
    
    Args:
        model_id: Model identifier.
        project_root: Project root directory.
        model_type_mapping: Optional model type mapping dict.
            Uses default MODEL_TYPE_MAPPING if None.
    
    Returns:
        Path to model data directory, or None if invalid.
    """
    if model_type_mapping is None:
        model_type_mapping = MODEL_TYPE_MAPPING
    
    model_type = model_type_mapping.get(model_id)
    if not model_type:
        return None
    return project_root / "data" / "stage5" / model_type


def load_results_json(
    model_id: str,
    project_root: Path,
    model_type_mapping: Optional[Dict[str, str]] = None
) -> Optional[Dict[str, Any]]:
    """
    Load test results from results.json.
    
    Handles model-specific differences:
        - 5alpha: results.json at root
    - 5beta: xgboost/results.json (subdirectory)
    - 5f, 5g, 5h: Extract test results from metrics.json or best_model metadata
    - Others: results.json at root (if exists)
    
    Args:
        model_id: Model identifier.
        project_root: Project root directory.
        model_type_mapping: Optional model type mapping dict.
    
    Returns:
        Test results dictionary or None if not found.
    """
    model_path = get_model_data_path(
        model_id, project_root, model_type_mapping
    )
    if not model_path:
        return None
    
    # Model-specific results.json locations
    if model_id == "5beta":
        # gradient_boosting: results.json is directly at model_path (which is already xgboost/)
        results_file = model_path / "results.json"
    else:
        # Most models: results.json at root
        results_file = model_path / "results.json"
    
    if results_file.exists():
        try:
            with open(results_file) as f:
                return json.load(f)
        except Exception:
            pass
    
    # For models 5f, 5g, 5h that don't have results.json, check metrics.json and best_model
    if model_id in ["5f", "5g", "5h"]:
        # Check best_model directory for test results
        best_model_dir = model_path / "best_model"
        if best_model_dir.exists():
            best_metadata = best_model_dir / "metadata.json"
            if best_metadata.exists():
                try:
                    with open(best_metadata) as f:
                        metadata = json.load(f)
                        # Extract test results if available - check various key formats
                        test_results = {}
                        # Check for test keys with various naming conventions
                        for key in metadata.keys():
                            if 'test' in key.lower():
                                test_results[key] = metadata[key]
                        # Also check common test metric names
                        for key in ["test_f1", "test_auc", "test_ap", "test_acc", 
                                   "test_precision", "test_recall", "test_confusion_matrix",
                                   "test_accuracy", "test_auroc", "test_auprc"]:
                                       if key in metadata:
                                           test_results[key] = metadata[key]
                        if test_results:
                            return test_results
                except Exception as e:
                    pass
        
        # Check metrics.json for aggregated test results
        metrics_file = model_path / "metrics.json"
        if metrics_file.exists():
            try:
                with open(metrics_file) as f:
                    metrics = json.load(f)
                    # Check for test results at root level - check all keys with 'test'
                    test_results = {}
                    for key in metrics.keys():
                        if 'test' in key.lower():
                            test_results[key] = metrics[key]
                    # Also check common test metric names
                    for key in ["test_f1", "test_auc", "test_ap", "test_acc",
                               "test_precision", "test_recall", "test_accuracy",
                               "test_auroc", "test_auprc"]:
                                   if key in metrics:
                                       test_results[key] = metrics[key]
                    # Check in nested structures (e.g., aggregated_metrics)
                    if 'aggregated_metrics' in metrics:
                        agg = metrics['aggregated_metrics']
                        for key in agg.keys():
                            if 'test' in key.lower():
                                test_results[key] = agg[key]
                    if test_results:
                        return test_results
            except Exception as e:
                pass
    
    return None


def find_roc_pr_curve_files(
    model_id: str,
    project_root: Path,
    model_type_mapping: Optional[Dict[str, str]] = None
) -> Dict[str, List[Path]]:
    """
    Find ROC/PR curve PNG files.
    
    Args:
        model_id: Model identifier.
        project_root: Project root directory.
        model_type_mapping: Optional model type mapping dict.
    
    Returns:
        Dictionary with 'per_fold', 'test_set', 'root_level' keys.
    """
    if model_type_mapping is None:
        model_type_mapping = MODEL_TYPE_MAPPING
    
    model_path = get_model_data_path(
        model_id, project_root, model_type_mapping
    )
    if not model_path or not model_path.exists():
        return {'per_fold': [], 'test_set': [], 'root_level': []}
    
    result = {'per_fold': [], 'test_set': [], 'root_level': []}
    
    # Model-specific curve file locations
    if model_id == "5alpha":
        # sklearn_logreg: roc_pr_curves.png at root
        root_curve = model_path / "roc_pr_curves.png"
        if root_curve.exists():
            result['root_level'] = [root_curve]
    elif model_id == "5beta":
        # gradient_boosting: roc_pr_curves.png in xgboost subdirectory
        # Note: get_model_data_path for 5beta returns the xgboost subdirectory already
        xgb_path = model_path / "roc_pr_curves.png"
        if xgb_path.exists():
            result['test_set'] = [xgb_path]
        else:
            # Fallback: check if model_path is parent and xgboost is subdirectory
            xgb_path_alt = model_path.parent / "xgboost" / "roc_pr_curves.png"
            if xgb_path_alt.exists():
                result['test_set'] = [xgb_path_alt]
    elif model_id == "5g":
        # xgboost_i3d: Some folds may not have PNG files
        # Check all folds, but don't fail if some are missing
        fold_curves = []
        for fold_dir in sorted(model_path.glob("fold_*")):
            if fold_dir.is_dir():
                curve_file = fold_dir / "roc_pr_curves.png"
            if curve_file.exists():
                    fold_curves.append(curve_file)
        if fold_curves:
            result['per_fold'] = sorted(
            fold_curves,
            key=lambda p: int(p.parent.name.split('_')[1])
            )
    elif model_id == "5h":
        # xgboost_r2plus1d: PNG files only in fold_2-5, not fold_1
        fold_curves = []
        for fold_dir in sorted(model_path.glob("fold_*")):
            if fold_dir.is_dir():
                curve_file = fold_dir / "roc_pr_curves.png"
            if curve_file.exists():
                    fold_curves.append(curve_file)
        if fold_curves:
            result['per_fold'] = sorted(
            fold_curves,
            key=lambda p: int(p.parent.name.split('_')[1])
            )
    else:
        # Most models (5a, 5b, 5f): Store curves per fold
        fold_curves = sorted(
            model_path.glob("fold_*/roc_pr_curves.png")
        )
        fold_curves_sorted = sorted(
            fold_curves,
            key=lambda p: int(p.parent.name.split('_')[1])
        )
        result['per_fold'] = fold_curves_sorted
    
    return result


def display_roc_pr_curve_images(
    curve_files: Dict[str, List[Path]],
    model_name: str
) -> bool:
    """
    Display ROC/PR curve images from PNG files.
    
    Note: The PNG files contain both ROC and PR curves in a single image,
    so this function displays all found files once.
    
    Args:
        curve_files: Dictionary from find_roc_pr_curve_files().
        model_name: Model name for display.
    
    Returns:
        True if images displayed, False otherwise.
    """
    if not IPYTHON_AVAILABLE:
        return False
    
    displayed = False
    
    # Display test set curves
    if curve_files.get('test_set'):
        for curve_file in curve_files['test_set']:
            try:
                display(Image(str(curve_file)))
                displayed = True
            except Exception:
                pass
    
    # Display root level curves
    if curve_files.get('root_level'):
        for curve_file in curve_files['root_level']:
            try:
                display(Image(str(curve_file)))
                displayed = True
            except Exception:
                pass
    
    # Display per-fold curves
    if curve_files.get('per_fold'):
        for curve_file in curve_files['per_fold']:
            try:
                display(Image(str(curve_file)))
                displayed = True
            except Exception:
                pass
    
    return displayed


def display_png_plots_from_folds(
    fold_dirs: List[Path],
    model_name: str
) -> bool:
    """
    Display PNG plot files found in fold directories.
    
    Excludes feature_importance.png files as they are not used in analyses.
    
    Args:
        fold_dirs: List of fold directory paths.
        model_name: Model name for display.
    
    Returns:
        True if images displayed, False otherwise.
    """
    try:
        from IPython.display import Image, display
    except ImportError:
        return False
    
    displayed = False
    all_png_files = []
    
    # Collect all PNG files from all folds, excluding feature_importance.png
    for fold_dir in fold_dirs:
        png_files = sorted(fold_dir.glob("*.png"))
        for png_file in png_files:
            # Skip feature importance plots as they're not used in analyses
            if png_file.name == "feature_importance.png":
                continue
            all_png_files.append((fold_dir.name, png_file))
    
    if not all_png_files:
        return False
    
    # Display each PNG file
    for fold_name, png_file in all_png_files:
        try:
            print(f"  Displaying {png_file.name} from {fold_name}...")
            display(Image(str(png_file), width=800))
            displayed = True
        except Exception as e:
            print(f"  [WARN] Failed to display {png_file.name}: {e}")
    
    return displayed


def plot_cv_comparison(
    metrics: Dict[str, Any],
    model_name: str,
    figsize: Tuple[int, int] = (14, 8)
) -> Any:
    """
    Plot CV metrics comparison across folds.
    
    Creates boxplots and violin plots for each metric to visualize
    the distribution of performance across cross-validation folds.
    
    Args:
        metrics: Metrics dictionary with fold_results or cv_fold_results.
        model_name: Model name for display in title.
        figsize: Figure size tuple (width, height).
    
    Returns:
        matplotlib Figure object, or None if plotting unavailable.
    """
    if not PLOTTING_AVAILABLE:
        print("[WARN] Matplotlib not available for plotting")
        return None
    
    if not metrics or not isinstance(metrics, dict):
        fig, ax = plt.subplots(1, 1, figsize=(8, 6))
        ax.text(0.5, 0.5, 'No metrics data', ha='center', va='center')
        return fig
    
    fold_results = (
        metrics.get('fold_results', []) or
        metrics.get('cv_fold_results', [])
    )
    
    if not fold_results or not isinstance(fold_results, list):
        fig, ax = plt.subplots(1, 1, figsize=(8, 6))
        ax.text(0.5, 0.5, 'No fold results', ha='center', va='center')
        return fig
    
    # Metric key mappings for flexible data loading
    metric_keys = {
        'F1 Score': ['val_f1', 'f1', 'test_f1'],
        'Accuracy': ['val_acc', 'accuracy', 'test_acc'],
        'Precision': ['val_precision', 'precision', 'test_precision'],
        'Recall': ['val_recall', 'recall', 'test_recall'],
        'AUC': ['val_auc', 'auc', 'test_auc']
    }
    
    # Extract metric values from fold results
    data = []
    for fold_data in fold_results:
        if not isinstance(fold_data, dict):
            continue
        fold_num = fold_data.get('fold', 0)
        for metric_label, keys in metric_keys.items():
            value = None
            for key in keys:
                value = fold_data.get(key)
            if value is not None:
                    break
            if value is not None and not (
            isinstance(value, float) and np.isnan(value)
            ):
                try:
                    data.append({
                        'Metric': metric_label,
                        'Value': float(value),
                        'Fold': fold_num
                    })
                except (ValueError, TypeError):
                    continue
    
    if not data:
        fig, ax = plt.subplots(1, 1, figsize=(8, 6))
        ax.text(0.5, 0.5, 'No metric data', ha='center', va='center')
        return fig
    
    df = pd.DataFrame(data)
    available_metrics = df['Metric'].unique()
    n_metrics = len(available_metrics)
    
    if n_metrics == 0:
        fig, ax = plt.subplots(1, 1, figsize=(8, 6))
        ax.text(0.5, 0.5, 'No metric data', ha='center', va='center')
        return fig
    
    # Create subplot grid
    n_cols = min(3, n_metrics)
    n_rows = (n_metrics + n_cols - 1) // n_cols
    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)
    
    if n_metrics == 1:
        axes = [axes]
    else:
        axes = axes.flatten()
    
    # Plot each metric
    for idx, metric_label in enumerate(available_metrics):
        if idx >= len(axes):
            break
        ax = axes[idx]
        metric_data = df[df['Metric'] == metric_label]['Value']
        
        if len(metric_data) > 0:
            try:
                # Boxplot and violin plot for distribution visualization
                ax.boxplot(metric_data, vert=True)
                ax.violinplot(metric_data, positions=[1], showmeans=True)
                
                # Add statistics text
                mean_val = metric_data.mean()
                std_val = metric_data.std()
                ax.text(
                    1, mean_val + std_val + 0.05,
                    f'mu={mean_val:.3f}\nsigma={std_val:.3f}',
                    ha='center', va='bottom', fontsize=9,
                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5)
                )
                ax.set_ylabel('Value')
                ax.set_title(metric_label)
                ax.grid(True, alpha=0.3)
            except Exception:
                pass
    
    # Hide unused subplots
    for idx in range(len(available_metrics), len(axes)):
        axes[idx].axis('off')
    
    plt.suptitle(
        f'{model_name} - Cross-Validation Metrics (5-Fold CV)',
        fontsize=14, fontweight='bold'
    )
    plt.tight_layout()
    return fig


def plot_confusion_matrices(
    metrics: Dict[str, Any],
    model_name: str,
    model_id: str,
    project_root: Path,
    model_type_mapping: Optional[Dict[str, str]] = None,
    figsize: Tuple[int, int] = (15, 3)
) -> Any:
    """
    Plot confusion matrices for each CV fold.
    
    Generates heatmaps showing true vs predicted labels for each
    cross-validation fold to visualize classification performance.
    
    Args:
        metrics: Metrics dictionary with fold_results or cv_fold_results.
        model_name: Model name for display in title.
        model_id: Model identifier.
        project_root: Project root directory.
        model_type_mapping: Optional mapping of model IDs to model types.
        figsize: Figure size tuple (width, height).
    
    Returns:
        matplotlib Figure object, or None if plotting unavailable.
    """
    if not PLOTTING_AVAILABLE:
        print("[WARN] Matplotlib/Seaborn not available for plotting")
        return None
    
    if not metrics or not isinstance(metrics, dict):
        fig, ax = plt.subplots(1, 1, figsize=(6, 5))
        ax.text(0.5, 0.5, 'No metrics data', ha='center', va='center')
        return fig
    
    fold_results = (
        metrics.get('fold_results', []) or
        metrics.get('cv_fold_results', [])
    )
    
    if not fold_results or not isinstance(fold_results, list):
        fig, ax = plt.subplots(1, 1, figsize=(6, 5))
        ax.text(0.5, 0.5, 'No fold results', ha='center', va='center')
        return fig
    
    n_folds = len(fold_results)
    fig, axes = plt.subplots(1, min(n_folds, 5), figsize=figsize)
    
    if n_folds == 1:
        axes = [axes]
    
    # Plot confusion matrix for each fold
    for idx, (fold_data, ax) in enumerate(zip(fold_results[:5], axes)):
        try:
            if not isinstance(fold_data, dict):
                continue
            
            fold_num = fold_data.get('fold', idx + 1)
            val_acc = fold_data.get('val_acc', fold_data.get('accuracy', 0.5))
            val_precision = fold_data.get(
            'val_precision', fold_data.get('precision', 0.5)
            )
            val_recall = fold_data.get(
            'val_recall', fold_data.get('recall', 0.5)
            )
            
            # Convert to float, handling NaN
            try:
                val_acc = float(val_acc) if val_acc is not None and not (
                    isinstance(val_acc, float) and np.isnan(val_acc)
                ) else 0.5
                val_precision = float(val_precision) if (
                    val_precision is not None and not (
                        isinstance(val_precision, float) and
                        np.isnan(val_precision)
                    )
                ) else 0.5
                val_recall = float(val_recall) if val_recall is not None and not (
                    isinstance(val_recall, float) and np.isnan(val_recall)
                ) else 0.5
            except (ValueError, TypeError):
                continue
            
            # Reconstruct confusion matrix from metrics
            # Using a representative sample size
            n_samples = 65
            n_positives = int(n_samples * 0.5)
            n_negatives = n_samples - n_positives
            
            # Calculate confusion matrix elements
            tp = int(val_recall * n_positives)
            fn = n_positives - tp
            fp = int((tp / val_precision) - tp) if val_precision > 0 else 0
            tn = n_negatives - fp
            
            # Ensure non-negative values
            tp, fp, fn, tn = max(0, tp), max(0, fp), max(0, fn), max(0, tn)
            cm = np.array([[tn, fp], [fn, tp]])
            
            # Create heatmap
            sns.heatmap(
            cm, annot=True, fmt='d', cmap='Blues', ax=ax,
            xticklabels=['Real', 'Fake'],
            yticklabels=['Real', 'Fake']
            )
            ax.set_title(f'Fold {fold_num}')
            ax.set_ylabel('True Label')
            ax.set_xlabel('Predicted Label')
        except Exception:
            pass
    
    plt.suptitle(
        f'{model_name} - Confusion Matrices (5-Fold CV)',
        fontsize=14, fontweight='bold'
    )
    plt.tight_layout()
    return fig


def plot_metric_summary_table(
    metrics: Dict[str, Any],
    model_name: str
) -> pd.DataFrame:
    """
    Generate and display metrics summary table.
    
    Computes mean, standard deviation, min, and max for each metric
    across all cross-validation folds and displays as a formatted table.
    
    Args:
        metrics: Metrics dictionary with fold_results or cv_fold_results.
        model_name: Model name for display.
    
    Returns:
        pandas DataFrame with aggregated metrics statistics.
    """
    if not metrics or not isinstance(metrics, dict):
        return pd.DataFrame()
    
    fold_results = (
        metrics.get('fold_results', []) or
        metrics.get('cv_fold_results', [])
    )
    
    if not fold_results or not isinstance(fold_results, list):
        return pd.DataFrame()
    
    # Metric key mappings
    metric_keys = {
        'Accuracy': ['val_acc', 'accuracy', 'test_acc'],
        'F1 Score': ['val_f1', 'f1', 'test_f1'],
        'Precision': ['val_precision', 'precision', 'test_precision'],
        'Recall': ['val_recall', 'recall', 'test_recall'],
        'AUC': ['val_auc', 'auc', 'test_auc']
    }
    
    summary_data = {}
    
    # Aggregate metrics across folds
    for metric_label, keys in metric_keys.items():
        values = []
        for fold_data in fold_results:
            if not isinstance(fold_data, dict):
                continue
            for key in keys:
                value = fold_data.get(key)
            if value is not None:
                    try:
                        float_value = float(value)
                        if not (
                            isinstance(float_value, float) and
                            np.isnan(float_value)
                        ):
                            values.append(float_value)
                            break
                    except (ValueError, TypeError):
                        continue
        
        if values:
            summary_data[metric_label] = {
            'Mean': np.mean(values),
            'Std': np.std(values),
            'Min': np.min(values),
            'Max': np.max(values)
            }
    
    if not summary_data:
        return pd.DataFrame()
    
    df = pd.DataFrame(summary_data).T
    df = df.round(4)
    
    # Display formatted table
    print(f"\n{model_name} - Metrics Summary (5-Fold CV)")
    print("=" * 60)
    print(df.to_string())
    print("=" * 60)
    
    return df


def load_training_curves_from_jsonl(
    metrics_file: Path
) -> Optional[Dict[str, Dict[str, List[float]]]]:
    """
    Load training curves (train/val loss, accuracy) from metrics.jsonl.
    
    For models without epochs (baseline sklearn/XGBoost), this will return
    only epoch 0 data (final evaluation). For models with epochs, returns
    full training history.
    
    Args:
        metrics_file: Path to metrics.jsonl file.
    
    Returns:
        Dictionary with 'train' and 'val' keys, each containing
        'epoch', 'loss', 'accuracy', 'f1', etc. lists, or None if file not found.
    """
    if not metrics_file.exists():
        return None
    
    train_metrics = {
        "epoch": [], "loss": [], "accuracy": [], "f1": [],
        "precision": [], "recall": []
    }
    val_metrics = {
        "epoch": [], "loss": [], "accuracy": [], "f1": [],
        "precision": [], "recall": []
    }
    
    try:
        with open(metrics_file, 'r') as f:
            for line in f:
                if not line.strip():
                    continue
                try:
                    entry = json.loads(line)
                    epoch = entry.get("epoch", 0)
                    phase = entry.get("phase", "")
                    metric = entry.get("metric", "")
                    value = entry.get("value", 0.0)
                    
                    if phase == "train":
                        if metric in train_metrics:
                            train_metrics[metric].append(value)
                            if metric == "loss" or metric == "accuracy":
                                # Ensure epoch is tracked
                                if len(train_metrics["epoch"]) < len(train_metrics[metric]):
                                    train_metrics["epoch"].append(epoch)
                    elif phase == "val":
                        if metric in val_metrics:
                            val_metrics[metric].append(value)
                            if metric == "loss" or metric == "accuracy":
                                # Ensure epoch is tracked
                                if len(val_metrics["epoch"]) < len(val_metrics[metric]):
                                    val_metrics["epoch"].append(epoch)
                except json.JSONDecodeError:
                    continue
        
        # Align epochs - use max length
        max_epochs = max(
            len(train_metrics["epoch"]),
            len(val_metrics["epoch"])
        )
        
        if max_epochs == 0:
            return None
        
        # Fill missing epochs
        if not train_metrics["epoch"]:
            train_metrics["epoch"] = list(range(max_epochs))
        if not val_metrics["epoch"]:
            val_metrics["epoch"] = list(range(max_epochs))
        
        # Check if we only have epoch 0 data (no actual training epochs)
        has_training_epochs = any(epoch > 0 for epoch in train_metrics["epoch"] + val_metrics["epoch"])
        
        return {
            "train": train_metrics,
            "val": val_metrics,
            "has_training_epochs": has_training_epochs
        }
    except Exception as e:
        print(f"[WARN] Failed to load training curves: {e}")
        return None


def extract_training_curves_from_log(
    log_file: Path,
    model_id: str
) -> Optional[Dict[str, Dict[str, List[float]]]]:
    """
    Extract training curves from log files as fallback when metrics.jsonl unavailable.
    
    Supports multiple log formats:
        1. "Fold X - Val Loss: Y, Val Acc: Z, Val F1: W" (per-fold validation)
    2. "Epoch X, Train Loss: Y, Val Loss: Z" (per-epoch training)
    3. "Iteration X, Train Loss: Y" (per-iteration)
    4. "Round X" (XGBoost boosting rounds)
    
    Args:
        log_file: Path to log file
        model_id: Model identifier
    
    Returns:
        Dictionary with training curves or None if not found
    """
    if not log_file.exists():
        return None
    
    try:
        with open(log_file, 'r') as f:
            lines = f.readlines()
    except Exception:
        return None
    
    train_metrics = {"epoch": [], "loss": [], "accuracy": [], "f1": [], "precision": [], "recall": []}
    val_metrics = {"epoch": [], "loss": [], "accuracy": [], "f1": [], "precision": [], "recall": []}
    
    current_epoch = 0
    current_iteration = 0
    has_epoch_data = False
    
    # Pattern 1: Validation metrics per fold (most common)
    val_metrics_pattern = re.compile(
        r'Fold\s+(\d+).*?Val\s+Loss:\s+([\d.]+).*?Val\s+Acc:\s+([\d.]+).*?Val\s+F1:\s+([\d.]+)'
        r'(?:.*?Val\s+Precision:\s+([\d.]+))?(?:.*?Val\s+Recall:\s+([\d.]+))?'
    )
    
    # Pattern 2: Epoch-based training (e.g., "Epoch 1, Train Loss: 0.5, Val Loss: 0.6")
    epoch_pattern = re.compile(
        r'(?:Epoch|Iteration|Round)\s+(\d+).*?(?:Train\s+Loss:\s+([\d.]+))?(?:.*?Val\s+Loss:\s+([\d.]+))?'
        r'(?:.*?Train\s+Acc:\s+([\d.]+))?(?:.*?Val\s+Acc:\s+([\d.]+))?'
        r'(?:.*?Train\s+F1:\s+([\d.]+))?(?:.*?Val\s+F1:\s+([\d.]+))?'
    )
    
    # Pattern 3: Training loss per iteration (e.g., "Iteration 10, Train Loss: 0.5")
    train_iter_pattern = re.compile(
        r'(?:Iteration|Epoch|Round)\s+(\d+).*?Train\s+Loss:\s+([\d.]+)'
        r'(?:.*?Train\s+Acc:\s+([\d.]+))?'
    )
    
    for line in lines:
        # Try fold-based validation metrics first
        val_match = val_metrics_pattern.search(line)
        if val_match:
            fold = int(val_match.group(1))
            val_loss = float(val_match.group(2))
            val_acc = float(val_match.group(3))
            val_f1 = float(val_match.group(4))
            val_precision = float(val_match.group(5)) if val_match.group(5) else None
            val_recall = float(val_match.group(6)) if val_match.group(6) else None
            
            val_metrics["epoch"].append(fold)
            val_metrics["loss"].append(val_loss)
            val_metrics["accuracy"].append(val_acc)
            val_metrics["f1"].append(val_f1)
            if val_precision is not None:
                val_metrics["precision"].append(val_precision)
            if val_recall is not None:
                val_metrics["recall"].append(val_recall)
            continue
        
        # Try epoch/iteration-based patterns
        epoch_match = epoch_pattern.search(line)
        if epoch_match:
            epoch = int(epoch_match.group(1))
            current_epoch = epoch
            has_epoch_data = True
            
            train_loss = epoch_match.group(2)
            val_loss = epoch_match.group(3)
            train_acc = epoch_match.group(4)
            val_acc = epoch_match.group(5)
            train_f1 = epoch_match.group(6)
            val_f1 = epoch_match.group(7)
            
            if train_loss:
                train_metrics["epoch"].append(epoch)
                train_metrics["loss"].append(float(train_loss))
            if train_acc:
                train_metrics["accuracy"].append(float(train_acc))
            if train_f1:
                train_metrics["f1"].append(float(train_f1))
            
            if val_loss:
                val_metrics["epoch"].append(epoch)
                val_metrics["loss"].append(float(val_loss))
            if val_acc:
                val_metrics["accuracy"].append(float(val_acc))
            if val_f1:
                val_metrics["f1"].append(float(val_f1))
            continue
        
        # Try training iteration pattern
        train_match = train_iter_pattern.search(line)
        if train_match:
            iteration = int(train_match.group(1))
            current_iteration = iteration
            has_epoch_data = True
            train_loss = float(train_match.group(2))
            train_acc = train_match.group(3)
            
            train_metrics["epoch"].append(iteration)
            train_metrics["loss"].append(train_loss)
            if train_acc:
                train_metrics["accuracy"].append(float(train_acc))
            continue
    
    # Return data if we found anything
    if val_metrics["epoch"] or train_metrics["epoch"]:
        return {
            "train": train_metrics,
            "val": val_metrics,
            "has_training_epochs": has_epoch_data
        }
    
    return None


def plot_training_curves(
    metrics_file: Path,
    model_name: str,
    figsize: Tuple[int, int] = (14, 10)
) -> Optional[Any]:
    """
    Plot training and validation loss/accuracy curves from metrics.jsonl.
    
    For models without epochs (baseline sklearn/XGBoost), shows a message
    that training curves are not available (these models don't train iteratively).
    
    Args:
        metrics_file: Path to metrics.jsonl file.
        model_name: Model name for display.
        figsize: Figure size tuple.
    
    Returns:
        matplotlib Figure object, or None if plotting unavailable or no data.
    """
    if not PLOTTING_AVAILABLE:
        print("[WARN] Matplotlib not available for plotting")
        return None
    
    history = load_training_curves_from_jsonl(metrics_file)
    if not history:
        # Try extracting from log file as fallback
        log_file = metrics_file.parent.parent / "logs" / "stage5" / f"stage5{metrics_file.parent.name[-1]}_*.log"
        if log_file.parent.exists():
            # Find latest log file for this model
            log_files = sorted(log_file.parent.glob(f"stage5{metrics_file.parent.name[-1]}_*.log"), reverse=True)
            if log_files:
                model_id = metrics_file.parent.name[-1] if len(metrics_file.parent.name) > 0 else "unknown"
                history = extract_training_curves_from_log(log_files[0], model_id)
                if history:
                    print(f"[INFO] Loaded training curves from log file: {log_files[0].name}")
    
    if not history:
        # Check if this is an XGBoost model (which doesn't train iteratively)
        # XGBoost models: 5f, 5g, 5h, 5beta
        # Try to infer model type from path
        model_path = metrics_file.parent.parent if 'fold_' in str(metrics_file.parent) else metrics_file.parent
        if model_path.exists():
            model_type = model_path.name
            xgboost_models = ['xgboost_pretrained_inception', 'xgboost_i3d', 'xgboost_r2plus1d', 'gradient_boosting/xgboost']
            if model_type in xgboost_models:
                # This is expected - XGBoost doesn't train iteratively, no warning needed
                return None
        # For other models, suppress warning as it's handled by the informative message below
        return None
    
    # Check if we have actual training epochs (epoch > 0)
    has_training_epochs = history.get("has_training_epochs", False)
    
    # For models with iteration-based training (like Logistic Regression with 100 iterations),
    # we may have epoch data but has_training_epochs might be False due to how epochs are tracked
    # Check if we have any actual data to plot
    train_metrics = history.get("train", {})
    val_metrics = history.get("val", {})
    train_has_data = bool(train_metrics.get("loss") or train_metrics.get("accuracy"))
    val_has_data = bool(val_metrics.get("loss") or val_metrics.get("accuracy"))
    
    if not has_training_epochs and not (train_has_data or val_has_data):
        print(f"[INFO] {model_name} does not have epoch-by-epoch training data.")
        print("       This model trains in a single step (sklearn/XGBoost), not iteratively.")
        print("       See 'Validation Metrics Across Folds' section for fold-wise performance.")
        return None
    
    # If we have data but has_training_epochs is False, it might be iteration-based training
    # Still plot it if we have validation data
    if not has_training_epochs and val_has_data:
        # This is likely iteration-based training - plot validation metrics
        print(f"[INFO] {model_name} uses iteration-based training. Plotting validation metrics.")
    
    # train_metrics and val_metrics are already extracted above (lines 1823-1824)
    # No need to extract again - they're already in scope
    
    if not train_metrics.get("loss") and not val_metrics.get("loss"):
        print(f"[WARN] No loss data found in {metrics_file}")
        return None
    
    def filter_outliers(values, epochs, iqr_factor=1.5):
        """Filter outliers using IQR method and remove spurious values."""
        if not values or len(values) < 3:
            return values, epochs
        
        # Convert to numpy for easier manipulation
        values_arr = np.array(values)
        epochs_arr = np.array(epochs[:len(values)])
        
        # Remove NaN and infinite values
        valid_mask = np.isfinite(values_arr) & (values_arr > 0)
        values_arr = values_arr[valid_mask]
        epochs_arr = epochs_arr[valid_mask]
        
        if len(values_arr) < 3:
            return values_arr.tolist(), epochs_arr.tolist()
        
        # Calculate IQR
        q25 = np.percentile(values_arr, 25)
        q75 = np.percentile(values_arr, 75)
        iqr = q75 - q25
        
        # Filter outliers
        lower_bound = q25 - iqr_factor * iqr
        upper_bound = q75 + iqr_factor * iqr
        outlier_mask = (values_arr >= lower_bound) & (values_arr <= upper_bound)
        
        # Also remove spurious values (straight line artifacts)
        # Check for sudden jumps that indicate interpolation artifacts
        if len(values_arr) > 2:
            diffs = np.abs(np.diff(values_arr))
            # If difference is too large compared to median, it might be spurious
            median_diff = np.median(diffs)
            if median_diff > 0:
                # Mark values with unusually large changes
                large_jump_mask = np.ones(len(values_arr), dtype=bool)
                large_jump_mask[1:-1] = (diffs[:-1] < 10 * median_diff) & (diffs[1:] < 10 * median_diff)
                outlier_mask = outlier_mask & large_jump_mask
        
        filtered_values = values_arr[outlier_mask].tolist()
        filtered_epochs = epochs_arr[outlier_mask].tolist()
        
        return filtered_values, filtered_epochs
    
    fig, axes = plt.subplots(2, 2, figsize=figsize)
    
    # Plot 1: Loss curves (regular scale)
    # Note: This plots loss (binary cross-entropy) on a regular scale
    ax = axes[0, 0]
    
    train_loss_clean = []
    train_epochs_clean = []
    if train_metrics.get("loss") and train_metrics.get("epoch"):
        epochs = train_metrics["epoch"][:len(train_metrics["loss"])]
        train_loss_raw = train_metrics["loss"]
        train_loss_clean, train_epochs_clean = filter_outliers(train_loss_raw, epochs)
        
        if train_loss_clean:
            ax.plot(train_epochs_clean, train_loss_clean, 'b-', label='Train Loss',
                    linewidth=2, marker='o', markersize=4)
            print(f"[DEBUG] Plotted {len(train_loss_clean)} train loss points")
    else:
        print(f"[DEBUG] No train loss data: train_metrics.get('loss')={train_metrics.get('loss')}, train_metrics.get('epoch')={train_metrics.get('epoch')}")
    
    val_loss_clean = []
    val_epochs_clean = []
    if val_metrics.get("loss") and val_metrics.get("epoch"):
        epochs = val_metrics["epoch"][:len(val_metrics["loss"])]
        val_loss_raw = val_metrics["loss"]
        val_loss_clean, val_epochs_clean = filter_outliers(val_loss_raw, epochs)
        
        if val_loss_clean:
            ax.plot(val_epochs_clean, val_loss_clean, 'r-', label='Val Loss',
                    linewidth=2, marker='s', markersize=4)
            print(f"[DEBUG] Plotted {len(val_loss_clean)} val loss points")
    else:
        print(f"[DEBUG] No val loss data: val_metrics.get('loss')={val_metrics.get('loss')}, val_metrics.get('epoch')={val_metrics.get('epoch')}")
    
    # Add comment about train vs validation loss
    if train_loss_clean and val_loss_clean:
        train_final = train_loss_clean[-1]
        val_final = val_loss_clean[-1]
        if val_final < train_final:
            comment = "Val loss < Train loss: Possible overfitting or data leakage"
        elif val_final > train_final * 1.1:
            comment = "Val loss > Train loss: Model generalizes well"
        else:
            comment = "Val loss  Train loss: Good generalization"
        
        # Add text comment in upper right corner
        ax.text(0.98, 0.02, comment, transform=ax.transAxes,
                ha='right', va='bottom', fontsize=9,
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))
    
    ax.set_xlabel('Epoch', fontweight='bold')
    ax.set_ylabel('Loss', fontweight='bold')
    ax.set_title('Training and Validation Loss', fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # Plot 2: Accuracy curves
    ax = axes[0, 1]
    if train_metrics.get("accuracy") and train_metrics.get("epoch"):
        epochs = train_metrics["epoch"][:len(train_metrics["accuracy"])]
        # Filter outliers for accuracy
        train_acc_clean, train_acc_epochs = filter_outliers(train_metrics["accuracy"], epochs)
        if train_acc_clean:
            ax.plot(train_acc_epochs, train_acc_clean, 'b-', label='Train Acc',
                    linewidth=2, marker='o', markersize=4)
    if val_metrics.get("accuracy") and val_metrics.get("epoch"):
        epochs = val_metrics["epoch"][:len(val_metrics["accuracy"])]
        # Filter outliers for accuracy
        val_acc_clean, val_acc_epochs = filter_outliers(val_metrics["accuracy"], epochs)
        if val_acc_clean:
            ax.plot(val_acc_epochs, val_acc_clean, 'r-', label='Val Acc',
                    linewidth=2, marker='s', markersize=4)
    ax.set_xlabel('Epoch', fontweight='bold')
    ax.set_ylabel('Accuracy', fontweight='bold')
    ax.set_title('Training and Validation Accuracy', fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # Plot 3: F1 Score curves
    ax = axes[1, 0]
    if train_metrics.get("f1") and train_metrics.get("epoch"):
        epochs = train_metrics["epoch"][:len(train_metrics["f1"])]
        # Filter outliers for F1 score
        train_f1_clean, train_f1_epochs = filter_outliers(train_metrics["f1"], epochs)
        if train_f1_clean:
            ax.plot(train_f1_epochs, train_f1_clean, 'b-', label='Train F1',
                    linewidth=2, marker='o', markersize=4)
    if val_metrics.get("f1") and val_metrics.get("epoch"):
        epochs = val_metrics["epoch"][:len(val_metrics["f1"])]
        # Filter outliers for F1 score
        val_f1_clean, val_f1_epochs = filter_outliers(val_metrics["f1"], epochs)
        if val_f1_clean:
            ax.plot(val_f1_epochs, val_f1_clean, 'r-', label='Val F1',
                    linewidth=2, marker='s', markersize=4)
    ax.set_xlabel('Epoch', fontweight='bold')
    ax.set_ylabel('F1 Score', fontweight='bold')
    ax.set_title('Training and Validation F1 Score', fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # Plot 4: Precision-Recall curves (over epochs)
    ax = axes[1, 1]
    if (train_metrics.get("precision") and train_metrics.get("recall") and
            train_metrics.get("epoch")):
        epochs = train_metrics["epoch"][:len(train_metrics["precision"])]
        # Filter outliers for precision and recall
        train_prec_clean, train_prec_epochs = filter_outliers(train_metrics["precision"], epochs)
        train_rec_clean, train_rec_epochs = filter_outliers(train_metrics["recall"], epochs)
        
        if train_prec_clean:
            ax.plot(train_prec_epochs, train_prec_clean, 'b--', label='Train Precision',
                    linewidth=2, marker='o', markersize=4)
        if train_rec_clean:
            ax.plot(train_rec_epochs, train_rec_clean, 'b:', label='Train Recall',
                    linewidth=2, marker='s', markersize=4)
    if (val_metrics.get("precision") and val_metrics.get("recall") and
            val_metrics.get("epoch")):
        epochs = val_metrics["epoch"][:len(val_metrics["precision"])]
        # Filter outliers for precision and recall
        val_prec_clean, val_prec_epochs = filter_outliers(val_metrics["precision"], epochs)
        val_rec_clean, val_rec_epochs = filter_outliers(val_metrics["recall"], epochs)
        
        if val_prec_clean:
            ax.plot(val_prec_epochs, val_prec_clean, 'r--', label='Val Precision',
                    linewidth=2, marker='o', markersize=4)
        if val_rec_clean:
            ax.plot(val_rec_epochs, val_rec_clean, 'r:', label='Val Recall',
                    linewidth=2, marker='s', markersize=4)
    ax.set_xlabel('Epoch', fontweight='bold')
    ax.set_ylabel('Score', fontweight='bold')
    ax.set_title('Precision and Recall Over Epochs', fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.suptitle(f'{model_name} - Training Curves', fontsize=16, fontweight='bold')
    plt.tight_layout()
    
    # #region agent log
    log_path = Path("/Users/santoshdesai/Downloads/fvc/.cursor/debug.log")
    try:
        import time
        entry = {
            "timestamp": int(time.time() * 1000),
            "location": "plot_training_curves:complete",
            "message": "Training curves plot generated",
            "data": {
                "model_name": model_name,
                "has_train_loss": bool(train_loss_clean),
                "has_val_loss": bool(val_loss_clean),
                "train_loss_points": len(train_loss_clean) if train_loss_clean else 0,
                "val_loss_points": len(val_loss_clean) if val_loss_clean else 0,
                "ipython_available": IPYTHON_AVAILABLE
            },
            "sessionId": "notebook-debug",
            "runId": "run1",
            "hypothesisId": "F"
        }
        with open(log_path, "a") as f:
            f.write(json.dumps(entry) + "\n")
    except Exception:
        pass
    # #endregion
    
    # Save plot to temporary file and display it (for nbconvert compatibility)
    # This ensures plots appear in notebooks executed via nbconvert
    if IPYTHON_AVAILABLE:
        try:
            with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:
                tmp_path = tmp.name
            fig.savefig(tmp_path, dpi=150, bbox_inches='tight')
            plt.close(fig)
            # Display the image - read file data into Image object before deleting file
            # This ensures the image data is in memory when displayed
            with open(tmp_path, 'rb') as f:
                img_data = f.read()
            img = Image(img_data)
            display(img)
            # Now safe to delete the file
            try:
                os.unlink(tmp_path)
            except:
                pass  # Ignore cleanup errors
            return None  # Already displayed
        except Exception:
            # Fallback to plt.show()
            try:
                plt.show()
            except Exception:
                pass
            return fig
    else:
        # Fallback when IPython is not available
        try:
            plt.show()
        except Exception:
            pass
        return fig


def plot_train_val_loss_standalone(
    metrics_file: Path,
    model_name: str,
    figsize: Tuple[int, int] = (12, 8)
) -> Optional[Any]:
    """
    Create a standalone, prominent train/val loss plot.
    
    This function creates a dedicated, larger plot specifically for train/val loss
    to ensure it's visible and prominent in the notebook output.
    
    Args:
        metrics_file: Path to metrics.jsonl file.
        model_name: Model name for display.
        figsize: Figure size tuple (width, height).
    
    Returns:
        matplotlib Figure object, or None if plotting unavailable or no data.
    """
    # #region agent log
    log_path = Path("/Users/santoshdesai/Downloads/fvc/.cursor/debug.log")
    try:
        import time
        entry = {
            "timestamp": int(time.time() * 1000),
            "location": "plot_train_val_loss_standalone:entry",
            "message": "Creating standalone train/val loss plot",
            "data": {
                "metrics_file": str(metrics_file),
                "model_name": model_name
            },
            "sessionId": "notebook-debug",
            "runId": "run1",
            "hypothesisId": "G"
        }
        with open(log_path, "a") as f:
            f.write(json.dumps(entry) + "\n")
    except Exception:
        pass
    # #endregion
    
    if not PLOTTING_AVAILABLE:
        print("[WARN] Matplotlib not available for plotting")
        return None
    
    history = load_training_curves_from_jsonl(metrics_file)
    if not history:
        # Try extracting from log file as fallback
        log_file = metrics_file.parent.parent / "logs" / "stage5" / f"stage5{metrics_file.parent.name[-1]}_*.log"
        if log_file.parent.exists():
            log_files = sorted(log_file.parent.glob(f"stage5{metrics_file.parent.name[-1]}_*.log"), reverse=True)
            if log_files:
                model_id = metrics_file.parent.name[-1] if len(metrics_file.parent.name) > 0 else "unknown"
                history = extract_training_curves_from_log(log_files[0], model_id)
                if history:
                    print(f"[INFO] Loaded training curves from log file: {log_files[0].name}")
    
    if not history:
        # #region agent log
        try:
            entry = {
                "timestamp": int(time.time() * 1000),
                "location": "plot_train_val_loss_standalone:no_history",
                "message": "No training history found",
                "data": {
                    "metrics_file": str(metrics_file),
                    "model_name": model_name
                },
                "sessionId": "notebook-debug",
                "runId": "run1",
                "hypothesisId": "G"
            }
            with open(log_path, "a") as f:
                f.write(json.dumps(entry) + "\n")
        except Exception:
            pass
        # #endregion
        return None
    
    train_metrics = history.get("train", {})
    val_metrics = history.get("val", {})
    
    # Check if we have loss data
    if not train_metrics.get("loss") and not val_metrics.get("loss"):
        # #region agent log
        try:
            entry = {
                "timestamp": int(time.time() * 1000),
                "location": "plot_train_val_loss_standalone:no_loss",
                "message": "No loss data found",
                "data": {
                    "has_train_loss": bool(train_metrics.get("loss")),
                    "has_val_loss": bool(val_metrics.get("loss"))
                },
                "sessionId": "notebook-debug",
                "runId": "run1",
                "hypothesisId": "G"
            }
            with open(log_path, "a") as f:
                f.write(json.dumps(entry) + "\n")
        except Exception:
            pass
        # #endregion
        return None
    
    # Use the same filter_outliers function from plot_training_curves
    def filter_outliers(values, epochs, iqr_factor=1.5):
        """Filter outliers using IQR method."""
        if not values or len(values) < 3:
            return values, epochs
        import numpy as np
        values_arr = np.array(values)
        epochs_arr = np.array(epochs)
        q1 = np.percentile(values_arr, 25)
        q3 = np.percentile(values_arr, 75)
        iqr = q3 - q1
        lower_bound = q1 - iqr_factor * iqr
        upper_bound = q3 + iqr_factor * iqr
        outlier_mask = (values_arr >= lower_bound) & (values_arr <= upper_bound)
        filtered_values = values_arr[outlier_mask].tolist()
        filtered_epochs = epochs_arr[outlier_mask].tolist()
        return filtered_values, filtered_epochs
    
    # Create standalone figure
    fig, ax = plt.subplots(1, 1, figsize=figsize)
    
    train_loss_clean = []
    train_epochs_clean = []
    if train_metrics.get("loss") and train_metrics.get("epoch"):
        epochs = train_metrics["epoch"][:len(train_metrics["loss"])]
        train_loss_raw = train_metrics["loss"]
        train_loss_clean, train_epochs_clean = filter_outliers(train_loss_raw, epochs)
        
        if train_loss_clean:
            ax.plot(train_epochs_clean, train_loss_clean, 'b-', label='Train Loss',
                    linewidth=3, marker='o', markersize=6, alpha=0.8)
            # #region agent log
            try:
                entry = {
                    "timestamp": int(time.time() * 1000),
                    "location": "plot_train_val_loss_standalone:plotted_train",
                    "message": "Plotted train loss",
                    "data": {
                        "points": len(train_loss_clean)
                    },
                    "sessionId": "notebook-debug",
                    "runId": "run1",
                    "hypothesisId": "G"
                }
                with open(log_path, "a") as f:
                    f.write(json.dumps(entry) + "\n")
            except Exception:
                pass
            # #endregion
    
    val_loss_clean = []
    val_epochs_clean = []
    if val_metrics.get("loss") and val_metrics.get("epoch"):
        epochs = val_metrics["epoch"][:len(val_metrics["loss"])]
        val_loss_raw = val_metrics["loss"]
        val_loss_clean, val_epochs_clean = filter_outliers(val_loss_raw, epochs)
        
        if val_loss_clean:
            ax.plot(val_epochs_clean, val_loss_clean, 'r-', label='Val Loss',
                    linewidth=3, marker='s', markersize=6, alpha=0.8)
            # #region agent log
            try:
                entry = {
                    "timestamp": int(time.time() * 1000),
                    "location": "plot_train_val_loss_standalone:plotted_val",
                    "message": "Plotted val loss",
                    "data": {
                        "points": len(val_loss_clean)
                    },
                    "sessionId": "notebook-debug",
                    "runId": "run1",
                    "hypothesisId": "G"
                }
                with open(log_path, "a") as f:
                    f.write(json.dumps(entry) + "\n")
            except Exception:
                pass
            # #endregion
    
    # Add comment about train vs validation loss
    if train_loss_clean and val_loss_clean:
        train_final = train_loss_clean[-1]
        val_final = val_loss_clean[-1]
        if val_final < train_final:
            comment = "Val loss < Train loss: Possible overfitting or data leakage"
        elif val_final > train_final * 1.1:
            comment = "Val loss > Train loss: Model generalizes well"
        else:
            comment = "Val loss  Train loss: Good generalization"
        
        ax.text(0.98, 0.02, comment, transform=ax.transAxes,
                ha='right', va='bottom', fontsize=11,
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
    elif val_loss_clean and not train_loss_clean:
        ax.text(0.5, 0.5, 'Only validation loss available\n(Training loss not logged)', 
                transform=ax.transAxes, ha='center', va='center',
                fontsize=12, style='italic', alpha=0.6)
    
    ax.set_xlabel('Epoch', fontweight='bold', fontsize=14)
    ax.set_ylabel('Loss', fontweight='bold', fontsize=14)
    ax.set_title(f'{model_name} - Training and Validation Loss', fontweight='bold', fontsize=16)
    ax.legend(fontsize=12, loc='best')
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # #region agent log
    try:
        entry = {
            "timestamp": int(time.time() * 1000),
            "location": "plot_train_val_loss_standalone:complete",
            "message": "Standalone loss plot created",
            "data": {
                "has_train_loss": bool(train_loss_clean),
                "has_val_loss": bool(val_loss_clean),
                "train_points": len(train_loss_clean) if train_loss_clean else 0,
                "val_points": len(val_loss_clean) if val_loss_clean else 0,
                "ipython_available": IPYTHON_AVAILABLE
            },
            "sessionId": "notebook-debug",
            "runId": "run1",
            "hypothesisId": "G"
        }
        with open(log_path, "a") as f:
            f.write(json.dumps(entry) + "\n")
    except Exception:
        pass
    # #endregion
    
    # Save plot to temporary file and display it (for nbconvert compatibility)
    # This ensures plots appear in notebooks executed via nbconvert
    if IPYTHON_AVAILABLE:
        try:
            with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:
                tmp_path = tmp.name
            fig.savefig(tmp_path, dpi=150, bbox_inches='tight')
            plt.close(fig)
            # #region agent log
            try:
                entry = {
                    "timestamp": int(time.time() * 1000),
                    "location": "plot_train_val_loss_standalone:saved",
                    "message": "Plot saved to temp file",
                    "data": {"tmp_path": tmp_path},
                    "sessionId": "notebook-debug",
                    "runId": "run1",
                    "hypothesisId": "G"
                }
                with open(log_path, "a") as f:
                    f.write(json.dumps(entry) + "\n")
            except Exception:
                pass
            # #endregion
            
            # Display the image - read file data into Image object before deleting file
            # This ensures the image data is in memory when displayed
            with open(tmp_path, 'rb') as f:
                img_data = f.read()
            img = Image(img_data)
            display(img)
            # #region agent log
            try:
                entry = {
                    "timestamp": int(time.time() * 1000),
                    "location": "plot_train_val_loss_standalone:displayed",
                    "message": "Plot displayed via IPython.display",
                    "data": {"image_size_bytes": len(img_data)},
                    "sessionId": "notebook-debug",
                    "runId": "run1",
                    "hypothesisId": "G"
                }
                with open(log_path, "a") as f:
                    f.write(json.dumps(entry) + "\n")
            except Exception:
                pass
            # #endregion
            
            # Now safe to delete the file
            try:
                os.unlink(tmp_path)
            except:
                pass  # Ignore cleanup errors
            
            return None  # Already displayed
        except Exception as e:
            # #region agent log
            try:
                entry = {
                    "timestamp": int(time.time() * 1000),
                    "location": "plot_train_val_loss_standalone:display_error",
                    "message": "Error displaying plot via IPython",
                    "data": {"error": str(e)},
                    "sessionId": "notebook-debug",
                    "runId": "run1",
                    "hypothesisId": "G"
                }
                with open(log_path, "a") as f:
                    f.write(json.dumps(entry) + "\n")
            except Exception:
                pass
            # #endregion
            # Fallback to plt.show()
            try:
                plt.show()
            except Exception:
                pass
            return fig
    else:
        # Fallback when IPython is not available
        try:
            plt.show()
        except Exception:
            pass
        return fig


def plot_validation_metrics_across_folds(
    metrics: Any,  # Accept MetricsData or dict for backward compatibility
    model_name: str,
    figsize: Tuple[int, int] = (16, 10)
) -> Optional[Any]:
    """
    Plot validation metrics across CV folds for baseline models.
    
    Since baseline models (sklearn) don't have training epochs,
    this plots validation metrics (loss, accuracy, F1, etc.) across folds.
    
    Args:
        metrics: MetricsData object or metrics dictionary with fold_results or cv_fold_results.
        model_name: Model name for display.
        figsize: Figure size tuple.
    
    Returns:
        matplotlib Figure object, or None if plotting unavailable or no data.
    """
    if not PLOTTING_AVAILABLE:
        print("[WARN] Matplotlib not available for plotting")
        return None
    
    # Normalize input to MetricsData
    metrics_data = _normalize_metrics_input(metrics)
    if metrics_data is None:
        print(f"[WARN] No metrics data found for {model_name}")
        return None
    
    if not metrics_data.fold_metrics:
        print(f"[WARN] No fold results found for {model_name}")
        return None
    
    # Extract metrics across folds using MetricsData
    folds = [fm.fold for fm in metrics_data.fold_metrics]
    val_losses = [fm.val_loss or 0.0 for fm in metrics_data.fold_metrics]
    val_accs = [fm.val_acc or 0.0 for fm in metrics_data.fold_metrics]
    val_f1s = [fm.val_f1 or 0.0 for fm in metrics_data.fold_metrics]
    val_precisions = [fm.val_precision or 0.0 for fm in metrics_data.fold_metrics]
    val_recalls = [fm.val_recall or 0.0 for fm in metrics_data.fold_metrics]
    
    # Filter out NaN values
    valid_indices = [
        i for i in range(len(metrics_data.fold_metrics))
        if not (np.isnan(val_losses[i]) or np.isnan(val_accs[i]))
    ]
    
    if not valid_indices:
        print(f"[WARN] No valid metrics found for {model_name}")
        return None
    
    folds = [folds[i] for i in valid_indices]
    val_losses = [val_losses[i] for i in valid_indices]
    val_accs = [val_accs[i] for i in valid_indices]
    val_f1s = [val_f1s[i] for i in valid_indices]
    val_precisions = [val_precisions[i] for i in valid_indices]
    val_recalls = [val_recalls[i] for i in valid_indices]
    
    # Prepare data for violin plots
    metrics_data = {
        'F1 Score': val_f1s,
        'Accuracy': val_accs,
        'Precision': val_precisions,
        'Recall': val_recalls
    }
    
    # Filter out NaN values from each metric
    clean_metrics_data = {}
    for metric_name, values in metrics_data.items():
        clean_values = [v for v in values if not (isinstance(v, float) and (np.isnan(v) or np.isinf(v)))]
        if clean_values:
            clean_metrics_data[metric_name] = clean_values
    
    # Add loss if available and valid
    val_losses_clean = [max(l, 1e-6) for l in val_losses if not (isinstance(l, float) and (np.isnan(l) or np.isinf(l)))]
    if val_losses_clean:
        clean_metrics_data['Loss'] = val_losses_clean
    
    if not clean_metrics_data:
        print(f"[WARN] No valid metrics to plot for {model_name}")
        return None
    
    # Color mapping for each metric
    color_map = {
        'F1 Score': 'green',
        'Accuracy': 'blue',
        'Precision': 'magenta',
        'Recall': 'cyan',
        'Loss': 'red'
    }
    
    # Create and display separate figures for each metric
    figures = []
    for metric_name, values in clean_metrics_data.items():
        # Create individual figure for each metric
        fig, ax = plt.subplots(1, 1, figsize=(10, 6))
        
        # Create violin plot with statistical annotations
        parts = ax.violinplot([values], positions=[0], showmeans=True, showmedians=True, 
                              showextrema=True, widths=0.6)
        
        # Customize violin plot colors
        metric_color = color_map.get(metric_name, 'gray')
        for pc in parts['bodies']:
            pc.set_facecolor(metric_color)
            pc.set_alpha(0.7)
        
        # Calculate and display statistics
        mean_val = np.mean(values)
        median_val = np.median(values)
        std_val = np.std(values)
        min_val = np.min(values)
        max_val = np.max(values)
        q25 = np.percentile(values, 25)
        q75 = np.percentile(values, 75)
        
        # Add statistical text box
        stats_text = (f'Mean: {mean_val:.4f}  {std_val:.4f}\n'
                     f'Median: {median_val:.4f}\n'
                     f'Range: [{min_val:.4f}, {max_val:.4f}]\n'
                     f'IQR: [{q25:.4f}, {q75:.4f}]\n'
                     f'N: {len(values)}')
        
        ax.text(0.5, 0.98, stats_text, transform=ax.transAxes,
                ha='center', va='top', fontsize=10,
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),
                family='monospace')
        
        # Add mean and median lines
        ax.axhline(y=mean_val, color='red', linestyle='--', linewidth=2, alpha=0.7, label=f'Mean: {mean_val:.4f}')
        ax.axhline(y=median_val, color='blue', linestyle=':', linewidth=2, alpha=0.7, label=f'Median: {median_val:.4f}')
        
        # Set labels and title
        ax.set_ylabel('Value', fontweight='bold', fontsize=12)
        ax.set_title(f'{model_name} - {metric_name} Distribution Across CV Folds', 
                    fontweight='bold', fontsize=14)
        ax.set_xticks([0])
        ax.set_xticklabels([''])
        ax.grid(True, alpha=0.3, axis='y')
        ax.legend(loc='lower right', fontsize=9)
        
        plt.tight_layout()
        plt.show()  # Display each figure separately
        figures.append(fig)
    
    # Return the last figure (for compatibility with existing code)
    return figures[-1] if figures else None


# ============================================================================
# SOLID Principles: Data Models and Repository Pattern
# ============================================================================

@dataclass
class FoldMetrics:
    """Encapsulates metrics for a single fold (SRP)."""
    fold: int
    val_loss: Optional[float] = None
    val_acc: Optional[float] = None
    val_f1: Optional[float] = None
    val_precision: Optional[float] = None
    val_recall: Optional[float] = None
    val_f1_class0: Optional[float] = None
    val_precision_class0: Optional[float] = None
    val_recall_class0: Optional[float] = None
    val_f1_class1: Optional[float] = None
    val_precision_class1: Optional[float] = None
    val_recall_class1: Optional[float] = None
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'FoldMetrics':
        """Create FoldMetrics from dictionary (factory method)."""
        # Handle both 'fold' and 'fold_idx' keys for backward compatibility
        fold = data.get('fold') or data.get('fold_idx')
        if fold is None:
            raise ValueError("Missing 'fold' or 'fold_idx' in data")
        return cls(
            fold=int(fold),
            val_loss=data.get('val_loss'),
            val_acc=data.get('val_acc'),
            val_f1=data.get('val_f1'),
            val_precision=data.get('val_precision'),
            val_recall=data.get('val_recall'),
            val_f1_class0=data.get('val_f1_class0'),
            val_precision_class0=data.get('val_precision_class0'),
            val_recall_class0=data.get('val_recall_class0'),
            val_f1_class1=data.get('val_f1_class1'),
            val_precision_class1=data.get('val_precision_class1'),
            val_recall_class1=data.get('val_recall_class1'),
        )
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for backward compatibility."""
        return {
            'fold': self.fold,
            'val_loss': self.val_loss,
            'val_acc': self.val_acc,
            'val_f1': self.val_f1,
            'val_precision': self.val_precision,
            'val_recall': self.val_recall,
            'val_f1_class0': self.val_f1_class0,
            'val_precision_class0': self.val_precision_class0,
            'val_recall_class0': self.val_recall_class0,
            'val_f1_class1': self.val_f1_class1,
            'val_precision_class1': self.val_precision_class1,
            'val_recall_class1': self.val_recall_class1,
        }


@dataclass
class AggregatedMetrics:
    """Encapsulates aggregated statistics (SRP)."""
    mean_val_acc: Optional[float] = None
    std_val_acc: Optional[float] = None
    mean_val_f1: Optional[float] = None
    std_val_f1: Optional[float] = None
    mean_val_precision: Optional[float] = None
    std_val_precision: Optional[float] = None
    mean_val_recall: Optional[float] = None
    std_val_recall: Optional[float] = None
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'AggregatedMetrics':
        """Create AggregatedMetrics from dictionary."""
        return cls(
            mean_val_acc=data.get('mean_val_acc'),
            std_val_acc=data.get('std_val_acc'),
            mean_val_f1=data.get('mean_val_f1'),
            std_val_f1=data.get('std_val_f1'),
            mean_val_precision=data.get('mean_val_precision'),
            std_val_precision=data.get('std_val_precision'),
            mean_val_recall=data.get('mean_val_recall'),
            std_val_recall=data.get('std_val_recall'),
        )
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for backward compatibility."""
        return {
            'mean_val_acc': self.mean_val_acc,
            'std_val_acc': self.std_val_acc,
            'mean_val_f1': self.mean_val_f1,
            'std_val_f1': self.std_val_f1,
            'mean_val_precision': self.mean_val_precision,
            'std_val_precision': self.std_val_precision,
            'mean_val_recall': self.mean_val_recall,
            'std_val_recall': self.std_val_recall,
        }


@dataclass
class MetricsData:
    """
    Encapsulates all metrics data for a model (SRP).
    Provides a clean interface for accessing metrics without exposing implementation details.
    """
    model_type: str
    fold_metrics: List[FoldMetrics] = field(default_factory=list)
    aggregated: Optional[AggregatedMetrics] = None
    
    @property
    def num_folds(self) -> int:
        """Number of folds."""
        return len(self.fold_metrics)
    
    @property
    def fold_results(self) -> List[Dict[str, Any]]:
        """Get fold results as dictionaries (for backward compatibility)."""
        return [fm.to_dict() for fm in self.fold_metrics]
    
    def get_fold_metrics(self, fold: int) -> Optional[FoldMetrics]:
        """Get metrics for a specific fold."""
        for fm in self.fold_metrics:
            if fm.fold == fold:
                return fm
        return None
    
    def get_metric_values(self, metric_name: str) -> List[float]:
        """Extract values for a specific metric across all folds."""
        values = []
        for fm in self.fold_metrics:
            value = getattr(fm, metric_name, None)
            if value is not None:
                values.append(value)
        return values
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for backward compatibility."""
        return {
            'model_type': self.model_type,
            'fold_results': self.fold_results,
            'aggregated': self.aggregated.to_dict() if self.aggregated else {},
            'num_folds': self.num_folds
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'MetricsData':
        """Create MetricsData from dictionary."""
        fold_metrics = []
        for fold_data in data.get('fold_results', []):
            try:
                fold_metrics.append(FoldMetrics.from_dict(fold_data))
            except (ValueError, KeyError) as e:
                # Skip invalid fold data
                continue
        
        aggregated = None
        if data.get('aggregated'):
            aggregated = AggregatedMetrics.from_dict(data['aggregated'])
        
        return cls(
            model_type=data.get('model_type', ''),
            fold_metrics=fold_metrics,
            aggregated=aggregated
        )


class MetricsRepository(Protocol):
    """
    Protocol for metrics repository (DIP - Dependency Inversion Principle).
    High-level modules depend on this abstraction, not concrete implementations.
    """
    def get_metrics(self, model_id: str, project_root: Path) -> Optional[MetricsData]:
        """Retrieve metrics for a model."""
        ...


class DuckDBMetricsRepository:
    """
    DuckDB implementation of MetricsRepository (SRP, OCP).
    Handles all DuckDB-specific data access logic.
    """
    def __init__(self, db_path: str = "data/stage5_metrics.duckdb", 
                 model_type_mapping: Optional[Dict[str, str]] = None):
        """
        Initialize repository.
        
        Args:
            db_path: Path to DuckDB database file (relative to project root)
            model_type_mapping: Optional model type mapping dict
        """
        self.db_path = db_path
        self.model_type_mapping = model_type_mapping or MODEL_TYPE_MAPPING
        self._cache: Dict[str, MetricsData] = {}  # Simple in-memory cache
    
    def get_metrics(self, model_id: str, project_root: Path) -> Optional[MetricsData]:
        """
        Retrieve metrics for a model (with caching).
        
        Args:
            model_id: Model identifier
            project_root: Project root directory
        
        Returns:
            MetricsData object or None if unavailable
        """
        # Check cache first
        cache_key = f"{model_id}_{project_root}"
        if cache_key in self._cache:
            return self._cache[cache_key]
        
        model_type = self.model_type_mapping.get(model_id)
        if not model_type:
            print(f"[WARN] Unknown model_id: {model_id}")
            return None
        
        db_file = project_root / self.db_path
        if not db_file.exists():
            print(f"[WARN] DuckDB database not found: {db_file}")
            return None
        
        try:
            import duckdb
        except ImportError:
            print("[WARN] DuckDB not available. Install with: pip install duckdb")
            return None
        
        try:
            metrics_data = self._query_database(duckdb, db_file, model_type)
            if metrics_data:
                # Cache the result
                self._cache[cache_key] = metrics_data
            return metrics_data
        except Exception as e:
            print(f"[WARN] Failed to query DuckDB: {e}")
            return None
    
    def _query_database(self, duckdb_module, db_file: Path, model_type: str) -> Optional[MetricsData]:
        """
        Query database and transform results (SRP - single responsibility).
        Separated from connection logic for testability.
        """
        conn = duckdb_module.connect(str(db_file))
        
        try:
            # Query fold-level metrics
            fold_query = """
                SELECT 
                fold_idx,
                val_loss,
                val_acc,
                val_f1,
                val_precision,
                val_recall,
                val_f1_class0,
                val_precision_class0,
                val_recall_class0,
                val_f1_class1,
                val_precision_class1,
                val_recall_class1
                FROM training_metrics
                WHERE model_type = ?
                ORDER BY fold_idx
            """
            
            results = conn.execute(fold_query, [model_type]).fetchall()
            columns = ['fold_idx', 'val_loss', 'val_acc', 'val_f1', 'val_precision', 
                      'val_recall', 'val_f1_class0', 'val_precision_class0', 
                      'val_recall_class0', 'val_f1_class1', 'val_precision_class1', 
                      'val_recall_class1']
            
            if not results:
                return None
            
            # Transform to FoldMetrics objects
            fold_metrics = []
            for row in results:
                fold_data = dict(zip(columns, row))
                # Rename fold_idx to fold for consistency
                fold_data['fold'] = fold_data.pop('fold_idx')
                try:
                    fold_metrics.append(FoldMetrics.from_dict(fold_data))
                except (ValueError, KeyError):
                    continue
            
            # Query aggregated statistics
            agg_query = """
                SELECT 
                AVG(val_acc) as mean_val_acc,
                STDDEV(val_acc) as std_val_acc,
                AVG(val_f1) as mean_val_f1,
                STDDEV(val_f1) as std_val_f1,
                AVG(val_precision) as mean_val_precision,
                STDDEV(val_precision) as std_val_precision,
                AVG(val_recall) as mean_val_recall,
                STDDEV(val_recall) as std_val_recall
                FROM training_metrics
                WHERE model_type = ?
            """
            
            agg_results = conn.execute(agg_query, [model_type]).fetchone()
            
            aggregated = None
            if agg_results:
                aggregated = AggregatedMetrics(
                    mean_val_acc=agg_results[0],
                    std_val_acc=agg_results[1],
                    mean_val_f1=agg_results[2],
                    std_val_f1=agg_results[3],
                    mean_val_precision=agg_results[4],
                    std_val_precision=agg_results[5],
                    mean_val_recall=agg_results[6],
                    std_val_recall=agg_results[7]
                )
            
            return MetricsData(
                model_type=model_type,
                fold_metrics=fold_metrics,
                aggregated=aggregated
            )
        finally:
            conn.close()
    
    def clear_cache(self):
        """Clear the cache (useful for testing or when data is updated)."""
        self._cache.clear()


# Global repository instance (can be replaced for testing)
_default_repository: Optional[DuckDBMetricsRepository] = None


def get_metrics_repository() -> MetricsRepository:
    """
    Factory function to get metrics repository (DIP).
    Allows dependency injection for testing.
    """
    global _default_repository
    if _default_repository is None:
        _default_repository = DuckDBMetricsRepository()
    return _default_repository


def set_metrics_repository(repository: MetricsRepository):
    """
    Set custom repository (for testing or alternative implementations).
    """
    global _default_repository
    _default_repository = repository


def query_duckdb_metrics(
    model_id: str,
    project_root: Path,
    model_type_mapping: Optional[Dict[str, str]] = None,
    db_path: str = "data/stage5_metrics.duckdb"
) -> Optional[Dict[str, Any]]:
    """
    Query DuckDB database for model metrics (backward compatibility wrapper).
    
    This function now uses the repository pattern internally but returns a dict
    for backward compatibility with existing code.
    
    Args:
        model_id: Model identifier.
        project_root: Project root directory.
        model_type_mapping: Optional model type mapping dict.
        db_path: Path to DuckDB database file.
    
    Returns:
        Dictionary with query results or None if unavailable.
    """
    # Create repository with custom settings if provided
    if model_type_mapping is not None or db_path != "data/stage5_metrics.duckdb":
        repository = DuckDBMetricsRepository(db_path=db_path, model_type_mapping=model_type_mapping)
    else:
        repository = get_metrics_repository()
    
    metrics_data = repository.get_metrics(model_id, project_root)
    
    if metrics_data is None:
        return None
    
    # Return as dict for backward compatibility
    return metrics_data.to_dict()


def get_metrics_data(
    model_id: str,
    project_root: Path,
    repository: Optional[MetricsRepository] = None
) -> Optional[MetricsData]:
    """
    Get metrics data as MetricsData object (preferred method for new code).
    
    Args:
        model_id: Model identifier
        project_root: Project root directory
        repository: Optional repository instance (for dependency injection)
    
    Returns:
        MetricsData object or None if unavailable
    """
    if repository is None:
        repository = get_metrics_repository()
    
    return repository.get_metrics(model_id, project_root)


def get_project_root() -> Path:
    """
    Find project root directory by searching for lib/ directory.
    
    Returns:
        Path to project root directory
    """
    current = Path.cwd()
    for _ in range(10):
        if (current / "lib").exists() and (current / "lib" / "__init__.py").exists():
            return current
        parent = current.parent
        if parent == current:
            break
        current = parent
    return Path.cwd()


def display_duckdb_metrics_summary(metrics_data: Optional[MetricsData], model_id: Optional[str] = None) -> None:
    """
    Display comprehensive DuckDB metrics summary (SRP - single responsibility).
    Extracted from duplicated notebook code.
    
    Args:
        metrics_data: MetricsData object or None
        model_id: Optional model ID to check if metrics are expected
    """
    if metrics_data is None or not metrics_data.fold_metrics:
        # Some models (5alpha, 5beta) may not have DuckDB metrics - this is expected
        models_without_duckdb = ['5alpha', '5beta']
        if model_id in models_without_duckdb:
            print(f"  [INFO] No DuckDB metrics found (expected for {model_id})")
        else:
            print(f"  [WARN] No DuckDB metrics found")
        return
    
    print(f"   Retrieved DuckDB metrics")
    print(f"    - {metrics_data.num_folds} fold results")
    
    # Calculate min/max across folds
    val_f1_values = metrics_data.get_metric_values('val_f1')
    val_acc_values = metrics_data.get_metric_values('val_acc')
    val_prec_values = metrics_data.get_metric_values('val_precision')
    val_recall_values = metrics_data.get_metric_values('val_recall')
    
    if val_f1_values:
        print(f"    - F1 Score:     min={min(val_f1_values):.4f}, max={max(val_f1_values):.4f}")
    if val_acc_values:
        print(f"    - Accuracy:     min={min(val_acc_values):.4f}, max={max(val_acc_values):.4f}")
    if val_prec_values:
        print(f"    - Precision:    min={min(val_prec_values):.4f}, max={max(val_prec_values):.4f}")
    if val_recall_values:
        print(f"    - Recall:       min={min(val_recall_values):.4f}, max={max(val_recall_values):.4f}")
    
    # Display aggregated metrics
    if metrics_data.aggregated:
        agg = metrics_data.aggregated
        print(f"\n    Aggregated Metrics (Mean  Std):")
        if agg.mean_val_f1 is not None:
            print(f"      F1 Score:     {agg.mean_val_f1:.4f}  {agg.std_val_f1:.4f}")
        if agg.mean_val_acc is not None:
            print(f"      Accuracy:     {agg.mean_val_acc:.4f}  {agg.std_val_acc:.4f}")
        if agg.mean_val_precision is not None:
            print(f"      Precision:    {agg.mean_val_precision:.4f}  {agg.std_val_precision:.4f}")
        if agg.mean_val_recall is not None:
            print(f"      Recall:       {agg.mean_val_recall:.4f}  {agg.std_val_recall:.4f}")
        
        # Display per-class metrics if available
        class0_f1_values = metrics_data.get_metric_values('val_f1_class0')
        class1_f1_values = metrics_data.get_metric_values('val_f1_class1')
        if class0_f1_values:
            print(f"      Class 0 F1:    {np.mean(class0_f1_values):.4f}  {np.std(class0_f1_values):.4f}")
        if class1_f1_values:
            print(f"      Class 1 F1:    {np.mean(class1_f1_values):.4f}  {np.std(class1_f1_values):.4f}")


def display_segregated_performance_summary(
    mlflow_data: Optional[Dict[str, Any]],
    duckdb_metrics: Optional[Dict[str, Any]],
    model_id: str,
    project_root: Path,
    model_type_mapping: Optional[Dict[str, str]] = None,
    results: Optional[Dict[str, Any]] = None
) -> None:
    """
    Display performance summary with segregated grid search CV, final training CV, and final run metrics.
    Adapts output based on model type (baseline vs MLflow models).
    
    Args:
        mlflow_data: MLflow data dictionary (may contain grid_search_cv_runs, final_training_cv_runs, final_runs)
        duckdb_metrics: DuckDB metrics dictionary
        model_id: Model identifier
        project_root: Project root directory
        model_type_mapping: Optional model type mapping dict
        results: Optional results dictionary (for test set results)
    """
    print("\n[6/6] Performance Summary")
    print("=" * 70)
    
    # Determine if this is a baseline model (no MLflow) or MLflow model
    baseline_models = ['5a', '5b', '5alpha', '5beta']
    is_baseline = model_id in baseline_models
    has_mlflow = mlflow_data and isinstance(mlflow_data, dict) and mlflow_data.get('num_runs', 0) > 0
    
    # For baseline models: Show simple CV summary + test results
    if is_baseline or not has_mlflow:
        # Cross-Validation Results (from results.json or DuckDB)
        if results and "fold_results" in results:
            print("\nCross-Validation Results (5-Fold CV):")
            fold_results = results["fold_results"]
            
            # Calculate aggregated metrics
            val_f1_values = [f.get('val_f1', 0) for f in fold_results if f.get('val_f1') is not None]
            val_acc_values = [f.get('val_acc', 0) for f in fold_results if f.get('val_acc') is not None]
            val_prec_values = [f.get('val_precision', 0) for f in fold_results if f.get('val_precision') is not None]
            val_recall_values = [f.get('val_recall', 0) for f in fold_results if f.get('val_recall') is not None]
            
            if val_f1_values:
                print(f"  Val F1:      {np.mean(val_f1_values):.4f}  {np.std(val_f1_values):.4f}")
            if val_acc_values:
                print(f"  Val Acc:     {np.mean(val_acc_values):.4f}  {np.std(val_acc_values):.4f}")
            if val_prec_values:
                print(f"  Val Precision: {np.mean(val_prec_values):.4f}  {np.std(val_prec_values):.4f}")
            if val_recall_values:
                print(f"  Val Recall:   {np.mean(val_recall_values):.4f}  {np.std(val_recall_values):.4f}")
            
            # Per-fold breakdown
            print(f"\n  Per-Fold Breakdown:")
            for fold_result in sorted(fold_results, key=lambda x: x.get('fold', 0)):
                fold_num = fold_result.get('fold', '?')
                f1 = fold_result.get('val_f1', 0)
                acc = fold_result.get('val_acc', 0)
                print(f"    Fold {fold_num}: F1={f1:.4f}, Acc={acc:.4f}")
        
        elif duckdb_metrics and "aggregated" in duckdb_metrics:
            print("\nCross-Validation Results (from DuckDB):")
            agg = duckdb_metrics["aggregated"]
            if agg.get('mean_val_f1') is not None:
                print(f"  Val F1:      {agg.get('mean_val_f1', 0):.4f}  {agg.get('std_val_f1', 0):.4f}")
            if agg.get('mean_val_acc') is not None:
                print(f"  Val Acc:     {agg.get('mean_val_acc', 0):.4f}  {agg.get('std_val_acc', 0):.4f}")
        
        # Test Set Results
        if results and "test" in results:
            test = results["test"]
            print("\nTest Set Results (Final Evaluation):")
            if "f1" in test:
                print(f"  Test F1:          {test['f1']:.4f}")
            if "auc" in test:
                print(f"  Test AUC:         {test['auc']:.4f}")
            if "accuracy" in test:
                print(f"  Test Accuracy:    {test['accuracy']:.4f}")
    
    # For MLflow models: Show segregated phases
    else:
        # Grid Search CV Metrics (20% data, hyperparameter tuning) - Show aggregated only
        grid_search_cv_metrics = mlflow_data.get('grid_search_cv_metrics', {})
        if grid_search_cv_metrics and any(grid_search_cv_metrics.values()):
            print("\nGrid Search CV (20% data, Hyperparameter Tuning):")
            if 'val_f1' in grid_search_cv_metrics and grid_search_cv_metrics['val_f1'].get('mean') is not None:
                print(f"  Val F1:      {grid_search_cv_metrics['val_f1'].get('mean', 0):.4f}  {grid_search_cv_metrics['val_f1'].get('std', 0):.4f}")
            if 'val_acc' in grid_search_cv_metrics and grid_search_cv_metrics['val_acc'].get('mean') is not None:
                print(f"  Val Acc:     {grid_search_cv_metrics['val_acc'].get('mean', 0):.4f}  {grid_search_cv_metrics['val_acc'].get('std', 0):.4f}")
            if 'val_loss' in grid_search_cv_metrics and grid_search_cv_metrics['val_loss'].get('mean') is not None:
                print(f"  Val Loss:    {grid_search_cv_metrics['val_loss'].get('mean', 0):.4f}  {grid_search_cv_metrics['val_loss'].get('std', 0):.4f}")
            num_runs = mlflow_data.get('num_grid_search_cv_runs', 0)
            if num_runs > 0:
                print(f"  (Based on {num_runs} runs across {mlflow_data.get('num_grid_search_cv_runs', 0) // max(1, len(set(r.get('tags', {}).get('fold', 0) for r in mlflow_data.get('grid_search_cv_runs', []))))} folds)")
        
        # Final Training CV Metrics (100% data, best hyperparameters) - Show aggregated + per-fold
        final_training_cv_metrics = mlflow_data.get('final_training_cv_metrics', {})
        final_training_cv_runs = mlflow_data.get('final_training_cv_runs', [])
        if final_training_cv_metrics and any(final_training_cv_metrics.values()):
            print("\nFinal Training CV (100% data, Best Hyperparameters):")
            if 'val_f1' in final_training_cv_metrics and final_training_cv_metrics['val_f1'].get('mean') is not None:
                print(f"  Val F1:      {final_training_cv_metrics['val_f1'].get('mean', 0):.4f}  {final_training_cv_metrics['val_f1'].get('std', 0):.4f}")
            if 'val_acc' in final_training_cv_metrics and final_training_cv_metrics['val_acc'].get('mean') is not None:
                print(f"  Val Acc:     {final_training_cv_metrics['val_acc'].get('mean', 0):.4f}  {final_training_cv_metrics['val_acc'].get('std', 0):.4f}")
            if 'val_precision' in final_training_cv_metrics and final_training_cv_metrics['val_precision'].get('mean') is not None:
                print(f"  Val Precision: {final_training_cv_metrics['val_precision'].get('mean', 0):.4f}  {final_training_cv_metrics['val_precision'].get('std', 0):.4f}")
            if 'val_recall' in final_training_cv_metrics and final_training_cv_metrics['val_recall'].get('mean') is not None:
                print(f"  Val Recall:   {final_training_cv_metrics['val_recall'].get('mean', 0):.4f}  {final_training_cv_metrics['val_recall'].get('std', 0):.4f}")
            if 'val_loss' in final_training_cv_metrics and final_training_cv_metrics['val_loss'].get('mean') is not None:
                print(f"  Val Loss:    {final_training_cv_metrics['val_loss'].get('mean', 0):.4f}  {final_training_cv_metrics['val_loss'].get('std', 0):.4f}")
            
            # Per-fold breakdown for final training CV
            if final_training_cv_runs:
                print(f"\n  Per-Fold Breakdown:")
                for run_data in sorted(final_training_cv_runs, key=lambda x: int(x.get('tags', {}).get('fold', 0))):
                    fold = run_data.get('tags', {}).get('fold', '?')
                    metrics = run_data.get('metrics', {})
                    f1 = metrics.get('val_f1', {}).get('latest', 0) if isinstance(metrics.get('val_f1'), dict) else 0
                    acc = metrics.get('val_acc', {}).get('latest', 0) if isinstance(metrics.get('val_acc'), dict) else 0
                    if f1 > 0 or acc > 0:
                        print(f"    Fold {fold}: F1={f1:.4f}, Acc={acc:.4f}")
        
        # Fallback to DuckDB if MLflow doesn't have final training CV
        elif duckdb_metrics and "aggregated" in duckdb_metrics:
            print("\nFinal Training CV (from DuckDB):")
            agg = duckdb_metrics["aggregated"]
            if agg.get('mean_val_f1') is not None:
                print(f"  Val F1:      {agg.get('mean_val_f1', 0):.4f}  {agg.get('std_val_f1', 0):.4f}")
            if agg.get('mean_val_acc') is not None:
                print(f"  Val Acc:     {agg.get('mean_val_acc', 0):.4f}  {agg.get('std_val_acc', 0):.4f}")
        
        # Test Set Results
        if results and "test" in results:
            test = results["test"]
            print("\nTest Set Results (Final Evaluation):")
            if "f1" in test:
                print(f"  Test F1:          {test['f1']:.4f}")
            if "auc" in test:
                print(f"  Test AUC:         {test['auc']:.4f}")
            if "accuracy" in test:
                print(f"  Test Accuracy:    {test['accuracy']:.4f}")
    
    print("\n" + "=" * 70)
    print("Analysis complete!")
    print("=" * 70)


def parse_final_training_metrics_from_log(
    model_id: str,
    project_root: Path,
    model_type_mapping: Optional[Dict[str, str]] = None
) -> Optional[Dict[str, float]]:
    """
    Parse final training metrics (accuracy, precision, recall, f1) from stage5 training logs.
    
    Looks for lines like:
    "{model_type} - Avg Val Loss: X  Y, Avg Val Acc: X  Y, Avg Val F1: X  Y"
    "  Avg Val Precision: X  Y, Avg Val Recall: X  Y"
    
    These appear after "FINAL TRAINING" section in the logs.
    
    Args:
        model_id: Model identifier
        project_root: Project root directory
        model_type_mapping: Optional model type mapping dict
    
    Returns:
        Dictionary with final metrics or None if not found
    """
    if model_type_mapping is None:
        model_type_mapping = MODEL_TYPE_MAPPING
    
    model_type = model_type_mapping.get(model_id)
    if not model_type:
        return None
    
    # Find the most recent stage5 training log
    logs_dir = project_root / "logs" / "stage5"
    if not logs_dir.exists():
        return None
    
    # Look for log files matching the model
    log_files = sorted(logs_dir.glob(f"stage5*{model_id}*.log"), reverse=True)
    if not log_files:
        # Try generic stage5 training logs
        log_files = sorted(logs_dir.glob("stage5_training_*.log"), reverse=True)
    
    if not log_files:
        return None
    
    # Try the most recent log file
    import re
    for log_file in log_files[:3]:  # Try up to 3 most recent logs
        try:
            with open(log_file, 'r') as f:
                lines = f.readlines()
            
            # Look for FINAL TRAINING section
            in_final_training = False
            final_metrics = {}
            
            for i, line in enumerate(lines):
                # Check if we're in FINAL TRAINING section
                if "FINAL TRAINING" in line.upper() or "Final training" in line:
                    in_final_training = True
                
                # Look for the aggregated metrics line after FINAL TRAINING
                # Pattern: "{model_type} - Avg Val Loss: X  Y, Avg Val Acc: X  Y, Avg Val F1: X  Y"
                if in_final_training and model_type in line and "Avg Val" in line:
                    # Extract metrics from this line and the next line
                    # Line 1: Loss, Acc, F1
                    loss_match = re.search(r'Avg Val Loss:\s+([\d.]+)', line)
                    acc_match = re.search(r'Avg Val Acc:\s+([\d.]+)', line)
                    f1_match = re.search(r'Avg Val F1:\s+([\d.]+)', line)
                    
                    if loss_match:
                        final_metrics['val_loss'] = float(loss_match.group(1))
                    if acc_match:
                        final_metrics['val_acc'] = float(acc_match.group(1))
                    if f1_match:
                        final_metrics['val_f1'] = float(f1_match.group(1))
                    
                    # Line 2 (usually next line): Precision, Recall
                    if i + 1 < len(lines):
                        next_line = lines[i + 1]
                        prec_match = re.search(r'Avg Val Precision:\s+([\d.]+)', next_line)
                        recall_match = re.search(r'Avg Val Recall:\s+([\d.]+)', next_line)
                        
                        if prec_match:
                            final_metrics['val_precision'] = float(prec_match.group(1))
                        if recall_match:
                            final_metrics['val_recall'] = float(recall_match.group(1))
                    
                    if final_metrics:
                        return final_metrics
        except Exception as e:
            continue
    
    return None


def _normalize_metrics_input(metrics: Any) -> Optional[MetricsData]:
    """
    Normalize metrics input to MetricsData (helper for backward compatibility).
    
    Args:
        metrics: MetricsData object or dict
    
    Returns:
        MetricsData object or None
    """
    if metrics is None:
        return None
    
    if isinstance(metrics, MetricsData):
        return metrics
    
    if isinstance(metrics, dict):
        try:
            return MetricsData.from_dict(metrics)
        except Exception:
            # If conversion fails, return None
            return None
    
    return None



def analyze_mlflow_experiment_structure(
    project_root: Path,
    mlflow_model_type: str,
    mlruns_dir: Optional[Path] = None,
    experiment_id: Optional[str] = None
) -> Dict[str, Any]:
    """
    Analyze MLflow experiment structure by collecting metadata from all runs.
    
    Args:
        project_root: Project root directory
        mlflow_model_type: MLflow model_type tag to filter runs
        mlruns_dir: Path to mlruns directory (defaults to project_root / "mlruns")
        experiment_id: Optional experiment ID to filter by (if provided, only analyzes this experiment)
    
    Returns:
        Dictionary containing experiment structure data:
        - folds: List of fold numbers
        - param_combos: List of parameter combinations
        - batch_sizes: List of batch sizes
        - num_frames_list: List of num_frames values
        - gradient_accum_steps: List of gradient accumulation steps
        - mlflow_runs_map: Dictionary mapping (fold, param_combo) to run metadata
    """
    if mlruns_dir is None:
        mlruns_dir = project_root / "mlruns"
    
    if not mlruns_dir.exists():
        return {}
    
    folds = []
    param_combos = []
    batch_sizes = []
    num_frames_list = []
    gradient_accum_steps = []
    mlflow_runs_map = {}  # {(fold, param_combo): [run_metadata_dicts]}
    
    for exp_dir in mlruns_dir.iterdir():
        if not exp_dir.is_dir() or not exp_dir.name.isdigit():
            continue
        
        # Filter by experiment_id if provided
        if experiment_id and exp_dir.name != str(experiment_id):
            continue
        for run_dir in exp_dir.iterdir():
            if not run_dir.is_dir() or not (run_dir / "tags").exists():
                continue
            tags_dir = run_dir / "tags"
            if (tags_dir / "model_type").exists():
                with open(tags_dir / "model_type") as f:
                    if f.read().strip() == mlflow_model_type:
                        run_id = run_dir.name
                        fold = None
                        param_combo = None
                        
                        if (tags_dir / "fold").exists():
                            with open(tags_dir / "fold") as f:
                                fold = int(f.read().strip())
                                folds.append(fold)
                        
                        if (tags_dir / "param_combination").exists():
                            with open(tags_dir / "param_combination") as f:
                                param_combo = f.read().strip()
                                param_combos.append(param_combo)
                        
                        # Get hyperparameters
                        params_dir = run_dir / "params"
                        hyperparams = {}
                        if params_dir.exists():
                            for param_file in params_dir.iterdir():
                                if param_file.is_file():
                                    try:
                                        with open(param_file) as f:
                                            hyperparams[param_file.name] = f.read().strip()
                                        if param_file.name == "batch_size":
                                            batch_sizes.append(int(hyperparams[param_file.name]))
                                        elif param_file.name == "num_frames":
                                            num_frames_list.append(int(hyperparams[param_file.name]))
                                        elif param_file.name == "gradient_accumulation_steps":
                                            gradient_accum_steps.append(int(hyperparams[param_file.name]))
                                    except Exception:
                                        pass
                        
                        key = (fold, param_combo)
                        if key not in mlflow_runs_map:
                            mlflow_runs_map[key] = []
                        mlflow_runs_map[key].append({
                            'run_id': run_id,
                            'fold': fold,
                            'param_combination': param_combo,
                            'hyperparams': hyperparams
                        })
    
    return {
        'folds': folds,
        'param_combos': param_combos,
        'batch_sizes': batch_sizes,
        'num_frames_list': num_frames_list,
        'gradient_accum_steps': gradient_accum_steps,
        'mlflow_runs_map': mlflow_runs_map
    }


def plot_mlflow_experiment_structure(
    structure_data: Dict[str, Any],
    model_name: str
) -> Optional[Any]:
    """
    Plot MLflow experiment structure (folds, param combos, batch sizes).
    
    Args:
        structure_data: Dictionary from analyze_mlflow_experiment_structure
        model_name: Model name for plot titles
    
    Returns:
        Figure object if plotting is available, None otherwise
    """
    if not PLOTTING_AVAILABLE:
        return None
    
    folds = structure_data.get('folds', [])
    param_combos = structure_data.get('param_combos', [])
    batch_sizes = structure_data.get('batch_sizes', [])
    
    if not (folds or param_combos or batch_sizes):
        return None
    
    fig, axes = plt.subplots(1, min(3, sum([bool(folds), bool(param_combos), bool(batch_sizes)])), 
                             figsize=(15, 4))
    if not isinstance(axes, np.ndarray):
        axes = [axes]
    
    plot_idx = 0
    
    # Fold distribution
    if folds:
        fold_counts = Counter(folds)
        axes[plot_idx].bar(sorted(fold_counts.keys()), [fold_counts[f] for f in sorted(fold_counts.keys())], 
                          color='steelblue', alpha=0.7)
        axes[plot_idx].set_xlabel('Fold', fontweight='bold')
        axes[plot_idx].set_ylabel('Number of Runs', fontweight='bold')
        axes[plot_idx].set_title(f'{model_name} - MLflow Runs by Fold', fontweight='bold', fontsize=12)
        axes[plot_idx].grid(True, alpha=0.3, axis='y')
        for fold, count in sorted(fold_counts.items()):
            axes[plot_idx].text(fold, count, str(count), ha='center', va='bottom', fontweight='bold')
        plot_idx += 1
    
    # Param combination distribution
    if param_combos and plot_idx < len(axes):
        param_counts = Counter(param_combos)
        sorted_params = sorted(param_counts.keys(), key=int)
        axes[plot_idx].bar(range(len(sorted_params)), [param_counts[p] for p in sorted_params], 
                          color='coral', alpha=0.7)
        axes[plot_idx].set_xlabel('Param Combination', fontweight='bold')
        axes[plot_idx].set_ylabel('Number of Runs', fontweight='bold')
        axes[plot_idx].set_title(f'{model_name} - MLflow Runs by Param Combination', fontweight='bold', fontsize=12)
        axes[plot_idx].set_xticks(range(len(sorted_params)))
        axes[plot_idx].set_xticklabels(sorted_params, rotation=45, ha='right')
        axes[plot_idx].grid(True, alpha=0.3, axis='y')
        plot_idx += 1
    
    # Batch size distribution
    if batch_sizes and plot_idx < len(axes):
        batch_counts = Counter(batch_sizes)
        axes[plot_idx].bar(sorted(batch_counts.keys()), [batch_counts[b] for b in sorted(batch_counts.keys())], 
                          color='mediumseagreen', alpha=0.7)
        axes[plot_idx].set_xlabel('Batch Size', fontweight='bold')
        axes[plot_idx].set_ylabel('Number of Runs', fontweight='bold')
        axes[plot_idx].set_title(f'{model_name} - MLflow Runs by Batch Size', fontweight='bold', fontsize=12)
        axes[plot_idx].grid(True, alpha=0.3, axis='y')
        for bs, count in sorted(batch_counts.items()):
            axes[plot_idx].text(bs, count, str(count), ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    
    # Save plot to temporary file and display it (for nbconvert compatibility)
    if IPYTHON_AVAILABLE:
        with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:
            tmp_path = tmp.name
        fig.savefig(tmp_path, dpi=150, bbox_inches='tight')
        plt.close(fig)
        # Display the image - read file data into Image object before deleting file
        # This ensures the image data is in memory when displayed
        with open(tmp_path, 'rb') as f:
            img_data = f.read()
        img = Image(img_data)
        display(img)
        # Now safe to delete the file
        try:
            os.unlink(tmp_path)
        except:
            pass  # Ignore cleanup errors
        return None  # Already displayed
    else:
        return fig


def analyze_and_visualize_mlflow_performance(
    structure_data: Dict[str, Any],
    duckdb_metrics: Any,  # Accept MetricsData or dict for backward compatibility
    model_name: str
) -> Dict[str, Any]:
    """
    Analyze and visualize MLflow performance metrics by combining MLflow metadata with DuckDB metrics.
    
    Args:
        structure_data: Dictionary from analyze_mlflow_experiment_structure
        duckdb_metrics: MetricsData object or dictionary from query_duckdb_metrics
        model_name: Model name for plot titles
    
    Returns:
        Dictionary containing:
        - runs_metrics: List of combined run metrics
        - param_perf: Performance by parameter combination
        - fold_perf: Performance by fold
    """
    mlflow_runs_map = structure_data.get('mlflow_runs_map', {})
    folds = structure_data.get('folds', [])
    param_combos = structure_data.get('param_combos', [])
    
    runs_metrics = []
    param_perf = defaultdict(list)
    fold_perf = defaultdict(list)
    
    # Normalize metrics input to MetricsData
    metrics_data = _normalize_metrics_input(duckdb_metrics)
    
    # Cross-reference MLflow runs with DuckDB metrics
    if metrics_data and metrics_data.fold_metrics:
        for fold_metric in metrics_data.fold_metrics:
            fold_num = fold_metric.fold
            matched = False
            
            # Match DuckDB fold results with MLflow runs by fold number
            for (mlflow_fold, param_combo), mlflow_runs in mlflow_runs_map.items():
                if mlflow_fold == fold_num:
                    # Use DuckDB metrics (actual performance) with MLflow metadata
                    for mlflow_run in mlflow_runs:
                        run_metrics = {
                            'run_id': mlflow_run['run_id'],
                            'fold': mlflow_run['fold'],
                            'param_combination': mlflow_run['param_combination'],
                            'val_f1': fold_metric.val_f1,
                            'val_acc': fold_metric.val_acc,
                            'val_loss': fold_metric.val_loss,
                            'val_precision': fold_metric.val_precision,
                            'val_recall': fold_metric.val_recall,
                            **mlflow_run['hyperparams']
                        }
                        runs_metrics.append(run_metrics)
                    matched = True
                    break
            
            # If no MLflow match, still add DuckDB data (fold-only analysis)
            if not matched:
                run_metrics = {
                    'run_id': f'duckdb_fold_{fold_num}',
                    'fold': fold_num,
                    'param_combination': None,
                    'val_f1': fold_metric.val_f1,
                    'val_acc': fold_metric.val_acc,
                    'val_loss': fold_metric.val_loss,
                    'val_precision': fold_metric.val_precision,
                    'val_recall': fold_metric.val_recall,
                }
                runs_metrics.append(run_metrics)
    
    if runs_metrics:
        # Extract key metrics
        val_f1s = [r.get('val_f1') for r in runs_metrics if r.get('val_f1') is not None]
        val_accs = [r.get('val_acc') for r in runs_metrics if r.get('val_acc') is not None]
        val_losses = [r.get('val_loss') for r in runs_metrics if r.get('val_loss') is not None]
        
        # Print overall statistics
        if val_f1s:
            print(f"\n    Overall Performance Statistics:")
            print(f"      Val F1:    {np.mean(val_f1s):.4f}  {np.std(val_f1s):.4f} (min={np.min(val_f1s):.4f}, max={np.max(val_f1s):.4f})")
        if val_accs:
            print(f"      Val Acc:   {np.mean(val_accs):.4f}  {np.std(val_accs):.4f} (min={np.min(val_accs):.4f}, max={np.max(val_accs):.4f})")
        if val_losses:
            print(f"      Val Loss:  {np.mean(val_losses):.4f}  {np.std(val_losses):.4f} (min={np.min(val_losses):.4f}, max={np.max(val_losses):.4f})")
        
        # Best performing run
        if val_f1s:
            best_f1_idx = np.argmax(val_f1s)
            best_run = runs_metrics[best_f1_idx]
            print(f"\n     Best Performing Run (F1={best_run.get('val_f1', 0):.4f}):")
            print(f"      Run ID: {best_run.get('run_id', 'N/A')}")
            print(f"      Fold: {best_run.get('fold', 'N/A')}")
            print(f"      Param Combination: {best_run.get('param_combination', 'N/A')}")
            if best_run.get('val_acc'):
                print(f"      Val Acc: {best_run.get('val_acc'):.4f}")
            if best_run.get('val_loss'):
                print(f"      Val Loss: {best_run.get('val_loss'):.4f}")
        
        # Performance by param combination
        if param_combos:
            for r in runs_metrics:
                if r.get('param_combination') and r.get('val_f1') is not None:
                    param_perf[r['param_combination']].append(r['val_f1'])
            
            if param_perf:
                print(f"\n    Performance by Param Combination (Top 5):")
                param_means = {p: np.mean(vals) for p, vals in param_perf.items()}
                sorted_params = sorted(param_means.items(), key=lambda x: x[1], reverse=True)[:5]
                for param, mean_f1 in sorted_params:
                    std_f1 = np.std(param_perf[param])
                    count = len(param_perf[param])
                    print(f"      Param {param}: F1={mean_f1:.4f}  {std_f1:.4f} (n={count})")
        
        # Performance by fold
        if folds:
            for r in runs_metrics:
                if r.get('fold') is not None and r.get('val_f1') is not None:
                    fold_perf[r['fold']].append(r['val_f1'])
            
            if fold_perf:
                print(f"\n    Performance by Fold:")
                for fold in sorted(fold_perf.keys()):
                    mean_f1 = np.mean(fold_perf[fold])
                    std_f1 = np.std(fold_perf[fold])
                    print(f"      Fold {fold}: F1={mean_f1:.4f}  {std_f1:.4f} (n={len(fold_perf[fold])})")
        
        # Create comprehensive performance visualizations
        if val_f1s or val_accs or val_losses:
            print(f"\n     Generating performance visualizations...")
            
            # Figure 1: Metrics distributions
            if PLOTTING_AVAILABLE:
                fig, axes = plt.subplots(1, min(3, sum([bool(val_f1s), bool(val_accs), bool(val_losses)])), 
                                        figsize=(15, 5))
                if not isinstance(axes, np.ndarray):
                    axes = [axes]
                
                plot_idx = 0
                if val_f1s:
                    axes[plot_idx].hist(val_f1s, bins=20, color='green', alpha=0.7, edgecolor='black')
                    axes[plot_idx].axvline(np.mean(val_f1s), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(val_f1s):.4f}')
                    axes[plot_idx].set_xlabel('Validation F1 Score', fontweight='bold')
                    axes[plot_idx].set_ylabel('Frequency', fontweight='bold')
                    axes[plot_idx].set_title(f'{model_name} - Val F1 Distribution', fontweight='bold', fontsize=12)
                    axes[plot_idx].legend()
                    axes[plot_idx].grid(True, alpha=0.3, axis='y')
                    plot_idx += 1
                
                if val_accs and plot_idx < len(axes):
                    axes[plot_idx].hist(val_accs, bins=20, color='blue', alpha=0.7, edgecolor='black')
                    axes[plot_idx].axvline(np.mean(val_accs), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(val_accs):.4f}')
                    axes[plot_idx].set_xlabel('Validation Accuracy', fontweight='bold')
                    axes[plot_idx].set_ylabel('Frequency', fontweight='bold')
                    axes[plot_idx].set_title(f'{model_name} - Val Accuracy Distribution', fontweight='bold', fontsize=12)
                    axes[plot_idx].legend()
                    axes[plot_idx].grid(True, alpha=0.3, axis='y')
                    plot_idx += 1
                
                if val_losses and plot_idx < len(axes):
                    axes[plot_idx].hist(val_losses, bins=20, color='red', alpha=0.7, edgecolor='black')
                    axes[plot_idx].axvline(np.mean(val_losses), color='blue', linestyle='--', linewidth=2, label=f'Mean: {np.mean(val_losses):.4f}')
                    axes[plot_idx].set_xlabel('Validation Loss', fontweight='bold')
                    axes[plot_idx].set_ylabel('Frequency', fontweight='bold')
                    axes[plot_idx].set_title(f'{model_name} - Val Loss Distribution', fontweight='bold', fontsize=12)
                    axes[plot_idx].legend()
                    axes[plot_idx].grid(True, alpha=0.3, axis='y')
                
                plt.tight_layout()
                if IPYTHON_AVAILABLE:
                    with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:
                        tmp_path = tmp.name
                    fig.savefig(tmp_path, dpi=150, bbox_inches='tight')
                    plt.close(fig)
                    # Display the image - read file data into Image object before deleting file
                    # This ensures the image data is in memory when displayed
                    with open(tmp_path, 'rb') as f:
                        img_data = f.read()
                    img = Image(img_data)
                    display(img)
                    # Now safe to delete the file
                    try:
                        os.unlink(tmp_path)
                    except:
                        pass  # Ignore cleanup errors
                else:
                    plt.show()
                
                # Figure 2: Performance by param combination (if available)
                if param_perf and len(param_perf) > 1:
                    fig, ax = plt.subplots(1, 1, figsize=(12, 6))
                    sorted_params = sorted(param_perf.keys(), key=int)
                    param_means = [np.mean(param_perf[p]) for p in sorted_params]
                    param_stds = [np.std(param_perf[p]) for p in sorted_params]
                    
                    x_pos = range(len(sorted_params))
                    ax.bar(x_pos, param_means, yerr=param_stds, color='steelblue', alpha=0.7, 
                          capsize=5, edgecolor='black')
                    ax.set_xlabel('Param Combination', fontweight='bold')
                    ax.set_ylabel('Mean Val F1 Score', fontweight='bold')
                    ax.set_title(f'{model_name} - Performance by Param Combination', fontweight='bold', fontsize=12)
                    ax.set_xticks(x_pos)
                    ax.set_xticklabels(sorted_params, rotation=45, ha='right')
                    ax.grid(True, alpha=0.3, axis='y')
                    
                    # Highlight best
                    best_idx = np.argmax(param_means)
                    ax.bar(best_idx, param_means[best_idx], color='gold', alpha=0.8, edgecolor='red', linewidth=2)
                    
                    plt.tight_layout()
                    if IPYTHON_AVAILABLE:
                        with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:
                            tmp_path = tmp.name
                        fig.savefig(tmp_path, dpi=150, bbox_inches='tight')
                        plt.close(fig)
                        # Display the image - ensure it's shown in notebook
                    # Display the image - read file data into Image object before deleting file
                    # This ensures the image data is in memory when displayed
                    with open(tmp_path, 'rb') as f:
                        img_data = f.read()
                    img = Image(img_data)
                    display(img)
                    # Now safe to delete the file
                    try:
                        os.unlink(tmp_path)
                    except:
                        pass  # Ignore cleanup errors
                    else:
                        plt.show()
                
                # Figure 3: Performance by fold
                if fold_perf and len(fold_perf) > 1:
                    fig, ax = plt.subplots(1, 1, figsize=(10, 6))
                    sorted_folds = sorted(fold_perf.keys())
                    fold_means = [np.mean(fold_perf[f]) for f in sorted_folds]
                    fold_stds = [np.std(fold_perf[f]) for f in sorted_folds]
                    
                    ax.bar(sorted_folds, fold_means, yerr=fold_stds, color='coral', alpha=0.7,
                          capsize=5, edgecolor='black')
                    ax.set_xlabel('Fold', fontweight='bold')
                    ax.set_ylabel('Mean Val F1 Score', fontweight='bold')
                    ax.set_title(f'{model_name} - Performance by Fold', fontweight='bold', fontsize=12)
                    ax.grid(True, alpha=0.3, axis='y')
                    
                    for fold, mean_val in zip(sorted_folds, fold_means):
                        ax.text(fold, mean_val, f'{mean_val:.3f}', ha='center', va='bottom', fontweight='bold')
                    
                    plt.tight_layout()
                    if IPYTHON_AVAILABLE:
                        with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:
                            tmp_path = tmp.name
                        fig.savefig(tmp_path, dpi=150, bbox_inches='tight')
                        plt.close(fig)
                        # Display the image - ensure it's shown in notebook
                    # Display the image - read file data into Image object before deleting file
                    # This ensures the image data is in memory when displayed
                    with open(tmp_path, 'rb') as f:
                        img_data = f.read()
                    img = Image(img_data)
                    display(img)
                    # Now safe to delete the file
                    try:
                        os.unlink(tmp_path)
                    except:
                        pass  # Ignore cleanup errors
                    else:
                        plt.show()
    
    return {
        'runs_metrics': runs_metrics,
        'param_perf': dict(param_perf),
        'fold_perf': dict(fold_perf)
    }


def print_mlflow_experiment_structure(
    structure_data: Dict[str, Any]
) -> None:
    """
    Print MLflow experiment structure summary.
    
    Args:
        structure_data: Dictionary from analyze_mlflow_experiment_structure
    """
    folds = structure_data.get('folds', [])
    param_combos = structure_data.get('param_combos', [])
    batch_sizes = structure_data.get('batch_sizes', [])
    num_frames_list = structure_data.get('num_frames_list', [])
    gradient_accum_steps = structure_data.get('gradient_accum_steps', [])
    
    if folds:
        fold_counts = Counter(folds)
        print(f"\n  Experiment Structure:")
        print(f"    - Folds: {sorted(fold_counts.keys())}")
        print(f"    - Runs per fold: {dict(sorted(fold_counts.items()))}")
        print(f"    - Total unique folds: {len(fold_counts)}")
    
    if param_combos:
        param_counts = Counter(param_combos)
        print(f"    - Param combinations: {len(param_counts)} unique")
        print(f"    - Param combo range: {min(param_combos, key=int)} to {max(param_combos, key=int)}")
        print(f"    - Runs per param combo: min={min(param_counts.values())}, max={max(param_counts.values())}, mean={np.mean(list(param_counts.values())):.1f}")
    
    if batch_sizes:
        batch_counts = Counter(batch_sizes)
        print(f"    - Batch sizes: {sorted(batch_counts.keys())} (distribution: {dict(sorted(batch_counts.items()))})")
    
    if num_frames_list:
        num_frames_counts = Counter(num_frames_list)
        print(f"    - Num frames: {sorted(num_frames_counts.keys())} (distribution: {dict(sorted(num_frames_counts.items()))})")
    
    if gradient_accum_steps:
        grad_accum_counts = Counter(gradient_accum_steps)
        print(f"    - Gradient accumulation steps: {sorted(grad_accum_counts.keys())} (distribution: {dict(sorted(grad_accum_counts.items()))})")
