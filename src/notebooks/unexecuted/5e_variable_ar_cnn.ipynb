{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5e: Variable AR CNN\n",
    "\n",
    "**Inception-like 3D CNN supporting variable aspect ratios**\n",
    "\n",
    "This notebook demonstrates the complete pipeline for training and evaluating this model, including:\n",
    "- Architecture deep-dive with mathematical foundations\n",
    "- Training methodology (5-fold CV, hyperparameter optimization)\n",
    "- Regularization and optimization strategies\n",
    "- MLOps integration (MLflow, DuckDB, Airflow)\n",
    "- Model evaluation with saved checkpoints\n",
    "- Video demonstrations and examples\n",
    "\n",
    "**Category**: CNN  \n",
    "**Pretrained**: No  \n",
    "**Input Shape**: (N, C, T, H, W) with arbitrary H, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML, Video, Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Add project root\n",
    "project_root = Path().absolute().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f'[FOLDER] Project root: {project_root}')\n",
    "print(f'[OK] Imports successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Deep-Dive\n",
    "\n",
    "### Variable AR CNN Architecture\n",
    "\n",
    "**Inception3D blocks ? AdaptiveAvgPool3d ? Classification**\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Variable aspect ratio support**\n",
    "- **Global pooling**\n",
    "- **Efficient memory usage**\n",
    "\n",
    "### Implementation Location\n",
    "\n",
    "- **Model Class**: `lib/training/variable_ar_cnn.py`\n",
    "- **Factory**: `lib/training/model_factory.py` (create_model function)\n",
    "- **Training**: `lib/training/pipeline.py` (stage5_train_models function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Checkpoint Verification\n",
    "\n",
    "Check if trained model exists before demonstrating usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for saved model\n",
    "model_dir = project_root / \"data\" / \"stage5\" / \"variable_ar_cnn\"\n",
    "\n",
    "if model_dir.exists():\n",
    "    # Find model checkpoints\n",
    "    checkpoint_files = list(model_dir.glob(\"**/*.pt\")) + list(model_dir.glob(\"**/*.joblib\"))\n",
    "    metrics_files = list(model_dir.glob(\"**/metrics.json\"))\n",
    "    \n",
    "    print(f\"[OK] Model directory found: {model_dir}\")\n",
    "    print(f\"   Checkpoints: {len(checkpoint_files)}\")\n",
    "    print(f\"   Metrics files: {len(metrics_files)}\")\n",
    "    \n",
    "    if metrics_files:\n",
    "        with open(metrics_files[0], 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "        \n",
    "        print(f\"\\n[PLOT] Model Performance:\")\n",
    "        print(f\"   Mean F1: {metrics.get('mean_test_f1', 'N/A'):.4f}\" if isinstance(metrics.get('mean_test_f1'), (int, float)) else f\"   Mean F1: {metrics.get('mean_test_f1', 'N/A')}\")\n",
    "        print(f\"   Mean Accuracy: {metrics.get('mean_test_acc', 'N/A'):.4f}\" if isinstance(metrics.get('mean_test_acc'), (int, float)) else f\"   Mean Accuracy: {metrics.get('mean_test_acc', 'N/A')}\")\n",
    "        \n",
    "        model_available = True\n",
    "    else:\n",
    "        print(\"[WARN] No metrics file found\")\n",
    "        model_available = False\n",
    "else:\n",
    "    print(f\"[WARN] Model not trained yet: {model_dir}\")\n",
    "    model_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Code (Commented)\n",
    "\n",
    "**Note**: This section shows how to train the model. The code is commented out to prevent accidental training.\n",
    "\n",
    "### SLURM Script\n",
    "\n",
    "```bash\n",
    "# sbatch scripts/slurm_jobs/slurm_stage5e.sh\n",
    "```\n",
    "\n",
    "### Python API\n",
    "\n",
    "```python\n",
    "# from lib.training.pipeline import stage5_train_models\n",
    "# \n",
    "# results = stage5_train_models(\n",
    "#     project_root='.',\n",
    "#     scaled_metadata_path='data/scaled_videos/scaled_metadata.parquet',\n",
    "#     features_stage2_path='data/features_stage2/features_metadata.parquet',\n",
    "#     features_stage4_path='data/features_stage4/features_metadata.parquet',\n",
    "#     model_types=['variable_ar_cnn'],\n",
    "#     n_splits=5,\n",
    "#     num_frames=1000,\n",
    "#     output_dir='data/stage5',\n",
    "#     use_tracking=True,\n",
    "#     use_mlflow=True\n",
    "# )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Configuration\n",
    "\n",
    "**Single Hyperparameter Combination** (optimized for efficiency):\n",
    "\n",
    "See `lib/training/grid_search.py` for full configuration.\n",
    "\n",
    "**Rationale for Single Combination**:\n",
    "- Reduced from 5+ combinations to 1 for training efficiency\n",
    "- Hyperparameters selected based on model architecture best practices\n",
    "- Grid search performed on sample, best params applied to full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLOps Integration\n",
    "\n",
    "### Experiment Tracking with MLflow\n",
    "\n",
    "**Location**: `lib/mlops/mlflow_tracker.py`\n",
    "\n",
    "**What's Tracked**:\n",
    "- Hyperparameters (learning_rate, batch_size, weight_decay, etc.)\n",
    "- Metrics (train_loss, val_acc, test_f1, precision, recall, AUC-ROC)\n",
    "- Model artifacts (checkpoints, configs, plots)\n",
    "- Run metadata (tags, timestamps, fold numbers, model_type)\n",
    "\n",
    "**Access MLflow UI**:\n",
    "```bash\n",
    "mlflow ui --port 5000\n",
    "# Open http://localhost:5000\n",
    "```\n",
    "\n",
    "### Analytics with DuckDB\n",
    "\n",
    "**Location**: `lib/utils/duckdb_analytics.py`\n",
    "\n",
    "**Fast SQL Queries on Training Results**:\n",
    "```python\n",
    "from lib.utils.duckdb_analytics import DuckDBAnalytics\n",
    "\n",
    "analytics = DuckDBAnalytics()\n",
    "analytics.register_parquet('results', 'data/stage5/variable_ar_cnn/metrics.json')\n",
    "result = analytics.query(\"\"\"\n",
    "    SELECT \n",
    "        fold,\n",
    "        AVG(test_f1) as avg_f1,\n",
    "        STDDEV(test_f1) as std_f1\n",
    "    FROM results\n",
    "    GROUP BY fold\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "### Airflow Orchestration\n",
    "\n",
    "**Location**: `airflow/dags/fvc_pipeline_dag.py`\n",
    "\n",
    "**Pipeline Stages**:\n",
    "1. Stage 1: Video Augmentation\n",
    "2. Stage 2: Feature Extraction\n",
    "3. Stage 3: Video Scaling\n",
    "4. Stage 4: Scaled Feature Extraction\n",
    "5. Stage 5: Model Training (this model)\n",
    "\n",
    "**Benefits**:\n",
    "- Dependency management (automatic task ordering)\n",
    "- Retry logic (automatic retries on failure)\n",
    "- Monitoring (web UI for pipeline status)\n",
    "- Scheduling (cron-based scheduling support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Methodology\n",
    "\n",
    "### 5-Fold Stratified Cross-Validation\n",
    "\n",
    "**Why 5-Fold CV?**\n",
    "- **Robust Estimates**: More reliable than single train/test split\n",
    "- **Stratification**: Ensures class balance in each fold\n",
    "- **Group-Aware**: Prevents data leakage (same video ID not in train/val)\n",
    "- **Reproducibility**: Fixed random seed (42)\n",
    "\n",
    "**Evaluation**: Metrics averaged across 5 folds with standard deviation\n",
    "\n",
    "### Regularization Strategy\n",
    "\n",
    "**L2 Regularization (Weight Decay)**:\n",
    "- **Value**: 1e-4 (standard) to 1e-3 (stronger)\n",
    "- **Rationale**: Prevents overfitting, improves generalization\n",
    "- **Implementation**: AdamW optimizer with weight_decay parameter\n",
    "\n",
    "**Dropout**:\n",
    "- **Value**: 0.3-0.5 in classification heads\n",
    "- **Rationale**: Prevents co-adaptation of neurons\n",
    "- **Location**: Fully connected layers before final classification\n",
    "\n",
    "**Batch Normalization**:\n",
    "- **Rationale**: Stabilizes training, enables higher learning rates\n",
    "- **Location**: After convolutional layers\n",
    "\n",
    "**Gradient Clipping**:\n",
    "- **Value**: max_norm=1.0\n",
    "- **Rationale**: Prevents exploding gradients in deep networks\n",
    "\n",
    "**Early Stopping**:\n",
    "- **Patience**: 5 epochs\n",
    "- **Metric**: Validation F1 score\n",
    "- **Rationale**: Prevents overfitting, saves training time\n",
    "\n",
    "### Optimization Strategy\n",
    "\n",
    "**Optimizer**: AdamW\n",
    "- **Learning Rate**: 1e-4 to 5e-4 (model-dependent)\n",
    "- **Betas**: (0.9, 0.999)\n",
    "- **Weight Decay**: 1e-4\n",
    "- **Rationale**: AdamW decouples weight decay from gradient updates\n",
    "\n",
    "**Learning Rate Schedule**:\n",
    "- **Type**: Cosine annealing with warmup\n",
    "- **Warmup Epochs**: 2\n",
    "- **Warmup Factor**: 0.1 (starts at 10% of LR)\n",
    "- **Rationale**: Smooth learning rate decay improves convergence\n",
    "\n",
    "**Differential Learning Rates** (for pretrained models):\n",
    "- **Backbone LR**: 5e-6 (frozen or fine-tuned slowly)\n",
    "- **Head LR**: 5e-4 (new layers trained faster)\n",
    "- **Rationale**: Preserves pretrained features while adapting to new task\n",
    "\n",
    "**Mixed Precision Training (AMP)**:\n",
    "- **Enabled**: Yes (default)\n",
    "- **Benefits**: 2x speedup, 50% memory reduction\n",
    "- **Rationale**: FP16 operations faster on modern GPUs\n",
    "\n",
    "**Gradient Accumulation**:\n",
    "- **Dynamic**: Based on batch size and memory constraints\n",
    "- **Effective Batch Size**: batch_size ? gradient_accumulation_steps\n",
    "- **Rationale**: Maintains large effective batch size despite memory constraints\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "**ReLU**:\n",
    "- **Location**: Convolutional layers\n",
    "- **Rationale**: Standard for CNNs, prevents vanishing gradients\n",
    "\n",
    "**GELU**:\n",
    "- **Location**: Transformer layers\n",
    "- **Rationale**: Smoother gradients than ReLU, better for Transformers\n",
    "\n",
    "**Sigmoid**:\n",
    "- **Location**: Final output (binary classification)\n",
    "- **Rationale**: Maps logits to [0, 1] probability\n",
    "\n",
    "### Data Pipeline\n",
    "\n",
    "**Video Loading**:\n",
    "- **Method**: Frame-by-frame decoding (50x memory reduction)\n",
    "- **Chunked Loading**: Process videos in chunks to avoid OOM\n",
    "- **Caching**: Frame cache for faster subsequent loads\n",
    "\n",
    "**Augmentation**:\n",
    "- **Method**: Pre-generated augmentations (reproducible, fast)\n",
    "- **Types**: Spatial (rotation, flip, color jitter, noise, blur) + Temporal (frame drop, duplicate, reverse)\n",
    "- **Rationale**: Increases dataset diversity, prevents overfitting\n",
    "\n",
    "**Scaling**:\n",
    "- **Target**: 256x256 max dimension (letterboxing preserves aspect ratio)\n",
    "- **Method**: Bilinear interpolation (default) or autoencoder upscaling (optional)\n",
    "- **Rationale**: Consistent input size, memory efficiency\n",
    "\n",
    "**Normalization**:\n",
    "- **Method**: ImageNet statistics or [0, 1] normalization\n",
    "- **Rationale**: Consistent input distribution improves training stability\n",
    "\n",
    "**Frame Sampling**:\n",
    "- **Method**: Uniform sampling across video duration\n",
    "- **Frames**: 1000 frames per video (configurable)\n",
    "- **Rationale**: Captures temporal patterns across entire video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Demonstration\n",
    "\n",
    "Load and display sample videos for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample videos for demonstration\n",
    "scaled_metadata = project_root / \"data\" / \"scaled_videos\" / \"scaled_metadata.parquet\"\n",
    "\n",
    "if scaled_metadata.exists():\n",
    "    from lib.utils.paths import load_metadata_flexible\n",
    "    \n",
    "    df = load_metadata_flexible(str(scaled_metadata))\n",
    "    \n",
    "    if df is not None and df.height > 0:\n",
    "        # Sample real and fake videos\n",
    "        real_videos = df.filter(pl.col('label') == 'real').head(2)\n",
    "        fake_videos = df.filter(pl.col('label') == 'fake').head(2)\n",
    "        \n",
    "        print(f\"[?] Sample Videos:\")\n",
    "        print(f\"   Real videos: {real_videos.height}\")\n",
    "        print(f\"   Fake videos: {fake_videos.height}\")\n",
    "        \n",
    "        # Display video paths (actual video display requires video files)\n",
    "        if real_videos.height > 0:\n",
    "            real_path = real_videos['video_path'][0]\n",
    "            print(f\"\\n[OK] Real video: {Path(real_path).name}\")\n",
    "            # Video(real_path, width=400)  # Uncomment if video file exists\n",
    "        \n",
    "        if fake_videos.height > 0:\n",
    "            fake_path = fake_videos['video_path'][0]\n",
    "            print(f\"\\n[X] Fake video: {Path(fake_path).name}\")\n",
    "            # Video(fake_path, width=400)  # Uncomment if video file exists\n",
    "    else:\n",
    "        print(\"[WARN] No videos found in metadata\")\n",
    "else:\n",
    "    print(\"[WARN] Scaled videos metadata not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference Example\n",
    "\n",
    "Load saved model and perform inference on sample videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and perform inference\n",
    "# Check if model checkpoint exists\n",
    "checkpoint_files = list(model_dir.glob(\"**/*.pt\")) + list(model_dir.glob(\"**/*.joblib\")) if model_dir.exists() else []\n",
    "\n",
    "if len(checkpoint_files) > 0:\n",
    "    try:\n",
    "        import torch\n",
    "        from lib.training.model_factory import create_model\n",
    "        from lib.mlops.config import RunConfig\n",
    "        \n",
    "        # Create model\n",
    "        config = RunConfig(\n",
    "            run_id='demo',\n",
    "            experiment_name='demo',\n",
    "            model_type='variable_ar_cnn',\n",
    "            num_frames=1000\n",
    "        )\n",
    "        \n",
    "        model = create_model('variable_ar_cnn', config)\n",
    "        \n",
    "        # Load checkpoint\n",
    "        checkpoint_path = checkpoint_files[0]\n",
    "        \n",
    "        if checkpoint_path.suffix == '.pt':\n",
    "            checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "            if 'model_state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            else:\n",
    "                model.load_state_dict(checkpoint)\n",
    "            print(f\"[OK] Loaded PyTorch checkpoint: {checkpoint_path.name}\")\n",
    "        else:\n",
    "            import joblib\n",
    "            model = joblib.load(checkpoint_path)\n",
    "            print(f\"[OK] Loaded sklearn/XGBoost model: {checkpoint_path.name}\")\n",
    "        \n",
    "        model.eval()\n",
    "        print(f\"\\n[PLOT] Model loaded and ready for inference\")\n",
    "        print(f\"   Model type: {type(model).__name__}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Error loading model: {e}\")\n",
    "else:\n",
    "    print(\"[WARN] Model checkpoint not available for inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Visualization\n",
    "\n",
    "Visualize training results and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize metrics\n",
    "if model_available and metrics_files:\n",
    "    with open(metrics_files[0], 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # F1 Score across folds\n",
    "    if 'fold_results' in metrics:\n",
    "        fold_f1s = [fold.get('test_f1', 0) for fold in metrics['fold_results']]\n",
    "        axes[0, 0].bar(range(1, len(fold_f1s)+1), fold_f1s, color='#4CAF50')\n",
    "        axes[0, 0].axhline(metrics.get('mean_test_f1', 0), color='red', linestyle='--', label='Mean')\n",
    "        axes[0, 0].set_title('F1 Score by Fold', fontsize=12, fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Fold')\n",
    "        axes[0, 0].set_ylabel('F1 Score')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Accuracy across folds\n",
    "    if 'fold_results' in metrics:\n",
    "        fold_accs = [fold.get('test_acc', 0) for fold in metrics['fold_results']]\n",
    "        axes[0, 1].bar(range(1, len(fold_accs)+1), fold_accs, color='#2196F3')\n",
    "        axes[0, 1].axhline(metrics.get('mean_test_acc', 0), color='red', linestyle='--', label='Mean')\n",
    "        axes[0, 1].set_title('Accuracy by Fold', fontsize=12, fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Fold')\n",
    "        axes[0, 1].set_ylabel('Accuracy')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Metrics summary\n",
    "    metrics_summary = {\n",
    "        'F1 Score': metrics.get('mean_test_f1', 0),\n",
    "        'Accuracy': metrics.get('mean_test_acc', 0),\n",
    "        'Precision': metrics.get('mean_test_precision', 0),\n",
    "        'Recall': metrics.get('mean_test_recall', 0)\n",
    "    }\n",
    "    \n",
    "    axes[1, 0].bar(metrics_summary.keys(), metrics_summary.values(), color=['#4CAF50', '#2196F3', '#FF9800', '#9C27B0'])\n",
    "    axes[1, 0].set_title('Average Metrics', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('Score')\n",
    "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "    axes[1, 0].set_ylim([0, 1])\n",
    "    \n",
    "    # Confusion matrix (if available)\n",
    "    if 'confusion_matrix' in metrics:\n",
    "        cm = np.array(metrics['confusion_matrix'])\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 1])\n",
    "        axes[1, 1].set_title('Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('Predicted')\n",
    "        axes[1, 1].set_ylabel('Actual')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n[PLOT] Performance Summary:\")\n",
    "    for key, value in metrics_summary.items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            print(f\"   {key}: {value:.4f}\")\n",
    "else:\n",
    "    print(\"[WARN] Metrics not available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Plots\n",
    "\n",
    "The following plots were generated during model training and provide insights into model performance across cross-validation folds and hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training plots if available\n",
    "from IPython.display import Image, display, HTML\n",
    "\n",
    "plots_dir = MODEL_DIR / \"plots\"\n",
    "\n",
    "if plots_dir.exists():\n",
    "    print(f\"[OK] Found plots directory: {plots_dir}\")\n",
    "    \n",
    "    # List of expected plot files\n",
    "    plot_files = {\n",
    "        \"cv_fold_comparison.png\": \"Cross-Validation Fold Comparison\",\n",
    "        \"hyperparameter_search.png\": \"Hyperparameter Search Results\",\n",
    "        \"learning_curves.png\": \"Learning Curves (if available)\",\n",
    "        \"roc_curve.png\": \"ROC Curve (if available)\",\n",
    "        \"precision_recall_curve.png\": \"Precision-Recall Curve (if available)\",\n",
    "        \"confusion_matrix.png\": \"Confusion Matrix (if available)\"\n",
    "    }\n",
    "    \n",
    "    plots_found = []\n",
    "    for plot_file, plot_name in plot_files.items():\n",
    "        plot_path = plots_dir / plot_file\n",
    "        if plot_path.exists():\n",
    "            plots_found.append((plot_path, plot_name))\n",
    "            print(f\"  [OK] Found: {plot_file}\")\n",
    "    \n",
    "    if plots_found:\n",
    "        print(f\"\\n[PLOT] Displaying {len(plots_found)} training plot(s):\\n\")\n",
    "        for plot_path, plot_name in plots_found:\n",
    "            print(f\"\\n### {plot_name}\")\n",
    "            display(Image(str(plot_path), width=800))\n",
    "    else:\n",
    "        print(\"[WARN]  No plot files found in plots directory.\")\n",
    "        print(f\"Expected plots directory: {plots_dir}\")\n",
    "else:\n",
    "    print(f\"[WARN]  Plots directory not found: {plots_dir}\")\n",
    "    print(\"Plots are generated during training. Please ensure training has completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the Variable AR CNN model for deepfake video detection, including:\n",
    "\n",
    "- [OK] **Architecture**: Inception3D blocks ? AdaptiveAvgPool3d ? Classification\n",
    "- [OK] **Training Methodology**: 5-fold CV, hyperparameter optimization, regularization\n",
    "- [OK] **MLOps Integration**: MLflow tracking, DuckDB analytics, Airflow orchestration\n",
    "- [OK] **Evaluation**: Model checkpoint verification and inference examples\n",
    "\n",
    "**Next Steps**:\n",
    "1. Compare with other models (see other notebooks 5a-5u)\n",
    "2. Explore MLflow UI for detailed experiment tracking\n",
    "3. Use DuckDB for custom analytics queries\n",
    "4. Deploy best model to production"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
