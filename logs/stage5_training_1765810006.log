2025-12-15 09:46:46 [INFO] [__main__:310] ================================================================================
2025-12-15 09:46:46 [INFO] [__main__:311] STAGE 5: MODEL TRAINING
2025-12-15 09:46:46 [INFO] [__main__:312] ================================================================================
2025-12-15 09:46:46 [INFO] [__main__:313] Project root: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc
2025-12-15 09:46:46 [INFO] [__main__:314] Scaled metadata: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/scaled_videos/scaled_metadata.arrow
2025-12-15 09:46:46 [INFO] [__main__:315] Features Stage 2: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/features_stage2/features_metadata.arrow
2025-12-15 09:46:46 [INFO] [__main__:316] Features Stage 4: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/features_stage4/features_scaled_metadata.arrow
2025-12-15 09:46:46 [INFO] [__main__:317] Model types: ['xgboost_vit_transformer']
2025-12-15 09:46:46 [INFO] [__main__:318] K-fold splits: 5
2025-12-15 09:46:46 [INFO] [__main__:319] Number of frames: 1000
2025-12-15 09:46:46 [INFO] [__main__:320] Output directory: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/stage5
2025-12-15 09:46:46 [INFO] [__main__:321] Experiment tracking: Enabled
2025-12-15 09:46:46 [INFO] [__main__:324] Delete existing: False
2025-12-15 09:46:46 [INFO] [__main__:325] Resume mode: True
2025-12-15 09:46:46 [INFO] [__main__:326] Log file: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/logs/stage5_training_1765810006.log
2025-12-15 09:46:46 [INFO] [__main__:333] ================================================================================
2025-12-15 09:46:46 [INFO] [__main__:334] Checking prerequisites...
2025-12-15 09:46:46 [INFO] [__main__:335] ================================================================================
2025-12-15 09:46:46 [INFO] [__main__:342] ✓ Scaled metadata file found: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/scaled_videos/scaled_metadata.arrow
2025-12-15 09:46:46 [INFO] [__main__:352] ✓ All model types are valid
2025-12-15 09:46:46 [INFO] [__main__:358] ================================================================================
2025-12-15 09:46:46 [INFO] [__main__:359] Initial memory statistics:
2025-12-15 09:46:46 [INFO] [__main__:360] ================================================================================
2025-12-15 09:46:46 [INFO] [lib.utils.memory:91] Memory stats (Stage 5: before training): {'cpu_memory_mb': 736.1015625, 'cpu_memory_gb': 0.7188491821289062, 'cpu_vms_mb': 10218.80078125, 'gpu_allocated_gb': 0.0, 'gpu_reserved_gb': 0.0, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 16.928342016}
2025-12-15 09:46:46 [INFO] [__main__:364] ================================================================================
2025-12-15 09:46:46 [INFO] [__main__:365] Starting Stage 5: Model Training
2025-12-15 09:46:46 [INFO] [__main__:366] ================================================================================
2025-12-15 09:46:46 [INFO] [__main__:367] Training 1 model(s) with 5-fold cross-validation
2025-12-15 09:46:46 [INFO] [__main__:368] This may take a while depending on dataset size and model complexity...
2025-12-15 09:46:46 [INFO] [__main__:369] Progress will be logged in real-time
2025-12-15 09:46:46 [INFO] [__main__:370] ================================================================================
2025-12-15 09:46:46 [INFO] [__main__:375] Calling Stage 5 training pipeline...
2025-12-15 09:46:46 [INFO] [__main__:376] This may take a while - progress will be logged in real-time
2025-12-15 09:46:46 [INFO] [lib.training.pipeline:1323] Applied global PyTorch memory optimizations at pipeline start
2025-12-15 09:46:46 [INFO] [lib.training.pipeline:1326] ================================================================================
2025-12-15 09:46:46 [INFO] [lib.training.pipeline:1327] Stage 5: Model Training Pipeline Started
2025-12-15 09:46:46 [INFO] [lib.training.pipeline:1328] ================================================================================
2025-12-15 09:46:46 [INFO] [lib.training.pipeline:1329] Model types: ['xgboost_vit_transformer']
2025-12-15 09:46:46 [INFO] [lib.training.pipeline:1330] K-fold splits: 5
2025-12-15 09:46:46 [INFO] [lib.training.pipeline:1331] Frames per video: 1000
2025-12-15 09:46:46 [INFO] [lib.training.pipeline:1332] Output directory: data/stage5
2025-12-15 09:46:46 [INFO] [lib.training.pipeline:1333] Initializing pipeline...
2025-12-15 09:46:46 [INFO] [lib.training.pipeline:246] ================================================================================
2025-12-15 09:46:46 [INFO] [lib.training.pipeline:247] STAGE 5 PREREQUISITE VALIDATION
2025-12-15 09:46:46 [INFO] [lib.training.pipeline:248] ================================================================================
2025-12-15 09:46:46 [INFO] [lib.training.pipeline:250] 
[1/3] Checking Stage 3 (scaled videos) - REQUIRED for all models...
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:259] ✓ Stage 3 metadata found: 3278 scaled videos
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:260]   Path: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/scaled_videos/scaled_metadata.arrow
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:262] 
[2/3] Checking Stage 2 (features) - REQUIRED for *_stage2 models...
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:270] ✓ Stage 2 metadata found: 3278 feature rows
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:271]   Path: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/features_stage2/features_metadata.arrow
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:273] 
[3/3] Checking Stage 4 (scaled features) - REQUIRED for *_stage2_stage4 models...
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:281] ✓ Stage 4 metadata found: 3277 feature rows
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:282]   Path: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/features_stage4/features_scaled_metadata.arrow
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:284] 
================================================================================
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:285] MODEL REQUIREMENTS CHECK
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:286] ================================================================================
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:305] ✓ xgboost_vit_transformer: CAN RUN
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:307] 
================================================================================
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:308] VALIDATION SUMMARY
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:309] ================================================================================
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:310] Stage 3 (scaled videos): ✓ Available
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:311]   Count: 3278 videos
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:312] Stage 2 (features): ✓ Available
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:313]   Count: 3278 feature rows
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:314] Stage 4 (scaled features): ✓ Available
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:315]   Count: 3277 feature rows
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:316] 
Runnable models: 1/1
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:317]   ['xgboost_vit_transformer']
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:324] ================================================================================
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:1414] 
Stage 5: Loading metadata...
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:1438] Loaded metadata: 3278 rows
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:1448] ✓ Data validation passed: 3278 rows (> 3000 required)
2025-12-15 09:46:47 [WARNING] [lib.utils.guardrails:171] Disk usage high: free=821929.94GB, used=1677910.06GB (67.1%)
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:1469] Loading Stage 2 and Stage 4 features (required for feature-based models)...
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:1479] Stage 5: Found 3278 scaled videos
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:1482] ================================================================================
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:1483] STAGE 5: VALIDATING VIDEOS (checking for corruption and empty videos)
2025-12-15 09:46:47 [INFO] [lib.training.pipeline:1484] ================================================================================
2025-12-15 09:46:47 [INFO] [lib.data.loading:156] Checking file existence for 3278 videos...
2025-12-15 09:46:49 [INFO] [lib.data.loading:166] Found 3278 existing video files (filtered out 0 missing files)
2025-12-15 09:46:49 [INFO] [lib.data.loading:170] Checking for corrupted videos (moov atom errors, etc.)...
2025-12-15 09:46:50 [INFO] [lib.data.loading:92] Loaded validation cache from /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/.video_validation_cache/validation_0192be168a40eb7e_corruptTrue_framesTrue.parquet (3278 entries)
2025-12-15 09:46:50 [INFO] [lib.data.loading:194] Cache hit: 3278 videos, Cache miss: 0 videos
2025-12-15 09:46:50 [WARNING] [lib.data.loading:306] Filtered out 1 corrupted videos and 0 empty videos. Keeping 3277 valid videos.
2025-12-15 09:46:50 [WARNING] [lib.data.loading:311] Sample of invalid videos:
2025-12-15 09:46:50 [WARNING] [lib.data.loading:313]   data/scaled_videos/FX5aeuJFQ64_aug1_scaled_aug1.mp4: Corrupted video: moov atom not found
2025-12-15 09:46:50 [INFO] [lib.data.loading:323] Found /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/skip_frame_check.txt - skipping frame check as requested.
2025-12-15 09:46:50 [WARNING] [lib.data.loading:387] Filtered out 1 invalid videos (corrupted). Keeping 3277 valid videos.
2025-12-15 09:46:50 [INFO] [lib.training.pipeline:1493] ✓ Video validation complete: 3277 valid videos ready for training
2025-12-15 09:46:50 [INFO] [lib.training.pipeline:1494] ================================================================================
2025-12-15 09:46:50 [INFO] [lib.training.pipeline:1580] Frame caching enabled (default): cache_dir=data/.frame_cache. This will cache processed frames to disk to speed up training. First epoch will be slower (building cache), subsequent epochs will be faster. To disable, set FVC_USE_FRAME_CACHE=0
2025-12-15 09:46:50 [INFO] [lib.training.pipeline:1653] Overriding num_frames to 400 for xgboost_vit_transformer. Original num_frames was 1000.
2025-12-15 09:46:50 [INFO] [lib.training.pipeline:1696] 
================================================================================
2025-12-15 09:46:50 [INFO] [lib.training.pipeline:1697] Stage 5: Training model: xgboost_vit_transformer
2025-12-15 09:46:50 [INFO] [lib.training.pipeline:1698] ================================================================================
2025-12-15 09:46:50 [INFO] [lib.training.pipeline:1716] Overriding model_config num_frames to 400 for xgboost_vit_transformer (XGBoost pretrained model with enhanced feature extraction) to improve feature quality.
2025-12-15 09:46:50 [INFO] [lib.training.pipeline:1734] Grid search: 1 hyperparameter combinations to try
2025-12-15 09:46:50 [INFO] [lib.training.pipeline:1742] ================================================================================
2025-12-15 09:46:50 [INFO] [lib.training.pipeline:1743] HYPERPARAMETER SEARCH: Using 10.0% stratified sample for efficiency
2025-12-15 09:46:50 [INFO] [lib.training.pipeline:1744] ================================================================================
2025-12-15 09:46:50 [INFO] [lib.training.pipeline:1753] Hyperparameter search sample: 327 rows (10.0% of 3277 total)
2025-12-15 09:46:50 [INFO] [lib.training.pipeline:1754] To change sample size, set FVC_GRID_SEARCH_SAMPLE_SIZE environment variable (current: 0.1)
2025-12-15 09:46:50 [INFO] [lib.training.pipeline:1766] Using 5-fold stratified cross-validation on 10.0% sample for hyperparameter search
2025-12-15 09:46:50 [INFO] [lib.training.pipeline:1777] 
================================================================================
2025-12-15 09:46:50 [INFO] [lib.training.pipeline:1778] Grid Search: Hyperparameter combination 1/1
2025-12-15 09:46:50 [INFO] [lib.training.pipeline:1779] Parameters: {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.8}
2025-12-15 09:46:50 [INFO] [lib.training.pipeline:1780] ================================================================================
2025-12-15 09:46:50 [INFO] [lib.training.pipeline:1798] 
Hyperparameter Search - xgboost_vit_transformer - Fold 1/5 (10.0% sample)
2025-12-15 09:46:50 [INFO] [lib.training.pipeline:448] Training XGBoost model xgboost_vit_transformer on fold 1...
2025-12-15 09:46:50 [INFO] [lib.training._xgboost_pretrained:636] Training XGBoost on features from vit_transformer...
2025-12-15 09:46:50 [INFO] [lib.training._xgboost_pretrained:637] Processing 261 videos...
2025-12-15 09:46:51 [INFO] [lib.training._xgboost_pretrained:522] Loading pretrained model: vit_transformer (num_frames=400)
2025-12-15 09:46:57 [INFO] [timm.models._builder:217] Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-12-15 09:46:57 [INFO] [timm.models._hub:232] [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-12-15 09:46:57 [INFO] [timm.layers.pos_embed:57] Resized position embedding: (14, 14) to (16, 16).
2025-12-15 09:46:58 [INFO] [lib.training._xgboost_pretrained:537] Loaded vit_transformer model to GPU, cleared cache
2025-12-15 09:46:58 [INFO] [lib.training._xgboost_pretrained:554] ViT model (vit_transformer) requires 256x256 input. Setting fixed_size=256 (VideoDataset will resize even with use_scaled_videos=True).
2025-12-15 09:46:58 [INFO] [lib.training._xgboost_pretrained:604] Extracting features using vit_transformer (num_frames=400)...
2025-12-15 09:46:58 [INFO] [lib.training._xgboost_pretrained:101] Applied PyTorch memory optimizations for feature extraction
2025-12-15 10:05:56 [INFO] [lib.training._xgboost_pretrained:410] Extracted features shape: (261, 3072)
2025-12-15 10:05:56 [INFO] [lib.training._xgboost_pretrained:622] Cleared vit_transformer model from GPU after feature extraction
2025-12-15 10:05:56 [INFO] [lib.training._xgboost_pretrained:660] Removing collinear features...
2025-12-15 10:05:56 [INFO] [lib.training.feature_preprocessing:77] Removed 0 features with zero variance
2025-12-15 10:05:57 [INFO] [lib.training.feature_preprocessing:141] Removed 0 collinear features (correlation >= 0.95)
2025-12-15 10:05:57 [INFO] [lib.training.feature_preprocessing:162] Final feature count: 3072/3072 (100.0% retained)
2025-12-15 10:05:57 [INFO] [lib.training._xgboost_pretrained:667] Using 3072 features after collinearity removal
2025-12-15 10:05:57 [INFO] [lib.training._xgboost_pretrained:683] Class distribution: Class 0=128, Class 1=133, scale_pos_weight=0.962
2025-12-15 10:05:57 [INFO] [lib.training._xgboost_pretrained:701] Training XGBoost: 208 train samples, 53 validation samples for early stopping
2025-12-15 10:05:57 [INFO] [lib.training._xgboost_pretrained:704] Training XGBoost with improved architecture (multi-layer + temporal pooling + class weights + early stopping)...
2025-12-15 10:06:06 [INFO] [lib.training._xgboost_pretrained:770] Early stopping: Best iteration = 200 (out of 200)
2025-12-15 10:06:06 [INFO] [lib.training._xgboost_pretrained:772] ✓ XGBoost trained on pretrained model features with enhanced feature extraction
2025-12-15 10:06:07 [INFO] [lib.training._xgboost_pretrained:522] Loading pretrained model: vit_transformer (num_frames=400)
2025-12-15 10:06:08 [INFO] [timm.models._builder:217] Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-12-15 10:06:08 [INFO] [timm.models._hub:232] [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-12-15 10:06:08 [INFO] [timm.layers.pos_embed:57] Resized position embedding: (14, 14) to (16, 16).
2025-12-15 10:06:08 [INFO] [lib.training._xgboost_pretrained:537] Loaded vit_transformer model to GPU, cleared cache
2025-12-15 10:06:08 [INFO] [lib.training._xgboost_pretrained:554] ViT model (vit_transformer) requires 256x256 input. Setting fixed_size=256 (VideoDataset will resize even with use_scaled_videos=True).
2025-12-15 10:06:08 [INFO] [lib.training._xgboost_pretrained:604] Extracting features using vit_transformer (num_frames=400)...
2025-12-15 10:06:08 [INFO] [lib.training._xgboost_pretrained:101] Applied PyTorch memory optimizations for feature extraction
2025-12-15 10:10:42 [INFO] [lib.training._xgboost_pretrained:410] Extracted features shape: (66, 3072)
2025-12-15 10:10:42 [INFO] [lib.training._xgboost_pretrained:622] Cleared vit_transformer model from GPU after feature extraction
2025-12-15 10:10:42 [INFO] [lib.training.pipeline:500] Fold 1 - Val Loss: 0.4664, Val Acc: 0.7727, Val F1: 0.7692, Val Precision: 0.8065, Val Recall: 0.7353
2025-12-15 10:10:42 [INFO] [lib.training.pipeline:505]   Class 0 - Precision: 0.7429, Recall: 0.8125, F1: 0.7761
2025-12-15 10:10:42 [INFO] [lib.training.pipeline:509]   Class 1 - Precision: 0.8065, Recall: 0.7353, F1: 0.7692
2025-12-15 10:10:42 [INFO] [lib.training._xgboost_pretrained:840] Saved XGBoost model to /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/stage5/xgboost_vit_transformer/fold_1
2025-12-15 10:10:42 [INFO] [lib.training.pipeline:516] Saved XGBoost model to /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/stage5/xgboost_vit_transformer/fold_1
2025-12-15 10:10:43 [INFO] [lib.training.pipeline:1798] 
Hyperparameter Search - xgboost_vit_transformer - Fold 2/5 (10.0% sample)
2025-12-15 10:10:43 [INFO] [lib.training.pipeline:448] Training XGBoost model xgboost_vit_transformer on fold 2...
2025-12-15 10:10:43 [INFO] [lib.training._xgboost_pretrained:636] Training XGBoost on features from vit_transformer...
2025-12-15 10:10:43 [INFO] [lib.training._xgboost_pretrained:637] Processing 261 videos...
2025-12-15 10:10:43 [INFO] [lib.training._xgboost_pretrained:522] Loading pretrained model: vit_transformer (num_frames=400)
2025-12-15 10:10:44 [INFO] [timm.models._builder:217] Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-12-15 10:10:44 [INFO] [timm.models._hub:232] [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-12-15 10:10:44 [INFO] [timm.layers.pos_embed:57] Resized position embedding: (14, 14) to (16, 16).
2025-12-15 10:10:45 [INFO] [lib.training._xgboost_pretrained:537] Loaded vit_transformer model to GPU, cleared cache
2025-12-15 10:10:45 [INFO] [lib.training._xgboost_pretrained:554] ViT model (vit_transformer) requires 256x256 input. Setting fixed_size=256 (VideoDataset will resize even with use_scaled_videos=True).
2025-12-15 10:10:45 [INFO] [lib.training._xgboost_pretrained:604] Extracting features using vit_transformer (num_frames=400)...
2025-12-15 10:10:45 [INFO] [lib.training._xgboost_pretrained:101] Applied PyTorch memory optimizations for feature extraction
2025-12-15 10:29:04 [INFO] [lib.training._xgboost_pretrained:410] Extracted features shape: (261, 3072)
2025-12-15 10:29:04 [INFO] [lib.training._xgboost_pretrained:622] Cleared vit_transformer model from GPU after feature extraction
2025-12-15 10:29:04 [INFO] [lib.training._xgboost_pretrained:660] Removing collinear features...
2025-12-15 10:29:04 [INFO] [lib.training.feature_preprocessing:77] Removed 0 features with zero variance
2025-12-15 10:29:05 [INFO] [lib.training.feature_preprocessing:141] Removed 0 collinear features (correlation >= 0.95)
2025-12-15 10:29:05 [INFO] [lib.training.feature_preprocessing:162] Final feature count: 3072/3072 (100.0% retained)
2025-12-15 10:29:05 [INFO] [lib.training._xgboost_pretrained:667] Using 3072 features after collinearity removal
2025-12-15 10:29:05 [INFO] [lib.training._xgboost_pretrained:683] Class distribution: Class 0=128, Class 1=133, scale_pos_weight=0.962
2025-12-15 10:29:05 [INFO] [lib.training._xgboost_pretrained:701] Training XGBoost: 208 train samples, 53 validation samples for early stopping
2025-12-15 10:29:05 [INFO] [lib.training._xgboost_pretrained:704] Training XGBoost with improved architecture (multi-layer + temporal pooling + class weights + early stopping)...
2025-12-15 10:29:14 [INFO] [lib.training._xgboost_pretrained:770] Early stopping: Best iteration = 172 (out of 200)
2025-12-15 10:29:14 [INFO] [lib.training._xgboost_pretrained:772] ✓ XGBoost trained on pretrained model features with enhanced feature extraction
2025-12-15 10:29:14 [INFO] [lib.training._xgboost_pretrained:522] Loading pretrained model: vit_transformer (num_frames=400)
2025-12-15 10:29:15 [INFO] [timm.models._builder:217] Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-12-15 10:29:15 [INFO] [timm.models._hub:232] [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-12-15 10:29:15 [INFO] [timm.layers.pos_embed:57] Resized position embedding: (14, 14) to (16, 16).
2025-12-15 10:29:15 [INFO] [lib.training._xgboost_pretrained:537] Loaded vit_transformer model to GPU, cleared cache
2025-12-15 10:29:15 [INFO] [lib.training._xgboost_pretrained:554] ViT model (vit_transformer) requires 256x256 input. Setting fixed_size=256 (VideoDataset will resize even with use_scaled_videos=True).
2025-12-15 10:29:16 [INFO] [lib.training._xgboost_pretrained:604] Extracting features using vit_transformer (num_frames=400)...
2025-12-15 10:29:16 [INFO] [lib.training._xgboost_pretrained:101] Applied PyTorch memory optimizations for feature extraction
2025-12-15 10:33:51 [INFO] [lib.training._xgboost_pretrained:410] Extracted features shape: (66, 3072)
2025-12-15 10:33:51 [INFO] [lib.training._xgboost_pretrained:622] Cleared vit_transformer model from GPU after feature extraction
2025-12-15 10:33:51 [INFO] [lib.training.pipeline:500] Fold 2 - Val Loss: 0.3902, Val Acc: 0.7879, Val F1: 0.7879, Val Precision: 0.8125, Val Recall: 0.7647
2025-12-15 10:33:51 [INFO] [lib.training.pipeline:505]   Class 0 - Precision: 0.7647, Recall: 0.8125, F1: 0.7879
2025-12-15 10:33:51 [INFO] [lib.training.pipeline:509]   Class 1 - Precision: 0.8125, Recall: 0.7647, F1: 0.7879
2025-12-15 10:33:51 [INFO] [lib.training._xgboost_pretrained:840] Saved XGBoost model to /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/stage5/xgboost_vit_transformer/fold_2
2025-12-15 10:33:51 [INFO] [lib.training.pipeline:516] Saved XGBoost model to /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/stage5/xgboost_vit_transformer/fold_2
2025-12-15 10:33:52 [INFO] [lib.training.pipeline:1798] 
Hyperparameter Search - xgboost_vit_transformer - Fold 3/5 (10.0% sample)
2025-12-15 10:33:52 [INFO] [lib.training.pipeline:448] Training XGBoost model xgboost_vit_transformer on fold 3...
2025-12-15 10:33:52 [INFO] [lib.training._xgboost_pretrained:636] Training XGBoost on features from vit_transformer...
2025-12-15 10:33:52 [INFO] [lib.training._xgboost_pretrained:637] Processing 262 videos...
2025-12-15 10:33:52 [INFO] [lib.training._xgboost_pretrained:522] Loading pretrained model: vit_transformer (num_frames=400)
2025-12-15 10:33:53 [INFO] [timm.models._builder:217] Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-12-15 10:33:53 [INFO] [timm.models._hub:232] [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-12-15 10:33:53 [INFO] [timm.layers.pos_embed:57] Resized position embedding: (14, 14) to (16, 16).
2025-12-15 10:33:53 [INFO] [lib.training._xgboost_pretrained:537] Loaded vit_transformer model to GPU, cleared cache
2025-12-15 10:33:53 [INFO] [lib.training._xgboost_pretrained:554] ViT model (vit_transformer) requires 256x256 input. Setting fixed_size=256 (VideoDataset will resize even with use_scaled_videos=True).
2025-12-15 10:33:53 [INFO] [lib.training._xgboost_pretrained:604] Extracting features using vit_transformer (num_frames=400)...
2025-12-15 10:33:53 [INFO] [lib.training._xgboost_pretrained:101] Applied PyTorch memory optimizations for feature extraction
2025-12-15 10:52:26 [INFO] [lib.training._xgboost_pretrained:410] Extracted features shape: (262, 3072)
2025-12-15 10:52:26 [INFO] [lib.training._xgboost_pretrained:622] Cleared vit_transformer model from GPU after feature extraction
2025-12-15 10:52:26 [INFO] [lib.training._xgboost_pretrained:660] Removing collinear features...
2025-12-15 10:52:26 [INFO] [lib.training.feature_preprocessing:77] Removed 0 features with zero variance
2025-12-15 10:52:27 [INFO] [lib.training.feature_preprocessing:141] Removed 0 collinear features (correlation >= 0.95)
2025-12-15 10:52:27 [INFO] [lib.training.feature_preprocessing:162] Final feature count: 3072/3072 (100.0% retained)
2025-12-15 10:52:27 [INFO] [lib.training._xgboost_pretrained:667] Using 3072 features after collinearity removal
2025-12-15 10:52:27 [INFO] [lib.training._xgboost_pretrained:683] Class distribution: Class 0=128, Class 1=134, scale_pos_weight=0.955
2025-12-15 10:52:27 [INFO] [lib.training._xgboost_pretrained:701] Training XGBoost: 209 train samples, 53 validation samples for early stopping
2025-12-15 10:52:27 [INFO] [lib.training._xgboost_pretrained:704] Training XGBoost with improved architecture (multi-layer + temporal pooling + class weights + early stopping)...
2025-12-15 10:52:31 [INFO] [lib.training._xgboost_pretrained:770] Early stopping: Best iteration = 32 (out of 200)
2025-12-15 10:52:31 [INFO] [lib.training._xgboost_pretrained:772] ✓ XGBoost trained on pretrained model features with enhanced feature extraction
2025-12-15 10:52:31 [INFO] [lib.training._xgboost_pretrained:522] Loading pretrained model: vit_transformer (num_frames=400)
2025-12-15 10:52:32 [INFO] [timm.models._builder:217] Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-12-15 10:52:33 [INFO] [timm.models._hub:232] [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-12-15 10:52:33 [INFO] [timm.layers.pos_embed:57] Resized position embedding: (14, 14) to (16, 16).
2025-12-15 10:52:33 [INFO] [lib.training._xgboost_pretrained:537] Loaded vit_transformer model to GPU, cleared cache
2025-12-15 10:52:33 [INFO] [lib.training._xgboost_pretrained:554] ViT model (vit_transformer) requires 256x256 input. Setting fixed_size=256 (VideoDataset will resize even with use_scaled_videos=True).
2025-12-15 10:52:33 [INFO] [lib.training._xgboost_pretrained:604] Extracting features using vit_transformer (num_frames=400)...
2025-12-15 10:52:34 [INFO] [lib.training._xgboost_pretrained:101] Applied PyTorch memory optimizations for feature extraction
2025-12-15 10:57:09 [INFO] [lib.training._xgboost_pretrained:410] Extracted features shape: (65, 3072)
2025-12-15 10:57:10 [INFO] [lib.training._xgboost_pretrained:622] Cleared vit_transformer model from GPU after feature extraction
2025-12-15 10:57:10 [INFO] [lib.training.pipeline:500] Fold 3 - Val Loss: 0.5288, Val Acc: 0.7385, Val F1: 0.7213, Val Precision: 0.7857, Val Recall: 0.6667
2025-12-15 10:57:10 [INFO] [lib.training.pipeline:505]   Class 0 - Precision: 0.7027, Recall: 0.8125, F1: 0.7536
2025-12-15 10:57:10 [INFO] [lib.training.pipeline:509]   Class 1 - Precision: 0.7857, Recall: 0.6667, F1: 0.7213
2025-12-15 10:57:10 [INFO] [lib.training._xgboost_pretrained:840] Saved XGBoost model to /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/stage5/xgboost_vit_transformer/fold_3
2025-12-15 10:57:10 [INFO] [lib.training.pipeline:516] Saved XGBoost model to /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/stage5/xgboost_vit_transformer/fold_3
2025-12-15 10:57:10 [INFO] [lib.training.pipeline:1798] 
Hyperparameter Search - xgboost_vit_transformer - Fold 4/5 (10.0% sample)
2025-12-15 10:57:10 [INFO] [lib.training.pipeline:448] Training XGBoost model xgboost_vit_transformer on fold 4...
2025-12-15 10:57:10 [INFO] [lib.training._xgboost_pretrained:636] Training XGBoost on features from vit_transformer...
2025-12-15 10:57:10 [INFO] [lib.training._xgboost_pretrained:637] Processing 262 videos...
2025-12-15 10:57:10 [INFO] [lib.training._xgboost_pretrained:522] Loading pretrained model: vit_transformer (num_frames=400)
2025-12-15 10:57:12 [INFO] [timm.models._builder:217] Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-12-15 10:57:12 [INFO] [timm.models._hub:232] [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-12-15 10:57:12 [INFO] [timm.layers.pos_embed:57] Resized position embedding: (14, 14) to (16, 16).
2025-12-15 10:57:12 [INFO] [lib.training._xgboost_pretrained:537] Loaded vit_transformer model to GPU, cleared cache
2025-12-15 10:57:12 [INFO] [lib.training._xgboost_pretrained:554] ViT model (vit_transformer) requires 256x256 input. Setting fixed_size=256 (VideoDataset will resize even with use_scaled_videos=True).
2025-12-15 10:57:12 [INFO] [lib.training._xgboost_pretrained:604] Extracting features using vit_transformer (num_frames=400)...
2025-12-15 10:57:12 [INFO] [lib.training._xgboost_pretrained:101] Applied PyTorch memory optimizations for feature extraction
2025-12-15 11:16:21 [INFO] [lib.training._xgboost_pretrained:410] Extracted features shape: (262, 3072)
2025-12-15 11:16:21 [INFO] [lib.training._xgboost_pretrained:622] Cleared vit_transformer model from GPU after feature extraction
2025-12-15 11:16:21 [INFO] [lib.training._xgboost_pretrained:660] Removing collinear features...
2025-12-15 11:16:21 [INFO] [lib.training.feature_preprocessing:77] Removed 0 features with zero variance
2025-12-15 11:16:23 [INFO] [lib.training.feature_preprocessing:141] Removed 0 collinear features (correlation >= 0.95)
2025-12-15 11:16:23 [INFO] [lib.training.feature_preprocessing:162] Final feature count: 3072/3072 (100.0% retained)
2025-12-15 11:16:23 [INFO] [lib.training._xgboost_pretrained:667] Using 3072 features after collinearity removal
2025-12-15 11:16:23 [INFO] [lib.training._xgboost_pretrained:683] Class distribution: Class 0=128, Class 1=134, scale_pos_weight=0.955
2025-12-15 11:16:23 [INFO] [lib.training._xgboost_pretrained:701] Training XGBoost: 209 train samples, 53 validation samples for early stopping
2025-12-15 11:16:23 [INFO] [lib.training._xgboost_pretrained:704] Training XGBoost with improved architecture (multi-layer + temporal pooling + class weights + early stopping)...
2025-12-15 11:16:27 [INFO] [lib.training._xgboost_pretrained:770] Early stopping: Best iteration = 51 (out of 200)
2025-12-15 11:16:27 [INFO] [lib.training._xgboost_pretrained:772] ✓ XGBoost trained on pretrained model features with enhanced feature extraction
2025-12-15 11:16:28 [INFO] [lib.training._xgboost_pretrained:522] Loading pretrained model: vit_transformer (num_frames=400)
2025-12-15 11:16:29 [INFO] [timm.models._builder:217] Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-12-15 11:16:29 [INFO] [timm.models._hub:232] [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-12-15 11:16:29 [INFO] [timm.layers.pos_embed:57] Resized position embedding: (14, 14) to (16, 16).
2025-12-15 11:16:29 [INFO] [lib.training._xgboost_pretrained:537] Loaded vit_transformer model to GPU, cleared cache
2025-12-15 11:16:29 [INFO] [lib.training._xgboost_pretrained:554] ViT model (vit_transformer) requires 256x256 input. Setting fixed_size=256 (VideoDataset will resize even with use_scaled_videos=True).
2025-12-15 11:16:29 [INFO] [lib.training._xgboost_pretrained:604] Extracting features using vit_transformer (num_frames=400)...
2025-12-15 11:16:29 [INFO] [lib.training._xgboost_pretrained:101] Applied PyTorch memory optimizations for feature extraction
2025-12-15 11:21:07 [INFO] [lib.training._xgboost_pretrained:410] Extracted features shape: (65, 3072)
2025-12-15 11:21:08 [INFO] [lib.training._xgboost_pretrained:622] Cleared vit_transformer model from GPU after feature extraction
2025-12-15 11:21:08 [INFO] [lib.training.pipeline:500] Fold 4 - Val Loss: 0.4936, Val Acc: 0.7846, Val F1: 0.7941, Val Precision: 0.7714, Val Recall: 0.8182
2025-12-15 11:21:08 [INFO] [lib.training.pipeline:505]   Class 0 - Precision: 0.8000, Recall: 0.7500, F1: 0.7742
2025-12-15 11:21:08 [INFO] [lib.training.pipeline:509]   Class 1 - Precision: 0.7714, Recall: 0.8182, F1: 0.7941
2025-12-15 11:21:08 [INFO] [lib.training._xgboost_pretrained:840] Saved XGBoost model to /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/stage5/xgboost_vit_transformer/fold_4
2025-12-15 11:21:08 [INFO] [lib.training.pipeline:516] Saved XGBoost model to /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/stage5/xgboost_vit_transformer/fold_4
2025-12-15 11:21:08 [INFO] [lib.training.pipeline:1798] 
Hyperparameter Search - xgboost_vit_transformer - Fold 5/5 (10.0% sample)
2025-12-15 11:21:08 [INFO] [lib.training.pipeline:448] Training XGBoost model xgboost_vit_transformer on fold 5...
2025-12-15 11:21:08 [INFO] [lib.training._xgboost_pretrained:636] Training XGBoost on features from vit_transformer...
2025-12-15 11:21:08 [INFO] [lib.training._xgboost_pretrained:637] Processing 262 videos...
2025-12-15 11:21:08 [INFO] [lib.training._xgboost_pretrained:522] Loading pretrained model: vit_transformer (num_frames=400)
2025-12-15 11:21:09 [INFO] [timm.models._builder:217] Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-12-15 11:21:09 [INFO] [timm.models._hub:232] [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-12-15 11:21:09 [INFO] [timm.layers.pos_embed:57] Resized position embedding: (14, 14) to (16, 16).
2025-12-15 11:21:10 [INFO] [lib.training._xgboost_pretrained:537] Loaded vit_transformer model to GPU, cleared cache
2025-12-15 11:21:10 [INFO] [lib.training._xgboost_pretrained:554] ViT model (vit_transformer) requires 256x256 input. Setting fixed_size=256 (VideoDataset will resize even with use_scaled_videos=True).
2025-12-15 11:21:10 [INFO] [lib.training._xgboost_pretrained:604] Extracting features using vit_transformer (num_frames=400)...
2025-12-15 11:21:10 [INFO] [lib.training._xgboost_pretrained:101] Applied PyTorch memory optimizations for feature extraction
2025-12-15 11:39:42 [INFO] [lib.training._xgboost_pretrained:410] Extracted features shape: (262, 3072)
2025-12-15 11:39:42 [INFO] [lib.training._xgboost_pretrained:622] Cleared vit_transformer model from GPU after feature extraction
2025-12-15 11:39:42 [INFO] [lib.training._xgboost_pretrained:660] Removing collinear features...
2025-12-15 11:39:42 [INFO] [lib.training.feature_preprocessing:77] Removed 0 features with zero variance
2025-12-15 11:39:43 [INFO] [lib.training.feature_preprocessing:141] Removed 0 collinear features (correlation >= 0.95)
2025-12-15 11:39:43 [INFO] [lib.training.feature_preprocessing:162] Final feature count: 3072/3072 (100.0% retained)
2025-12-15 11:39:43 [INFO] [lib.training._xgboost_pretrained:667] Using 3072 features after collinearity removal
2025-12-15 11:39:43 [INFO] [lib.training._xgboost_pretrained:683] Class distribution: Class 0=128, Class 1=134, scale_pos_weight=0.955
2025-12-15 11:39:43 [INFO] [lib.training._xgboost_pretrained:701] Training XGBoost: 209 train samples, 53 validation samples for early stopping
2025-12-15 11:39:43 [INFO] [lib.training._xgboost_pretrained:704] Training XGBoost with improved architecture (multi-layer + temporal pooling + class weights + early stopping)...
2025-12-15 11:39:50 [INFO] [lib.training._xgboost_pretrained:770] Early stopping: Best iteration = 111 (out of 200)
2025-12-15 11:39:50 [INFO] [lib.training._xgboost_pretrained:772] ✓ XGBoost trained on pretrained model features with enhanced feature extraction
2025-12-15 11:39:51 [INFO] [lib.training._xgboost_pretrained:522] Loading pretrained model: vit_transformer (num_frames=400)
2025-12-15 11:39:52 [INFO] [timm.models._builder:217] Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-12-15 11:39:52 [INFO] [timm.models._hub:232] [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-12-15 11:39:52 [INFO] [timm.layers.pos_embed:57] Resized position embedding: (14, 14) to (16, 16).
2025-12-15 11:39:52 [INFO] [lib.training._xgboost_pretrained:537] Loaded vit_transformer model to GPU, cleared cache
2025-12-15 11:39:52 [INFO] [lib.training._xgboost_pretrained:554] ViT model (vit_transformer) requires 256x256 input. Setting fixed_size=256 (VideoDataset will resize even with use_scaled_videos=True).
2025-12-15 11:39:52 [INFO] [lib.training._xgboost_pretrained:604] Extracting features using vit_transformer (num_frames=400)...
2025-12-15 11:39:53 [INFO] [lib.training._xgboost_pretrained:101] Applied PyTorch memory optimizations for feature extraction
2025-12-15 11:44:15 [INFO] [lib.training._xgboost_pretrained:410] Extracted features shape: (65, 3072)
2025-12-15 11:44:15 [INFO] [lib.training._xgboost_pretrained:622] Cleared vit_transformer model from GPU after feature extraction
2025-12-15 11:44:15 [INFO] [lib.training.pipeline:500] Fold 5 - Val Loss: 0.4740, Val Acc: 0.7692, Val F1: 0.7692, Val Precision: 0.7812, Val Recall: 0.7576
2025-12-15 11:44:15 [INFO] [lib.training.pipeline:505]   Class 0 - Precision: 0.7576, Recall: 0.7812, F1: 0.7692
2025-12-15 11:44:15 [INFO] [lib.training.pipeline:509]   Class 1 - Precision: 0.7812, Recall: 0.7576, F1: 0.7692
2025-12-15 11:44:15 [INFO] [lib.training._xgboost_pretrained:840] Saved XGBoost model to /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/stage5/xgboost_vit_transformer/fold_5
2025-12-15 11:44:15 [INFO] [lib.training.pipeline:516] Saved XGBoost model to /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/stage5/xgboost_vit_transformer/fold_5
2025-12-15 11:44:15 [INFO] [lib.training.pipeline:1932] Parameter combination 1 - Mean F1: 0.7684, Mean Acc: 0.7706
2025-12-15 11:44:15 [INFO] [lib.training.pipeline:1944] Using single parameter combination: {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.8}
2025-12-15 11:44:15 [INFO] [lib.training.pipeline:1947] ================================================================================
2025-12-15 11:44:15 [INFO] [lib.training.pipeline:1948] FINAL TRAINING: Using full dataset with best hyperparameters
2025-12-15 11:44:15 [INFO] [lib.training.pipeline:1949] ================================================================================
2025-12-15 11:44:16 [INFO] [lib.training.pipeline:1962] Final training: Using 5-fold stratified cross-validation on full dataset (3277 rows)
2025-12-15 11:44:16 [INFO] [lib.training.pipeline:1970] Final training using best hyperparameters: {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.8}
2025-12-15 11:44:16 [INFO] [lib.training.pipeline:1976] 
Final Training - xgboost_vit_transformer - Fold 1/5 (full dataset)
2025-12-15 11:44:16 [INFO] [lib.training.pipeline:448] Training XGBoost model xgboost_vit_transformer on fold 1...
2025-12-15 11:44:16 [INFO] [lib.training._xgboost_pretrained:636] Training XGBoost on features from vit_transformer...
2025-12-15 11:44:16 [INFO] [lib.training._xgboost_pretrained:637] Processing 2621 videos...
2025-12-15 11:44:16 [INFO] [lib.training._xgboost_pretrained:522] Loading pretrained model: vit_transformer (num_frames=400)
2025-12-15 11:44:17 [INFO] [timm.models._builder:217] Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-12-15 11:44:17 [INFO] [timm.models._hub:232] [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-12-15 11:44:17 [INFO] [timm.layers.pos_embed:57] Resized position embedding: (14, 14) to (16, 16).
2025-12-15 11:44:17 [INFO] [lib.training._xgboost_pretrained:537] Loaded vit_transformer model to GPU, cleared cache
2025-12-15 11:44:17 [INFO] [lib.training._xgboost_pretrained:554] ViT model (vit_transformer) requires 256x256 input. Setting fixed_size=256 (VideoDataset will resize even with use_scaled_videos=True).
2025-12-15 11:44:17 [INFO] [lib.training._xgboost_pretrained:604] Extracting features using vit_transformer (num_frames=400)...
2025-12-15 11:44:17 [INFO] [lib.training._xgboost_pretrained:101] Applied PyTorch memory optimizations for feature extraction
2025-12-15 14:54:03 [INFO] [lib.training._xgboost_pretrained:410] Extracted features shape: (2621, 3072)
2025-12-15 14:54:03 [INFO] [lib.training._xgboost_pretrained:622] Cleared vit_transformer model from GPU after feature extraction
2025-12-15 14:54:03 [INFO] [lib.training._xgboost_pretrained:660] Removing collinear features...
2025-12-15 14:54:04 [INFO] [lib.training.feature_preprocessing:77] Removed 0 features with zero variance
2025-12-15 14:54:05 [INFO] [lib.training.feature_preprocessing:141] Removed 0 collinear features (correlation >= 0.95)
2025-12-15 14:54:05 [INFO] [lib.training.feature_preprocessing:162] Final feature count: 3072/3072 (100.0% retained)
2025-12-15 14:54:05 [INFO] [lib.training._xgboost_pretrained:667] Using 3072 features after collinearity removal
2025-12-15 14:54:05 [INFO] [lib.training._xgboost_pretrained:683] Class distribution: Class 0=1284, Class 1=1337, scale_pos_weight=0.960
2025-12-15 14:54:05 [INFO] [lib.training._xgboost_pretrained:701] Training XGBoost: 2096 train samples, 525 validation samples for early stopping
2025-12-15 14:54:05 [INFO] [lib.training._xgboost_pretrained:704] Training XGBoost with improved architecture (multi-layer + temporal pooling + class weights + early stopping)...
2025-12-15 14:55:03 [INFO] [lib.training._xgboost_pretrained:770] Early stopping: Best iteration = 200 (out of 200)
2025-12-15 14:55:03 [INFO] [lib.training._xgboost_pretrained:772] ✓ XGBoost trained on pretrained model features with enhanced feature extraction
2025-12-15 14:55:04 [INFO] [lib.training._xgboost_pretrained:522] Loading pretrained model: vit_transformer (num_frames=400)
2025-12-15 14:55:05 [INFO] [timm.models._builder:217] Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-12-15 14:55:05 [INFO] [timm.models._hub:232] [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-12-15 14:55:05 [INFO] [timm.layers.pos_embed:57] Resized position embedding: (14, 14) to (16, 16).
2025-12-15 14:55:05 [INFO] [lib.training._xgboost_pretrained:537] Loaded vit_transformer model to GPU, cleared cache
2025-12-15 14:55:05 [INFO] [lib.training._xgboost_pretrained:554] ViT model (vit_transformer) requires 256x256 input. Setting fixed_size=256 (VideoDataset will resize even with use_scaled_videos=True).
2025-12-15 14:55:05 [INFO] [lib.training._xgboost_pretrained:604] Extracting features using vit_transformer (num_frames=400)...
2025-12-15 14:55:06 [INFO] [lib.training._xgboost_pretrained:101] Applied PyTorch memory optimizations for feature extraction
2025-12-15 15:39:42 [INFO] [lib.training._xgboost_pretrained:410] Extracted features shape: (656, 3072)
2025-12-15 15:39:43 [INFO] [lib.training._xgboost_pretrained:622] Cleared vit_transformer model from GPU after feature extraction
2025-12-15 15:39:43 [INFO] [lib.training.pipeline:500] Fold 1 - Val Loss: 0.0490, Val Acc: 0.9954, Val F1: 0.9955, Val Precision: 0.9970, Val Recall: 0.9940
2025-12-15 15:39:43 [INFO] [lib.training._xgboost_pretrained:840] Saved XGBoost model to /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/stage5/xgboost_vit_transformer/fold_1
2025-12-15 15:39:43 [INFO] [lib.training.pipeline:516] Saved XGBoost model to /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/stage5/xgboost_vit_transformer/fold_1
2025-12-15 15:39:43 [INFO] [lib.training.pipeline:1976] 
Final Training - xgboost_vit_transformer - Fold 2/5 (full dataset)
2025-12-15 15:39:43 [INFO] [lib.training.pipeline:448] Training XGBoost model xgboost_vit_transformer on fold 2...
2025-12-15 15:39:43 [INFO] [lib.training._xgboost_pretrained:636] Training XGBoost on features from vit_transformer...
2025-12-15 15:39:43 [INFO] [lib.training._xgboost_pretrained:637] Processing 2621 videos...
2025-12-15 15:39:43 [INFO] [lib.training._xgboost_pretrained:522] Loading pretrained model: vit_transformer (num_frames=400)
2025-12-15 15:39:44 [INFO] [timm.models._builder:217] Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-12-15 15:39:44 [INFO] [timm.models._hub:232] [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-12-15 15:39:44 [INFO] [timm.layers.pos_embed:57] Resized position embedding: (14, 14) to (16, 16).
2025-12-15 15:39:45 [INFO] [lib.training._xgboost_pretrained:537] Loaded vit_transformer model to GPU, cleared cache
2025-12-15 15:39:45 [INFO] [lib.training._xgboost_pretrained:554] ViT model (vit_transformer) requires 256x256 input. Setting fixed_size=256 (VideoDataset will resize even with use_scaled_videos=True).
2025-12-15 15:39:45 [INFO] [lib.training._xgboost_pretrained:604] Extracting features using vit_transformer (num_frames=400)...
2025-12-15 15:39:45 [INFO] [lib.training._xgboost_pretrained:101] Applied PyTorch memory optimizations for feature extraction
2025-12-15 18:45:19 [INFO] [lib.training._xgboost_pretrained:410] Extracted features shape: (2621, 3072)
2025-12-15 18:45:20 [INFO] [lib.training._xgboost_pretrained:622] Cleared vit_transformer model from GPU after feature extraction
2025-12-15 18:45:20 [INFO] [lib.training._xgboost_pretrained:660] Removing collinear features...
2025-12-15 18:45:20 [INFO] [lib.training.feature_preprocessing:77] Removed 0 features with zero variance
2025-12-15 18:45:22 [INFO] [lib.training.feature_preprocessing:141] Removed 0 collinear features (correlation >= 0.95)
2025-12-15 18:45:22 [INFO] [lib.training.feature_preprocessing:162] Final feature count: 3072/3072 (100.0% retained)
2025-12-15 18:45:22 [INFO] [lib.training._xgboost_pretrained:667] Using 3072 features after collinearity removal
2025-12-15 18:45:22 [INFO] [lib.training._xgboost_pretrained:683] Class distribution: Class 0=1284, Class 1=1337, scale_pos_weight=0.960
2025-12-15 18:45:22 [INFO] [lib.training._xgboost_pretrained:701] Training XGBoost: 2096 train samples, 525 validation samples for early stopping
2025-12-15 18:45:22 [INFO] [lib.training._xgboost_pretrained:704] Training XGBoost with improved architecture (multi-layer + temporal pooling + class weights + early stopping)...
2025-12-15 18:46:19 [INFO] [lib.training._xgboost_pretrained:770] Early stopping: Best iteration = 200 (out of 200)
2025-12-15 18:46:19 [INFO] [lib.training._xgboost_pretrained:772] ✓ XGBoost trained on pretrained model features with enhanced feature extraction
2025-12-15 18:46:19 [INFO] [lib.training._xgboost_pretrained:522] Loading pretrained model: vit_transformer (num_frames=400)
2025-12-15 18:46:21 [INFO] [timm.models._builder:217] Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-12-15 18:46:21 [INFO] [timm.models._hub:232] [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-12-15 18:46:21 [INFO] [timm.layers.pos_embed:57] Resized position embedding: (14, 14) to (16, 16).
2025-12-15 18:46:21 [INFO] [lib.training._xgboost_pretrained:537] Loaded vit_transformer model to GPU, cleared cache
2025-12-15 18:46:21 [INFO] [lib.training._xgboost_pretrained:554] ViT model (vit_transformer) requires 256x256 input. Setting fixed_size=256 (VideoDataset will resize even with use_scaled_videos=True).
2025-12-15 18:46:21 [INFO] [lib.training._xgboost_pretrained:604] Extracting features using vit_transformer (num_frames=400)...
2025-12-15 18:46:21 [INFO] [lib.training._xgboost_pretrained:101] Applied PyTorch memory optimizations for feature extraction
2025-12-15 19:31:08 [INFO] [lib.training._xgboost_pretrained:410] Extracted features shape: (656, 3072)
2025-12-15 19:31:09 [INFO] [lib.training._xgboost_pretrained:622] Cleared vit_transformer model from GPU after feature extraction
2025-12-15 19:31:09 [INFO] [lib.training.pipeline:500] Fold 2 - Val Loss: 0.0429, Val Acc: 0.9970, Val F1: 0.9970, Val Precision: 0.9970, Val Recall: 0.9970
2025-12-15 19:31:09 [INFO] [lib.training._xgboost_pretrained:840] Saved XGBoost model to /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/stage5/xgboost_vit_transformer/fold_2
2025-12-15 19:31:09 [INFO] [lib.training.pipeline:516] Saved XGBoost model to /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/stage5/xgboost_vit_transformer/fold_2
2025-12-15 19:31:09 [INFO] [lib.training.pipeline:1976] 
Final Training - xgboost_vit_transformer - Fold 3/5 (full dataset)
2025-12-15 19:31:09 [INFO] [lib.training.pipeline:448] Training XGBoost model xgboost_vit_transformer on fold 3...
2025-12-15 19:31:09 [INFO] [lib.training._xgboost_pretrained:636] Training XGBoost on features from vit_transformer...
2025-12-15 19:31:09 [INFO] [lib.training._xgboost_pretrained:637] Processing 2622 videos...
2025-12-15 19:31:09 [INFO] [lib.training._xgboost_pretrained:522] Loading pretrained model: vit_transformer (num_frames=400)
2025-12-15 19:31:10 [INFO] [timm.models._builder:217] Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-12-15 19:31:10 [INFO] [timm.models._hub:232] [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-12-15 19:31:10 [INFO] [timm.layers.pos_embed:57] Resized position embedding: (14, 14) to (16, 16).
2025-12-15 19:31:11 [INFO] [lib.training._xgboost_pretrained:537] Loaded vit_transformer model to GPU, cleared cache
2025-12-15 19:31:11 [INFO] [lib.training._xgboost_pretrained:554] ViT model (vit_transformer) requires 256x256 input. Setting fixed_size=256 (VideoDataset will resize even with use_scaled_videos=True).
2025-12-15 19:31:11 [INFO] [lib.training._xgboost_pretrained:604] Extracting features using vit_transformer (num_frames=400)...
2025-12-15 19:31:11 [INFO] [lib.training._xgboost_pretrained:101] Applied PyTorch memory optimizations for feature extraction
2025-12-15 22:34:28 [INFO] [lib.training._xgboost_pretrained:410] Extracted features shape: (2622, 3072)
2025-12-15 22:34:29 [INFO] [lib.training._xgboost_pretrained:622] Cleared vit_transformer model from GPU after feature extraction
2025-12-15 22:34:29 [INFO] [lib.training._xgboost_pretrained:660] Removing collinear features...
2025-12-15 22:34:29 [INFO] [lib.training.feature_preprocessing:77] Removed 0 features with zero variance
2025-12-15 22:34:31 [INFO] [lib.training.feature_preprocessing:141] Removed 0 collinear features (correlation >= 0.95)
2025-12-15 22:34:31 [INFO] [lib.training.feature_preprocessing:162] Final feature count: 3072/3072 (100.0% retained)
2025-12-15 22:34:31 [INFO] [lib.training._xgboost_pretrained:667] Using 3072 features after collinearity removal
2025-12-15 22:34:31 [INFO] [lib.training._xgboost_pretrained:683] Class distribution: Class 0=1284, Class 1=1338, scale_pos_weight=0.960
2025-12-15 22:34:31 [INFO] [lib.training._xgboost_pretrained:701] Training XGBoost: 2097 train samples, 525 validation samples for early stopping
2025-12-15 22:34:31 [INFO] [lib.training._xgboost_pretrained:704] Training XGBoost with improved architecture (multi-layer + temporal pooling + class weights + early stopping)...
2025-12-15 22:35:28 [INFO] [lib.training._xgboost_pretrained:770] Early stopping: Best iteration = 200 (out of 200)
2025-12-15 22:35:28 [INFO] [lib.training._xgboost_pretrained:772] ✓ XGBoost trained on pretrained model features with enhanced feature extraction
2025-12-15 22:35:28 [INFO] [lib.training._xgboost_pretrained:522] Loading pretrained model: vit_transformer (num_frames=400)
2025-12-15 22:35:30 [INFO] [timm.models._builder:217] Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-12-15 22:35:30 [INFO] [timm.models._hub:232] [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-12-15 22:35:30 [INFO] [timm.layers.pos_embed:57] Resized position embedding: (14, 14) to (16, 16).
2025-12-15 22:35:30 [INFO] [lib.training._xgboost_pretrained:537] Loaded vit_transformer model to GPU, cleared cache
2025-12-15 22:35:30 [INFO] [lib.training._xgboost_pretrained:554] ViT model (vit_transformer) requires 256x256 input. Setting fixed_size=256 (VideoDataset will resize even with use_scaled_videos=True).
2025-12-15 22:35:30 [INFO] [lib.training._xgboost_pretrained:604] Extracting features using vit_transformer (num_frames=400)...
2025-12-15 22:35:30 [INFO] [lib.training._xgboost_pretrained:101] Applied PyTorch memory optimizations for feature extraction
2025-12-15 23:19:50 [INFO] [lib.training._xgboost_pretrained:410] Extracted features shape: (655, 3072)
2025-12-15 23:19:51 [INFO] [lib.training._xgboost_pretrained:622] Cleared vit_transformer model from GPU after feature extraction
2025-12-15 23:19:51 [INFO] [lib.training.pipeline:500] Fold 3 - Val Loss: 0.0505, Val Acc: 0.9924, Val F1: 0.9925, Val Precision: 0.9940, Val Recall: 0.9910
2025-12-15 23:19:51 [INFO] [lib.training._xgboost_pretrained:840] Saved XGBoost model to /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/stage5/xgboost_vit_transformer/fold_3
2025-12-15 23:19:51 [INFO] [lib.training.pipeline:516] Saved XGBoost model to /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/stage5/xgboost_vit_transformer/fold_3
2025-12-15 23:19:51 [INFO] [lib.training.pipeline:1976] 
Final Training - xgboost_vit_transformer - Fold 4/5 (full dataset)
2025-12-15 23:19:51 [INFO] [lib.training.pipeline:448] Training XGBoost model xgboost_vit_transformer on fold 4...
2025-12-15 23:19:51 [INFO] [lib.training._xgboost_pretrained:636] Training XGBoost on features from vit_transformer...
2025-12-15 23:19:51 [INFO] [lib.training._xgboost_pretrained:637] Processing 2622 videos...
2025-12-15 23:19:52 [INFO] [lib.training._xgboost_pretrained:522] Loading pretrained model: vit_transformer (num_frames=400)
2025-12-15 23:19:53 [INFO] [timm.models._builder:217] Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-12-15 23:19:53 [INFO] [timm.models._hub:232] [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-12-15 23:19:53 [INFO] [timm.layers.pos_embed:57] Resized position embedding: (14, 14) to (16, 16).
2025-12-15 23:19:53 [INFO] [lib.training._xgboost_pretrained:537] Loaded vit_transformer model to GPU, cleared cache
2025-12-15 23:19:53 [INFO] [lib.training._xgboost_pretrained:554] ViT model (vit_transformer) requires 256x256 input. Setting fixed_size=256 (VideoDataset will resize even with use_scaled_videos=True).
2025-12-15 23:19:53 [INFO] [lib.training._xgboost_pretrained:604] Extracting features using vit_transformer (num_frames=400)...
2025-12-15 23:19:53 [INFO] [lib.training._xgboost_pretrained:101] Applied PyTorch memory optimizations for feature extraction
